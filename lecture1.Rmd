---
title: 'Lecture 1: Probability'
author: "Zhentao Shi"
date: "September 6, 2017"
output: pdf_document
linestretch: 1.5
fontsize: 12pt
urlcolor: blue
---

# Probability {#review-of-probability}


## Probability Space {#probability}


 A *sample space* $\Omega$ is a collection of all possible outcomes. It is a set
 of things.

An *event* $A$ is a subset of $\Omega$. It is something of interest on the sample space.

A $\sigma$-*field*, denoted by $\mathcal{F}$, is a collection of
  $(A_i\in \Omega)_{i \in \mathbb{N}}$  events such that

(i) $\emptyset\in\mathcal{F}$;
(ii) if an event $A\in\mathcal{F}$, then $A^{c}\in\mathcal{F}$;
(iii) if $A_{i}\in\mathcal{F}$ for $i\in\mathbb{N}$, then
    $\bigcup_{i\in\mathbb{N}}A_{i}\in\mathcal{F}$.



It is easy to show that $\Omega \in \mathcal{F}$ and $\bigcap_{i\in\mathbb{N}}A_{i}\in\mathcal{F}$.
The $\sigma$-field can be viewed as a well-organized structure built on the ground of the sample space. The pair $\left(\Omega,\mathcal{F}\right)$ is called a *measure space*.



Let $\mathcal{G} = \{B_1, B_2,\ldots\}$ be an arbitrary collection of sets, not necessarily a $\sigma$-field. We say $\mathcal{F}$ is
the smallest $\sigma$-field generated by $\mathcal{G}$ if $\mathcal{G}\subseteq \mathcal{F}$, and  $\mathcal{F}\subseteq \mathcal{\tilde{F}}$ for any  $\mathcal{\tilde{F}}$ such that $\mathcal{G}\subseteq \mathcal{\tilde{F}}$.
A *Borel $\sigma$-field* $\mathcal{R}$ is the
smallest $\sigma$-field generated by the open sets on the real line $\mathbb{R}$.




A function $\mu:(\Omega, \mathcal{F})\mapsto\left[0,\infty\right]$ is called a
    *measure* if it satisfies

(i) $\mu\left(A\right)\geq0$ for all     $A\in\mathcal{F}$;
(ii) if $A_{i}\in\mathcal{F}$, $i\in\mathbb{N}$,
are mutually disjoint, then
$$\mu\left(\bigcup_{i\in\mathbb{N}}A_{i}\right)=\sum_{i\in\mathbb{N}}\mu\left(A_{i}\right).$$

Measure can be understand as weight
    or length in our daily life experience.
    In particular, we call $\mu$ a *probability
    measure* if $\mu\left(\Omega\right)=1$. A probability measure is often denoted as $P$.
    The triple $\left(\Omega,\mathcal{F},P\right)$ is called a *probability space*.

## Random Variable

The terminology *random variable* somewhat belies its formal definition of a deterministic mapping. It is a link between
two measure spaces such that any event in the $\sigma$-field installed on the range
can be tracked back to an event in the $\sigma$-field installed
on the domain.

Formally, a function $X:\Omega\mapsto\mathbb{R}$ is
$\left(\Omega,\mathcal{F}\right)\backslash\left(\mathbb{R},\mathcal{R}\right)$
*measurable* if
$$X^{-1}\left(B\right)=\left\{ \omega\in\Omega:X\left(\omega\right)\in B\right\} \in\mathcal{F}$$
for any $B\in\mathcal{R}.$  *Random variable* is an alternative common
name for a measurable function. We say a measurable is a
*discrete random variable* if the set
$\left\{X\left(\omega\right):\omega\in\Omega\right\}$ is finite
or countable. We say it is a
*continuous random variable* if the set
$\left\{ X\left(\omega\right):\omega\in\Omega\right\}$
is uncountable.

A measurable function connects  two measurable spaces.
No probability is involved in its definition. However, if a probability measure $P$
is installed on  $(\Omega, \mathcal{F})$, the measurable function $X$
 will induce a probability measure on $(\mathbb{R},\mathcal{R})$.
It is easy to verify that $P_{X}:(\mathbb{R},\mathcal{R})\mapsto\left[0,1\right]$ is also a probability measure if defined as
    $P_{X}\left(B\right)=P\left(X^{-1}\left(B\right)\right)$
    for any $B\in\mathcal{R}$.  
(If $B_1, B_2\in \mathcal{R}$ are disjoint, then $X^{-1}(B_1), X^{-1}(B_2)\in \mathcal{F}$ are also disjoint.) This $P_{X}$ is called the probability measure
    *induced* by the measurable function $X$.
The induced probability measure $P_X$ is an offspring of the parent
probability measure $P$ though the channel of $X$.

## Distribution Function

We go back to some terms that we have learned in the undergraduate
probability course. A *(cumulative) distribution function*
$F:\mathbb{R}\mapsto [0,1]$ is defined as
$$F\left(x\right)=P\left(X\leq x\right)=
P\left(\{X\leq x\}\right)=P\left(\left\{ \omega\in\Omega:X\left(\omega\right)\leq x\right\} \right).$$
It is often abbreviated as CDF, and it has the following properties.

(i) $\lim_{x\to-\infty}F\left(x\right)=0$,
(ii) $\lim_{x\to\infty}F\left(x\right)=1$,
(iii) non-decreasing,
(iv)  right-continuous $\lim_{y\to x^{+}}F\left(y\right)=F\left(x\right).$

For continuous distribution, if there exists a function $f$ such that for all $x$,
$$F\left(x\right)=\int_{-\infty}^{x}f\left(y\right)dy,$$
 then $f$ is
    called the *probability density function* of $X$, often abbreviated as PDF.
It is easy to show that $f\left(x\right)\geq0$ and
    $\int_{a}^{b}f\left(x\right)dx=F\left(b\right)-F\left(a\right)$.







# Expected Value {#expectation}


## Integration

Integration is one of the most fundamental operations in mathematical analysis.
    We have studied Riemann's integral in the undergraduate calculus.
    Riemann's integral is intuitive, but it encounters difficulty when
    it handles infinite value of the integrand or the region.

Lebesgue integral is a more general way to define integration. It is
    constructed by the following steps. $X$ is called a *simple function* on a measurable space
        $\left(\Omega,\mathcal{F}\right)$ if
        $X=\sum_{i}a_{i}\cdot 1\left\{ A_{i}\right\}$ is a finite sum, where
        $a_{i}\in\mathbb{R}$ and $A_{i}\in\mathcal{F}$.


1. Let $\left(\Omega,\mathcal{F},\mu\right)$ be a probability space. The integral of the simple function $X$ with respect to $\mu$
    is
    $$\int X\mathrm{d}\mu=\sum_{i}a_{i}\mu\left(A_{i}\right).$$
2. Let $X$ be a non-negative measurable function. The integral of $X$
        with respect to $\mu$ is
        $$\int X\mathrm{d}\mu=\sup\left\{ \int Y\mathrm{d}\mu:0\leq Y\leq X,\text{ }Y\text{ is simple}\right\} .$$
3. Let $X$ be a measurable function. Define
        $X^{+}=\max\left\{ X,0\right\}$ and
        $X^{-}=-\min\left\{ X,0\right\}$. Both $X^{+}$ and $X^{-}$ are
        non-negative functions. The integral of $X$ with respect to $\mu$ is
        $$\int X\mathrm{d}\mu=\int X^{+}\mathrm{d}\mu-\int X^{-}\mathrm{d}\mu.$$

    Step 1 defines the integral of a simple function.
    Step 2 defines the integral of a non-negative function as the approximation of
    steps functions from below.
    Step 3 defines the integral of a general function as the difference of the
    integral of two non-negative parts.

    If the measure $\mu$ is a probability measure $P$, then the integral
        $\int X\mathrm{d}P$ is called the *expected value,* or
        *expectation,* of $X$. We often use the notation
        $E\left[X\right]$, instead of $\int X\mathrm{d}P$, for convenience.
        Expectation is the average of a random variable.

    If we know the probability mass function of a discrete random variable, its expectation
    is calculated as $E\left[X\right]=\sum_{x}xP\left(X=x\right)$, which is
    the integral of a simple function.
    If a continuous random variable has a PDF, its expectation
    is computed as  $E\left[X\right]=\int xf\left(x\right)\mathrm{d}x$.


Here are some properties of the expectation.


-  The probability of an event $A$ is the expectation
    of an indicator function. $E\left[1\left\{ A\right\}  \right]= 1\times P(A) + 0 \times P(A^c) =P\left(A\right)$.

- $E[\cdot]$ is a linear operation. If $\phi(\cdot)$ is a linear function, then $E[\phi(X)] = \phi(E[X])$.

-   *Jensen's inequality* is an important fact.
A function $\varphi(\cdot)$ is convex if
    $\varphi( a x_1 + (1-a) x_2 ) \leq a \varphi(x_1) + (1-a) \varphi(x_2)$ for all $x_1,x_2$
    in the domain and $a\in[0,1]$. For instance, $x^2$ is a convex function.
    Jensen's inequality says that if $\varphi\left(\cdot\right)$ is a convex
        function, then
        $\varphi\left(E\left[X\right]\right)\leq E\left[\varphi\left(X\right)\right].$



-   *Markov inequality* is another simple but important fact. If $E\left[\left|X\right|^{r}\right]$ exists,
        then
        $P\left(\left|X\right|>\epsilon\right)\leq E\left[\left|X\right|^{r}\right]/\epsilon^{r}$
        for all $r\geq1$. *Chebyshev inequality*
            $P\left(\left|X\right|>\epsilon\right)\leq E\left[X^{2}\right]/\epsilon^{2}$
            is a special case of the Markov inequality when $r=2$.

# Multivariate Random Variable

A bivariate random variable is a
    measurable function $X:\Omega\mapsto\mathbb{R}^{2}$, and more generally a multivariate random
    variable is a measurable function $X:\Omega\mapsto\mathbb{R}^{n}$.
    We can define the *joint CDF* as
    $F\left(x_{1},\ldots,x_{n}\right)=P\left(X_{1}\leq x_{1},\ldots,X_{n}\leq x_{n}\right)$.
    Joint PDF is defined similarly.

It is sufficient to introduce the joint distribution, conditional distribution
    and marginal distribution in the simple bivariate case, and these definitions
    can be extended to multivariate distributions. Suppose
    a bivariate random variable $(X,Y)$ has a joint density
    $f(\cdot,\cdot)$.
    The  *conditional density* can be roughly written as  $f\left(y|x\right)=f\left(x,y\right)/f\left(x\right)$ if we do not formally deal
    with the case $f(x)=0$.
    The *marginal density* $f\left(y\right)=\int f\left(x,y\right)dx$ integrates out
    the coordinate that is not interested.

## Independence

In a probability space $(\Omega, \mathcal{F}, P)$, for two events $A_1,A_2\in \mathcal{F}$ the *conditional probability* is
$$P\left(A_1|A_2\right)=\frac{P\left(A_1 A_2\right)}{P\left(A_2\right)}$$
if $P(A_2) \neq 0$. If $P(A_2) =0$, the conditional probability can still be valid
    in some cases,
    but we need to introduce the *dominance* between two measures,
    which I choose not to do at this time.
    In the definition of conditional probability, $A_2$ plays
    the role of the outcome space  so that
     $P(A_1 A_2)$ is standardized by the total mass $P(A_2)$.

Since $A_1$ and $A_2$ are symmetric, we also have $P(A_1 A_2) = P(A_2|A_1)P(A_1)$.
    It implies
    $$P(A_1 | A_2)=\frac{P\left(A_2| A_1\right)P\left(A_1\right)}{P\left(A_2\right)}$$
    This formula is the well-known *Bayes' Theorem*. It is particularly important in
    decision theory.


We say two events $A_1$ and $A_2$ are *independent* if $P(A_1A_2) = P(A_1)P(A_2)$.
    If $P(A_2) \neq 0$, it is equivalent to $P(A_1 | A_2 ) = P(A_1)$.
    In words, knowing $A_2$ does not change the probability of $A_1$.

Regarding the independence of two random variables,
    $X$ and $Y$ are *independent* if
    $P\left(X\in B_1,Y\in B_2\right)=P\left(X\in B_1\right)P\left(Y\in B_2\right)$
    for any two Borel sets $B_1$ and $B_2$.

If $X$ and $Y$ are independent, $E[XY] = E[X]E[Y]$.

**Application**: (Chebyshev law of large numbers) If
    $X_{1},X_{2},\ldots,X_{n}$ are independent, and they have the same mean
    $0$ and variance $\sigma^{2}<\infty$. Let
    $Z_{n}=\frac{1}{n}\sum_{i=1}^{n} X_{i}$. Then the
    probability $P\left(\left|Z_{n}\right|>\epsilon\right)\to0$ as
    $n\to\infty$,

## Law of Iterated Expectations


Given a probability space $\left(\Omega,\mathcal{F},P\right)$, a sub
        $\sigma$-algebra $\mathcal{G}\subset\mathcal{F}$ and a
        $\mathcal{F}$-measurable function $X$ with $E\left|X\right|<\infty$,
        the *conditional expectation* $E\left[X|\mathcal{G}\right]$ is
        defined as a $\mathcal{G}$-measurable function such that
        $\int_{A}XdP=\int_{A}E\left[X|\mathcal{G}\right]dP$ for all
        $A\in\mathcal{G}$. *Law of iterated expectation* is a trivial fact if we take
        $A=\Omega$.



In the bivariate case, if the conditional density exists, the conditional expectation can be computed as
        $E\left[Y|X\right]=\int yf\left(y|X\right)dy$.
The law of iterated expectation implies $E\left[E\left[Y|X\right]\right]=E\left[Y\right]$.


Below are some properties of conditional expectations

1.  $E\left[E\left[Y|X_{1},X_{2}\right]|X_{1}\right]=E\left[Y|X_{1}\right];$
2.  $E\left[E\left[Y|X_{1}\right]|X_{1},X_{2}\right]=E\left[Y|X_{1}\right];$
3.  $E\left[h\left(X\right)Y|X\right]=h\left(X\right)E\left[Y|X\right].$
