#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding utf8x
\fontencoding T1
\font_roman "palatino" "default"
\font_sans "default" "default"
\font_typewriter "mathpazo" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing single
\use_hyperref false
\pdf_bookmarks false
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks true
\pdf_pdfborder false
\pdf_colorlinks true
\pdf_backref section
\pdf_pdfusetitle false
\pdf_quoted_options "urlcolor=urlcolor,linkcolor=linkcolor,citecolor=citecolor,"
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Least Squares
\end_layout

\begin_layout Author
Zhentao Shi
\end_layout

\begin_layout Standard
Notation: 
\begin_inset Formula $y_{i}$
\end_inset

 is a scalar, and 
\begin_inset Formula $x_{i}$
\end_inset

 is a 
\begin_inset Formula $K\times1$
\end_inset

 vector.
 
\begin_inset Formula $Y$
\end_inset

 is an 
\begin_inset Formula $n\times1$
\end_inset

 vector, and 
\begin_inset Formula $X$
\end_inset

 is an 
\begin_inset Formula $n\times K$
\end_inset

 matrix.
\end_layout

\begin_layout Section
Algebra of Least Squares 
\begin_inset CommandInset label
LatexCommand label
name "algebra-of-least-squares"

\end_inset


\end_layout

\begin_layout Subsection
OLS estimator 
\begin_inset CommandInset label
LatexCommand label
name "ols-estimator"

\end_inset


\end_layout

\begin_layout Standard
As we have learned from the linear project model, the projection coefficient
 
\begin_inset Formula $\beta$
\end_inset

 in the regression 
\begin_inset Formula 
\[
\begin{aligned}y_{i} & =x'_{i}\beta+e_{i}\end{aligned}
\]

\end_inset

can be written as 
\begin_inset Formula $\beta=\left(E\left[x_{i}x_{i}'\right]\right)^{-1}E\left[x_{i}y_{i}\right].$
\end_inset

 While population is something imaginary, in reality we possess a sample
 of 
\begin_inset Formula $n$
\end_inset

 observations 
\begin_inset Formula $\left(y_{i},x_{i}\right)_{i=1}^{n}$
\end_inset

.
 We thus replace the population mean 
\begin_inset Formula $E\left[\cdot\right]$
\end_inset

 by the sample mean, and the resulting estimator is 
\begin_inset Formula 
\[
\widehat{\beta}=\left(\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}'\right)^{-1}\frac{1}{n}\sum_{i=1}^{n}x_{i}y_{i}=\left(X'X\right)^{-1}X'y.
\]

\end_inset

This is one way to motivate the OLS estimator.
\end_layout

\begin_layout Standard
Alternatively, we can derive the OLS estimator from minimizing the sum of
 squared residuals 
\begin_inset Formula 
\[
Q\left(\beta\right)=\sum_{i=1}^{n}\left(y_{i}-x_{i}'\beta\right)^{2}=\left(Y-X\beta\right)'\left(Y-X\beta\right).
\]

\end_inset

Solve the first-order condition 
\begin_inset Formula 
\[
\frac{\partial}{\partial\beta}Q\left(\beta\right)=-2X'\left(Y-X\beta\right)=0.
\]

\end_inset

This necessary condition for optimality gives exactly the same 
\begin_inset Formula $\widehat{\beta}$
\end_inset

.
 Moreover, the second-order condition 
\begin_inset Formula 
\[
\frac{\partial^{2}}{\partial\beta\partial\beta'}Q\left(\beta\right)=2X'X
\]

\end_inset

shows that 
\begin_inset Formula $Q\left(\beta\right)$
\end_inset

 is convex in 
\begin_inset Formula $\beta$
\end_inset

 due to the positive semi-definite matrix 
\begin_inset Formula $X'X$
\end_inset

.
 (
\begin_inset Formula $Q\left(\beta\right)$
\end_inset

 is strictly convex in 
\begin_inset Formula $\beta$
\end_inset

 if 
\begin_inset Formula $X'X$
\end_inset

 is positive definite.)
\end_layout

\begin_layout Standard
Here are some definitions and properties of the OLS estimator.
\end_layout

\begin_layout Itemize
Fitted value: 
\begin_inset Formula $\widehat{Y}=X\widehat{\beta}$
\end_inset

.
\end_layout

\begin_layout Itemize
Projector: 
\begin_inset Formula $P_{X}=X\left(X'X\right)^{-1}X$
\end_inset

; Annihilator: 
\begin_inset Formula $M_{X}=I_{n}-P_{X}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
\begin_inset Formula $P_{X}M_{X}=M_{X}P_{X}=0$
\end_inset

.
 
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $AA=A$
\end_inset

, we call it an idempotent matrix.
 Both 
\begin_inset Formula $P_{X}$
\end_inset

 and 
\begin_inset Formula $M_{X}$
\end_inset

 are idempotent.
\end_layout

\begin_layout Itemize
Residual: 
\begin_inset Formula $\widehat{e}=Y-\widehat{Y}=Y-X\widehat{\beta}=Y-X(X'X)^{-1}X'Y=(I-P_{X})Y=M_{X}Y=M_{X}\left(X\beta+e\right)=M_{X}e$
\end_inset

.
 (Note: 
\begin_inset Formula $M_{X}X=(I-P_{X})X=X-X=0\Longrightarrow M_{X}X\beta=0$
\end_inset

)
\end_layout

\begin_layout Itemize
\begin_inset Formula $X'\widehat{e}=X'M_{X}e=0$
\end_inset

.
 (Note again 
\begin_inset Formula $X'M_{X}=0$
\end_inset

)
\end_layout

\begin_layout Itemize
\begin_inset Formula $\frac{1}{n}\sum_{i=1}^{n}\widehat{e}_{i}=0$
\end_inset

 if 
\begin_inset Formula $x_{i}$
\end_inset

 contains a constant.
\end_layout

\begin_deeper
\begin_layout Standard
(Justification: 
\begin_inset Formula $X'\widehat{e}=\left[\begin{array}{cccc}
1 & 1 & \cdots & 1\\
* & * & \cdots & *\\
\cdots & \cdots & \ddots & \vdots\\
* & * & \cdots & *
\end{array}\right]\left[\begin{array}{c}
\widehat{e}_{1}\\
\widehat{e}_{2}\\
\vdots\\
\widehat{e}_{n}
\end{array}\right]=\left[\begin{array}{c}
0\\
0\\
\vdots\\
0
\end{array}\right]$
\end_inset

 and the the first row implies 
\begin_inset Formula $\sum_{i=1}^{n}\widehat{e}_{i}=0$
\end_inset

.)
\end_layout

\end_deeper
\begin_layout Subsection
Goodness of Fit 
\begin_inset CommandInset label
LatexCommand label
name "goodness-of-fit"

\end_inset


\end_layout

\begin_layout Standard
The so-called 
\emph on
R-squared
\emph default
 is a popular measure of goodness-of-fit in the linear regression.
 R-squared is well defined only when a constant is included in the regressors.
 Let 
\begin_inset Formula $M_{\iota}=I_{n}-\frac{1}{n}\iota\iota'$
\end_inset

, where 
\begin_inset Formula $\iota$
\end_inset

 is an 
\begin_inset Formula $n\times1$
\end_inset

 vector of 1's.
 
\begin_inset Formula $M_{\iota}$
\end_inset

 is the 
\emph on
demeaner
\emph default
, in the sense that 
\begin_inset Formula $M_{\iota}\left(z_{1},\ldots,z_{n}\right)'=\left(z_{1}-\overline{z},\ldots,z_{n}-\overline{z}\right)'$
\end_inset

, where 
\begin_inset Formula $\bar{z}=\frac{1}{n}\sum_{i=1}^{n}z_{i}$
\end_inset

.
 For any 
\begin_inset Formula $X$
\end_inset

, we can decompose 
\begin_inset Formula $Y=P_{X}Y+M_{X}Y=\widehat{Y}+\widehat{e}$
\end_inset

.
 The total variation is 
\begin_inset Formula 
\[
Y'M_{\iota}Y=\left(\widehat{Y}+\widehat{e}\right)'M_{\iota}\left(\widehat{Y}+\widehat{e}\right)=\widehat{Y}'M_{\iota}\widehat{Y}+2\widehat{Y}'M_{\iota}\widehat{e}+\widehat{e}'M_{\iota}\widehat{e}=\widehat{Y}'M_{\iota}\widehat{Y}+\widehat{e}'\widehat{e}
\]

\end_inset

where the last equality follows by 
\begin_inset Formula $M_{\iota}\widehat{e}=\widehat{e}$
\end_inset

 as 
\begin_inset Formula $\frac{1}{n}\sum_{i=1}^{n}\widehat{e}_{i}=0$
\end_inset

, and 
\begin_inset Formula $\widehat{Y}'\widehat{e}=Y'P_{X}M_{X}e=0$
\end_inset

.
 R-squared is defined as 
\begin_inset Formula $\widehat{Y}'M_{\iota}\widehat{Y}/Y'M_{\iota}Y$
\end_inset

.
\end_layout

\begin_layout Subsection
Frish-Waugh-Lovell Theorem
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "frish-waugh-lovell-theorem"

\end_inset


\end_layout

\begin_layout Standard
The Frish-Waugh-Lovell (FWL) theorem is an algebraic fact about the formula
 of a subvector of the OLS estimator.
 To derive the FWL theorem We need to use the inverse of partitioned matrix.
 For a positive definite symmetric matrix 
\begin_inset Formula $A=\begin{pmatrix}A_{11} & A_{12}\\
A_{12}' & A_{22}
\end{pmatrix}$
\end_inset

, the inverse can be written as 
\begin_inset Formula 
\[
A^{-1}=\begin{pmatrix}\left(A_{11}-A_{12}A_{22}^{-1}A_{12}'\right)^{-1} & -\left(A_{11}-A_{12}A_{22}^{-1}A_{12}'\right)^{-1}A_{12}A_{22}^{-1}\\
-A_{22}^{-1}A_{12}'\left(A_{11}-A_{12}A_{22}^{-1}A_{12}'\right)^{-1} & \left(A_{22}-A_{12}'A_{11}^{-1}A_{12}\right)^{-1}
\end{pmatrix}.
\]

\end_inset

In our context of OLS estimator, let 
\begin_inset Formula $X=\left(\begin{array}{cc}
X_{1} & X_{2}\end{array}\right)$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\widehat{\beta} & =\begin{pmatrix}\widehat{\beta}_{1}\\
\widehat{\beta}_{2}
\end{pmatrix}=(X'X)^{-1}X'Y\\
 & =\left(\begin{pmatrix}X_{1}'\\
X_{2}'
\end{pmatrix}\begin{pmatrix}X_{1} & X_{2}\end{pmatrix}\right)^{-1}\begin{pmatrix}X_{1}'Y\\
X_{2}'Y
\end{pmatrix}\\
 & =\begin{pmatrix}X_{1}'X_{1} & X_{1}'X_{2}\\
X_{2}'X_{1} & X_{2}'X_{2}
\end{pmatrix}^{-1}\begin{pmatrix}X_{1}'Y\\
X_{2}'Y
\end{pmatrix}\\
 & =\begin{pmatrix}\left(X_{1}'M_{X_{2}}'X_{1}\right)^{-1} & -\left(X_{1}'M_{X_{2}}'X_{1}\right)^{-1}X_{1}'X_{2}\left(X_{2}'X_{2}\right)^{-1}\\
* & *
\end{pmatrix}\begin{pmatrix}X_{1}'Y\\
X_{2}'Y
\end{pmatrix}.
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The subvector
\begin_inset Formula 
\begin{align*}
\widehat{\beta}_{1} & =\left(X_{1}'M_{X_{2}}'X_{1}\right)^{-1}X_{1}'Y-\left(X_{1}'M_{X_{2}}'X_{1}\right)^{-1}X_{1}'X_{2}\left(X_{2}'X_{2}\right)^{-1}X_{2}'Y\\
 & =\left(X_{1}'M_{X_{2}}'X_{1}\right)^{-1}\left(X_{1}'Y-X_{1}'P_{X_{2}}Y\right)\\
 & =\left(X_{1}'M_{X_{2}}'X_{1}\right)^{-1}X_{1}'M_{X_{2}}Y.
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Notice that 
\begin_inset Formula $\widehat{\beta}_{1}$
\end_inset

 can be obtained by the following:
\end_layout

\begin_layout Enumerate
Regress 
\begin_inset Formula $y$
\end_inset

 on 
\begin_inset Formula $X_{2}$
\end_inset

, obtain residuals 
\begin_inset Formula $\tilde{e}_{2}$
\end_inset

;
\end_layout

\begin_layout Enumerate
Regress 
\begin_inset Formula $X_{1}$
\end_inset

 on 
\begin_inset Formula $X_{2}$
\end_inset

, obtain residuals 
\begin_inset Formula $\tilde{X}_{2}$
\end_inset

;
\end_layout

\begin_layout Enumerate
Regress 
\begin_inset Formula $\tilde{e}_{2}$
\end_inset

 on 
\begin_inset Formula $\tilde{X}_{2}$
\end_inset

, obtain OLS estimates 
\begin_inset Formula $\widehat{\beta}_{1}$
\end_inset

.
\end_layout

\begin_layout Standard
Similar derivation can also be carried out in the population linear projection.
 See Hansen (2019) Chapter 2.22-23.
\end_layout

\begin_layout Section
Statistical Properties of Least Squares
\begin_inset CommandInset label
LatexCommand label
name "statistical-properties-of-least-squares"

\end_inset


\end_layout

\begin_layout Standard
In this section we return to the classical statistical framework under restricti
ve distributional assumption 
\begin_inset Formula $y_{i}|x_{i}\sim N\left(x_{i}'\beta,\gamma\right)$
\end_inset

, where 
\begin_inset Formula $\gamma=\sigma^{2}$
\end_inset

.
\end_layout

\begin_layout Subsection
Maximum Likelihood Estimation
\begin_inset CommandInset label
LatexCommand label
name "maximum-likelihood-estimation"

\end_inset


\end_layout

\begin_layout Standard
The 
\emph on
conditional
\emph default
 likelihood of observing a 
\emph on
random sample
\emph default
 
\begin_inset Formula $\left(y_{i},x_{i}\right)_{i=1}^{n}$
\end_inset

 is 
\begin_inset Formula 
\[
\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi\gamma}}\exp\left(-\frac{1}{2\gamma}\left(y_{i}-x_{i}'\beta\right)^{2}\right),
\]

\end_inset

and the (conditional) log-likelihood function is 
\begin_inset Formula 
\[
L\left(\beta,\gamma\right)=-\frac{n}{2}\log2\pi-\frac{n}{2}\log\gamma-\frac{1}{2\gamma}\sum_{i=1}^{n}\left(y_{i}-x_{i}'\beta\right)^{2}.
\]

\end_inset

The maximum likelihood estimator 
\begin_inset Formula $\widehat{\beta}_{MLE}$
\end_inset

 can be found using the FOC:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\partial}{\partial\beta}L\left(\beta,\gamma\right)=\frac{1}{2\gamma}\sum_{i=1}^{n}2x_{i}\left(y_{i}-x_{i}'\beta\right)^{2}=0.
\]

\end_inset

Rearranging the above equation in matrix form 
\begin_inset Formula $X'Y=X'X\widehat{\beta}_{MLE}$
\end_inset

, we explicitly solve 
\begin_inset Formula 
\[
\widehat{\beta}_{MLE}=(X'X)^{-1}X'Y.
\]

\end_inset

The maximum likelihood estimator (MLE) coincides with the OLS estimator.
 Similarly, another FOC gives 
\begin_inset Formula $\widehat{\gamma}_{\mathrm{MLE}}=\widehat{e}'\widehat{e}/n$
\end_inset

.
\end_layout

\begin_layout Subsection
Classical Finite Sample Distribution
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "finite-sample-distribution"

\end_inset


\end_layout

\begin_layout Standard
We can show the finite-sample exact distribution of 
\begin_inset Formula $\widehat{\beta}$
\end_inset

 assuming the error term follows a Gaussian distribution.
 
\emph on
Finite sample distribution
\emph default
 means that the distribution holds for any 
\begin_inset Formula $n$
\end_inset

; it is in contrast to 
\emph on
asymptotic distribution
\emph default
, which is a large sample approximation to the finite sample distribution.
 Let the 
\begin_inset Quotes eld
\end_inset

error term
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $e_{i}=y_{i}-x_{i}'\beta$
\end_inset

, we have 
\begin_inset Formula $e_{i}|x_{i}=y_{i}|x_{i}-x_{i}'\beta\sim N\left(0,\gamma\right)$
\end_inset

.
 Since the conditional distribution of 
\begin_inset Formula $e_{i}$
\end_inset

 on 
\begin_inset Formula $x_{i}$
\end_inset

 is invariant with 
\begin_inset Formula $x_{i}$
\end_inset

, the discrepancy 
\begin_inset Formula $e_{i}$
\end_inset

 is statistical independent of 
\begin_inset Formula $x_{i}$
\end_inset

.
 Assume The estimator
\begin_inset Formula 
\[
\widehat{\beta}=\left(X'X\right)^{-1}X'Y=\left(X'X\right)^{-1}X'\left(X'\beta+e\right)=\beta+\left(X'X\right)^{-1}X'e,
\]

\end_inset

and its conditional distribution can be written as 
\begin_inset Formula 
\begin{align*}
\widehat{\beta}|X & =\beta+\left(X'X\right)^{-1}X'e|X\\
 & \sim\beta+\left(X'X\right)^{-1}X'\cdot N\left(0_{n},\sigma^{2}\cdot I_{n}\right)\\
 & \sim N\left(\beta,\sigma^{2}\left(X'X\right)^{-1}X'X\left(X'X\right)^{-1}\right)\sim N\left(\beta,\sigma^{2}\left(X'X\right)^{-1}\right).
\end{align*}

\end_inset

The 
\begin_inset Formula $k$
\end_inset

-th element of the vector coefficient
\begin_inset Formula 
\[
\widehat{\beta}_{k}|X=\eta_{k}'\widehat{\beta}|X\sim N\left(\beta_{k},\sigma^{2}\eta_{k}'\left(X'X\right)^{-1}\eta_{k}\right)\sim N\left(\beta_{k},\sigma^{2}\left(X'X\right)_{kk}^{-1}\right),
\]

\end_inset

where 
\begin_inset Formula $\eta_{k}=\left(1\left\{ l=k\right\} \right)_{l=1,\ldots,K}$
\end_inset

 is the selector of the 
\begin_inset Formula $k$
\end_inset

-th element.
\end_layout

\begin_layout Standard
In reality, 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is an unknown parameter, and 
\begin_inset Formula 
\[
s^{2}=\widehat{e}'\widehat{e}/\left(n-K\right)=e'M_{X}e/\left(n-K\right)
\]

\end_inset

is an unbiased estimator of 
\begin_inset Formula $\sigma^{2}$
\end_inset

.
 Consider the 
\begin_inset Formula $t$
\end_inset

-statistic
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
T_{k} & =\frac{\widehat{\beta}_{k}-\beta_{k}}{\sqrt{s^{2}\left[\left(X'X\right)^{-1}\right]_{kk}}}\\
 & =\frac{\widehat{\beta}_{k}-\beta_{k}}{\sqrt{\sigma^{2}\left[\left(X'X\right)^{-1}\right]_{kk}}}\cdot\frac{\sqrt{\sigma^{2}}}{\sqrt{s^{2}}}\\
 & =\frac{\left(\widehat{\beta}_{k}-\beta_{k}\right)/\sqrt{\sigma^{2}\left[\left(X'X\right)^{-1}\right]_{kk}}}{\sqrt{\frac{e'}{\sigma}M_{X}\frac{e}{\sigma}/\left(n-K\right)}}.
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The numerator follows a standard normal, and the denominator follows 
\begin_inset Formula $\frac{1}{n-K}\chi^{2}\left(n-K\right)$
\end_inset

.
 Moreover, the numerator and the denominator are statistically independent
 (Basu's theorem).
 As a result, we conclude 
\begin_inset Formula $T_{k}\sim t\left(n-K\right)$
\end_inset

.
 This finite sample distribution is crucial when conducting statistical
 inference.
\end_layout

\begin_layout Subsection
Mean and Variance
\begin_inset CommandInset label
LatexCommand label
name "mean-and-variance"

\end_inset


\end_layout

\begin_layout Standard
Now we relax the normality assumption and statistical independence.
 Instead, we represent the regression model as 
\begin_inset Formula $y_{i}=x_{i}'\beta+e_{i}$
\end_inset

 and 
\begin_inset Formula 
\begin{align*}
E[e|X] & =0\\
\mathrm{var}\left[e|X\right] & =\sigma^{2}I_{n}.
\end{align*}

\end_inset

where the first condition is the 
\emph on
mean independence
\emph default
 assumption, and the second condition is the 
\emph on
homoskedasticity
\emph default
 assumption.
 These assumptions are about the first and second 
\emph on
moments
\emph default
 of 
\begin_inset Formula $e_{i}$
\end_inset

 conditional on 
\begin_inset Formula $x_{i}$
\end_inset

.
 Unlike the normality assumption, they do not restrict the 
\emph on
distribution
\emph default
 of 
\begin_inset Formula $e_{i}$
\end_inset

.
\end_layout

\begin_layout Itemize
Unbiasedness: 
\begin_inset Formula 
\[
E\left[\widehat{\beta}|X\right]=E\left[\left(X'X\right)^{-1}XY|X\right]=E\left[\left(X'X\right)^{-1}X\left(X'\beta+e\right)|X\right]=\beta.
\]

\end_inset

Unbiasedness does not rely on homoskedasticity.
 
\end_layout

\begin_layout Itemize
Variance: 
\begin_inset Formula 
\[
\begin{aligned}\mathrm{var}\left(\widehat{\beta}|X\right) & =E\left[\left(\widehat{\beta}-E\widehat{\beta}\right)\left(\widehat{\beta}-E\widehat{\beta}\right)'|X\right]\\
 & =E\left[\left(\widehat{\beta}-\beta\right)\left(\widehat{\beta}-\beta\right)'|X\right]\\
 & =E\left[\left(X'X\right)^{-1}X'ee'X\left(X'X\right)^{-1}|X\right]\\
 & =\left(X'X\right)^{-1}X'E\left[ee'|X\right]X\left(X'X\right)^{-1}\\
 & =\left(X'X\right)^{-1}X'\left(\sigma^{2}I_{n}\right)X\left(X'X\right)^{-1}\\
 & =\sigma^{2}\left(X'X\right)^{-1}.
\end{aligned}
\]

\end_inset

Homoskedasticity is essential in this derivation.
\end_layout

\begin_layout Standard

\series bold
Example
\series default
 (Heteroskedasticity) If 
\begin_inset Formula $e_{i}=x_{i}u_{i}$
\end_inset

, where 
\begin_inset Formula $x_{i}$
\end_inset

 is a scalar random variable, 
\begin_inset Formula $u_{i}$
\end_inset

 is independent of 
\begin_inset Formula $x_{i}$
\end_inset

, 
\begin_inset Formula $E\left[u_{i}\right]=0$
\end_inset

 and 
\begin_inset Formula $E\left[u_{i}^{2}\right]=\sigma^{2}$
\end_inset

.
 Then 
\begin_inset Formula $E\left[e_{i}|x_{i}\right]=0$
\end_inset

 but 
\begin_inset Formula $E\left[e_{i}^{2}|x_{i}\right]=\sigma^{2}x_{i}^{2}$
\end_inset

 is a function of 
\begin_inset Formula $x_{i}$
\end_inset

.
 We say 
\begin_inset Formula $e_{i}^{2}$
\end_inset

 is a heteroskedastic error.
\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Plain Layout
Notice that the above derivation only applies when 
\begin_inset Formula $\left(y_{i},x_{i}\right)$
\end_inset

 are independent across 
\begin_inset Formula $i$
\end_inset

.
 In time series the conditional zero mean condition (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:e_mean_zero"
plural "false"
caps "false"
noprefix "false"

\end_inset

) has to be strengthened to 
\begin_inset Formula $E\left[e_{i}|x_{1},\ldots,x_{n}\right]=0$
\end_inset

 for unbiasedness.
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Gauss-Markov Theorem
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "gauss-markov-theorem"

\end_inset


\end_layout

\begin_layout Standard
Gauss-Markov theorem justifies the OLS estimator as the efficient estimator
 among all linear unbiased ones.
 
\emph on
Efficient
\emph default
 here means that it enjoys the smallest variance in a family of estimators.
\end_layout

\begin_layout Standard
There are numerous linearly unbiased estimators.
 For example, 
\begin_inset Formula $\left(Z'X\right)^{-1}Z'y$
\end_inset

 for 
\begin_inset Formula $z_{i}=x_{i}^{2}$
\end_inset

 is unbiased because 
\begin_inset Formula $E\left[\left(Z'X\right)^{-1}Z'y\right]=E\left[\left(Z'X\right)^{-1}Z'\left(X\beta+e\right)\right]=\beta$
\end_inset

.
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $\tilde{\beta}=A'y$
\end_inset

 be a generic linear estimator, where 
\begin_inset Formula $A$
\end_inset

 is any 
\begin_inset Formula $n\times K$
\end_inset

 functions of 
\begin_inset Formula $X$
\end_inset

.
 As 
\begin_inset Formula 
\[
E\left[A'y|X\right]=E\left[A'\left(X\beta+e\right)|X\right]=A'X\beta.
\]

\end_inset

So the linearity and unbiasedness of 
\begin_inset Formula $\tilde{\beta}$
\end_inset

 implies 
\begin_inset Formula $A'X=I_{n}$
\end_inset

.
 Moreover, the variance 
\begin_inset Formula 
\[
\mbox{var}\left(A'y|X\right)=E\left[\left(A'y-\beta\right)\left(A'y-\beta\right)'|X\right]=E\left[A'ee'A|X\right]=\sigma^{2}A'A.
\]

\end_inset

Let 
\begin_inset Formula $C=A-X\left(X'X\right)^{-1}.$
\end_inset

 
\begin_inset Formula 
\[
\begin{aligned}A'A-\left(X'X\right)^{-1} & =\left(C+X\left(X'X\right)^{-1}\right)'\left(C+X\left(X'X\right)^{-1}\right)-\left(X'X\right)^{-1}\\
 & =C'C+\left(X'X\right)^{-1}X'C+C'X\left(X'X\right)^{-1}\\
 & =C'C,
\end{aligned}
\]

\end_inset

where the last equality follows as 
\begin_inset Formula 
\[
\left(X'X\right)^{-1}X'C=\left(X'X\right)^{-1}X'\left(A-X\left(X'X\right)^{-1}\right)=\left(X'X\right)^{-1}-\left(X'X\right)^{-1}=0.
\]

\end_inset

Therefore 
\begin_inset Formula $A'A-\left(X'X\right)^{-1}$
\end_inset

 is a positive semi-definite matrix.
 The variance of any 
\begin_inset Formula $\tilde{\beta}$
\end_inset

 is no smaller than the OLS estimator 
\begin_inset Formula $\widehat{\beta}$
\end_inset

.
\end_layout

\begin_layout Standard
Homoskedasticity is a restrictive assumption.
 Under homoskedasticity, 
\begin_inset Formula $\mathrm{var}\left(\widehat{\beta}\right)=\sigma^{2}\left(X'X\right)^{-1}$
\end_inset

.
 Popular estimator of 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is the sample mean of the residuals 
\begin_inset Formula $\widehat{\sigma}^{2}=\frac{1}{n}\widehat{e}'\widehat{e}$
\end_inset

 or the unbiased one 
\begin_inset Formula $s^{2}=\frac{1}{n-K}\widehat{e}'\widehat{e}$
\end_inset

.
 Under heteroskedasticity, Gauss-Markov theorem does not apply.
\end_layout

\end_body
\end_document
