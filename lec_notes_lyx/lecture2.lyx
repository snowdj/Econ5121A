#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble

\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{Lecture Notes}
\rhead{Zhentao Shi}
\end_preamble
\use_default_options false
\begin_modules
theorems-ams
theorems-ams-extended
\end_modules
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding auto
\fontencoding default
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing double
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date true
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
This version: 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
today
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Notation
\series default
: in this note, 
\begin_inset Formula $y$
\end_inset

 is a scale random variable, and 
\begin_inset Formula $x$
\end_inset

 is a 
\begin_inset Formula $K\times1$
\end_inset

 random vector.
\end_layout

\begin_layout Section
Conditional Expectation
\begin_inset CommandInset label
LatexCommand label
name "conditional-expectation-function"

\end_inset


\end_layout

\begin_layout Standard
A regression model can be written as 
\begin_inset Formula 
\[
y=m\left(x\right)+\epsilon,
\]

\end_inset

 where 
\begin_inset Formula $m(x)=E[y|x]$
\end_inset

 is called the 
\emph on
conditional mean function
\emph default
, and 
\begin_inset Formula $\epsilon=y-m\left(x\right)$
\end_inset

 is called the 
\emph on
regression error
\emph default
.
 Such an equation holds for 
\begin_inset Formula $\left(y,x\right)$
\end_inset

 that follows any joint distribution, as long as 
\begin_inset Formula $E\left[y|x\right]$
\end_inset

 exists.
 The error term 
\begin_inset Formula $\epsilon$
\end_inset

 satisfies these properties:
\end_layout

\begin_layout Itemize
\begin_inset Formula $E\left[\epsilon|x\right]=0$
\end_inset

, 
\end_layout

\begin_layout Itemize
\begin_inset Formula $E\left[\epsilon\right]=0$
\end_inset

, 
\end_layout

\begin_layout Itemize
\begin_inset Formula $E\left[h\left(x\right)\epsilon\right]=0$
\end_inset

, where 
\begin_inset Formula $h$
\end_inset

 is a function of 
\begin_inset Formula $x$
\end_inset

.
 
\end_layout

\begin_layout Standard
The last property implies that 
\begin_inset Formula $\epsilon$
\end_inset

 is uncorrelated with any function of 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Standard
If we are interested in predicting 
\begin_inset Formula $y$
\end_inset

 given 
\begin_inset Formula $x$
\end_inset

, then the conditional mean function 
\begin_inset Formula $E\left[y|x\right]$
\end_inset

 is 
\begin_inset Quotes eld
\end_inset

optimal
\begin_inset Quotes erd
\end_inset

 in terms of the 
\emph on
mean squared error
\emph default
 (MSE).
 
\end_layout

\begin_layout Standard
As 
\begin_inset Formula $y$
\end_inset

 is not a deterministic function of 
\begin_inset Formula $x$
\end_inset

, we cannot predict it with certainty.
 In order to evaluate different methods of prediction, we must therefore
 propose a criterion for comparison.
 For an arbitrary prediction method 
\begin_inset Formula $g\left(x\right)$
\end_inset

, we employ a 
\emph on
loss function
\emph default
 
\begin_inset Formula $L\left(y,g\left(x\right)\right)$
\end_inset

 to measure how wrong is the prediction, and the expected value of the loss
 function is called the 
\emph on
risk 
\begin_inset Formula $R\left(y,g\left(x\right)\right)$
\end_inset

.
 
\emph default
The 
\emph on
quadratic loss function
\emph default
 is defined as 
\begin_inset Formula 
\[
L\left(y,g\left(x\right)\right)=\left(y-g\left(x\right)\right)^{2},
\]

\end_inset

 and the corresponding risk 
\begin_inset Formula 
\[
R\left(y,g\left(x\right)\right)=E\left[\left(y-g\left(x\right)\right)^{2}\right]
\]

\end_inset

 is called the MSE.
 
\end_layout

\begin_layout Standard
Due to its operational convenience, MSE is one of the most widely used criterion.
 Under MSE, the conditional expectation function happens to be the best
 prediction method for 
\begin_inset Formula $y$
\end_inset

 given 
\begin_inset Formula $x$
\end_inset

.
 In other words, the conditional mean function 
\begin_inset Formula $m\left(x\right)$
\end_inset

 minimizes the MSE.
\end_layout

\begin_layout Standard
We can take a guess-and-verify this claim of optimality.
 For an arbitrary 
\begin_inset Formula $g\left(x\right)$
\end_inset

, the risk can be decomposed into three terms 
\begin_inset Formula 
\begin{eqnarray*}
 &  & E\left[\left(y-g\left(x\right)\right)^{2}\right]\\
 & = & E\left[\left(y-m\left(x\right)\right)^{2}\right]+2E\left[\left(y-m\left(x\right)\right)\left(m\left(x\right)-g\left(x\right)\right)\right]+E\left[\left(m\left(x\right)-g\left(x\right)\right)^{2}\right].
\end{eqnarray*}

\end_inset

The first term is irrelevant to 
\begin_inset Formula $g\left(x\right)$
\end_inset

.
 The second term 
\begin_inset Formula $2E\left[\epsilon\left(m\left(x\right)-g\left(x\right)\right)\right]=0$
\end_inset

 is again irrelevant of 
\begin_inset Formula $g\left(x\right)$
\end_inset

.
 The third term, obviously, is minimized at 
\begin_inset Formula $g\left(x\right)=m\left(x\right)$
\end_inset

.
\end_layout

\begin_layout Section
Linear Projection
\begin_inset CommandInset label
LatexCommand label
name "linear-projection-model"

\end_inset


\end_layout

\begin_layout Standard
As discussed in the previous section, we are interested in the conditional
 mean function 
\begin_inset Formula $m(x)$
\end_inset

.
 However, remind that 
\begin_inset Formula 
\[
m\left(x\right)=E\left[y|x\right]=\int yf\left(y|x\right)\mathrm{d}y
\]

\end_inset

 is a complex function of 
\begin_inset Formula $x$
\end_inset

, as it depends on the joint distribution of 
\begin_inset Formula $\left(y,x\right)$
\end_inset

.
 
\end_layout

\begin_layout Standard
A particular form of the conditional mean function is 
\begin_inset Formula 
\[
m\left(x\right)=x'\beta,
\]

\end_inset

a linear function of 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Remark*
The linear function is not as restrictive as one might thought.
 It can be used to generate some nonlinear (in random variables) effect
 if we re-define 
\begin_inset Formula $x$
\end_inset

.
 For example, if 
\begin_inset Formula 
\[
y=x_{1}\beta_{2}+x_{2}\beta_{2}+x_{1}x_{2}\beta_{3}+e,
\]

\end_inset

 then 
\begin_inset Formula $\frac{\partial}{\partial x_{1}}m\left(x_{1},x_{2}\right)=\beta_{1}+x_{2}\beta_{3}$
\end_inset

, which is nonlinear in 
\begin_inset Formula $x_{1}$
\end_inset

, while it is still linear in the parameter 
\begin_inset Formula $\beta$
\end_inset

 if we define a set of new regressors as 
\begin_inset Formula $\left(\tilde{x}_{1},\tilde{x}_{2},\tilde{x}_{3}\right)=\left(x_{1},x_{2},x_{1}x_{2}\right)$
\end_inset

.
 
\end_layout

\begin_layout Example*
If 
\begin_inset Formula $\begin{pmatrix}y\\
x
\end{pmatrix}\sim\mathrm{N}\left(\begin{pmatrix}\mu_{y}\\
\mu_{x}
\end{pmatrix},\begin{pmatrix}\sigma_{y}^{2} & \rho\sigma_{y}\sigma_{x}\\
\rho\sigma_{y}\sigma_{x} & \sigma_{x}^{2}
\end{pmatrix}\right)$
\end_inset

, then 
\begin_inset Formula 
\[
E\left[y|x\right]=\mu_{y}+\rho\frac{\sigma_{y}}{\sigma_{x}}\left(x-\mu_{x}\right)=\left(\mu_{y}-\rho\frac{\sigma_{y}}{\sigma_{x}}\mu_{x}\right)+\rho\frac{\sigma_{y}}{\sigma_{x}}x.
\]

\end_inset


\end_layout

\begin_layout Standard
Even though in general 
\begin_inset Formula $m\left(x\right)\neq x'\beta$
\end_inset

, the linear form 
\begin_inset Formula $x'\beta$
\end_inset

 is still useful as an approximation, as will be clear soon.
 Therefore, we may write the linear regression model, or the 
\emph on
linear projection model
\emph default
, as 
\begin_inset Formula 
\begin{eqnarray*}
y & = & x'\beta+e\\
E[xe] & = & 0,
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $e$
\end_inset

 is called the 
\emph on
projection error
\emph default
, to be distinguished from 
\begin_inset Formula $\varepsilon=y-m\left(x\right)$
\end_inset

.
 
\end_layout

\begin_layout Remark*
If a constant is included in 
\begin_inset Formula $x$
\end_inset

 as a regressor, we have 
\begin_inset Formula $E\left[e\right]=0$
\end_inset

.
\end_layout

\begin_layout Standard
The coefficient 
\begin_inset Formula $\beta$
\end_inset

 in the linear projection model has a straightforward closed-form.
 Multiplying 
\begin_inset Formula $x$
\end_inset

 on both sides and taking expectation, we have 
\begin_inset Formula $E[xy]=E[xx']\beta$
\end_inset

.
 If 
\begin_inset Formula $E[xx']$
\end_inset

 is invertible, we can explicitly solve 
\begin_inset Formula 
\[
\beta=\left(E\left[xx'\right]\right)^{-1}E\left[xy\right].
\]

\end_inset


\end_layout

\begin_layout Standard
Now we justify 
\begin_inset Formula $x'\beta$
\end_inset

 as an approximation to 
\begin_inset Formula $m\left(x\right)$
\end_inset

.
 Indeed, 
\begin_inset Formula $x'\beta$
\end_inset

 is the optimal 
\emph on
linear
\emph default
 predictor in terms of MSE; in other words, 
\begin_inset Formula 
\begin{equation}
\beta=\arg\min_{b\in\mathbb{R}^{K}}E\left[\left(y-x'b\right)^{2}\right].\label{eq:min_MSE}
\end{equation}

\end_inset

This fact can be verified by taking the first-order condition of the above
 minimization problem 
\begin_inset Formula $\frac{\partial}{\partial\beta}E\left[\left(y-x'\beta\right)^{2}\right]=2E\left[x\left(y-x'\beta\right)\right]=0.$
\end_inset


\end_layout

\begin_layout Standard
In the meantime, 
\begin_inset Formula $x'\beta$
\end_inset

 is also the best 
\emph on
linear
\emph default
 approximation to 
\begin_inset Formula $m(x)$
\end_inset

.
 If we replace 
\begin_inset Formula $y$
\end_inset

 in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:min_MSE"

\end_inset

) by 
\begin_inset Formula $m\left(x\right)$
\end_inset

, we solve the minimizer as 
\begin_inset Formula 
\[
\left(E\left[xx'\right]\right)^{-1}E\left[xm\left(x\right)\right]=\left(E\left[xx'\right]\right)^{-1}E\left[E\left[xy|x\right]\right]=\left(E\left[xx'\right]\right)^{-1}E\left[xy\right]=\beta.
\]

\end_inset

Therefore 
\begin_inset Formula $\beta$
\end_inset

 is also the best linear approximation to 
\begin_inset Formula $m\left(x\right)$
\end_inset

 in terms of MSE.
\end_layout

\begin_layout Subsection
Omitted Variable Bias
\begin_inset CommandInset label
LatexCommand label
name "omitted-variable-bias"

\end_inset


\end_layout

\begin_layout Standard
We write the 
\emph on
long regression
\emph default
 as 
\begin_inset Formula 
\[
y=x_{1}'\beta_{1}+x_{2}'\beta_{2}+\beta_{3}+e,
\]

\end_inset

 and the 
\emph on
short regression
\emph default
 as 
\begin_inset Formula 
\[
y=x_{1}'\gamma_{1}+\gamma_{2}+u.
\]

\end_inset

If 
\begin_inset Formula $\beta_{1}$
\end_inset

 in the long regression is the parameter of interest, omitting 
\begin_inset Formula $x_{2}$
\end_inset

 as in the short regression will render 
\emph on
omitted variable bias
\emph default
 (meaning 
\begin_inset Formula $\gamma_{1}\neq\beta_{1}$
\end_inset

) unless 
\begin_inset Formula $x_{1}$
\end_inset

 and 
\begin_inset Formula $x_{2}$
\end_inset

 are uncorrelated.
\end_layout

\begin_layout Standard
We first demean all the variables in the two regressions, which is equivalent
 as if we project out the effect of the constant.
 The long regression becomes 
\begin_inset Formula 
\[
\begin{aligned}\tilde{y} & = & \tilde{x}_{1}'\beta_{1}+\tilde{x}_{2}'\beta_{2}+\tilde{e}\end{aligned}
,
\]

\end_inset

 and the short regression becomes 
\begin_inset Formula 
\[
\tilde{y}=\tilde{x}_{1}'\gamma_{1}+\tilde{u},
\]

\end_inset

 where 
\emph on
tilde
\emph default
 denotes the demeaned variable.
 
\end_layout

\begin_layout Standard
After demeaning, the cross-moment equals to the covariance.
 The short regression coefficient 
\begin_inset Formula 
\begin{eqnarray*}
\gamma_{1} & = & \left(E\left[\tilde{x}_{1}\tilde{x}_{1}'\right]\right)^{-1}E\left[\tilde{x}_{1}\tilde{y}\right]\\
 & = & \left(E\left[\tilde{x}_{1}\tilde{x}_{1}'\right]\right)^{-1}E\left[\tilde{x}_{1}\left(\tilde{x}_{1}'\beta_{1}+\tilde{x}_{2}'\beta_{2}+e\right)\right]\\
 & = & \beta_{1}+\left(E\left[\tilde{x}_{1}\tilde{x}_{1}'\right]\right)^{-1}E\left[\tilde{x}_{1}\tilde{x}_{2}'\right]\beta_{2}.
\end{eqnarray*}

\end_inset

Therefore, 
\begin_inset Formula $\gamma_{1}=\beta_{1}$
\end_inset

 if and only if 
\begin_inset Formula $E\left[\tilde{x}_{1}\tilde{x}_{2}'\right]\beta_{2}=0$
\end_inset

, which demands either 
\begin_inset Formula $E\left[\tilde{x}_{1}\tilde{x}_{2}'\right]=0$
\end_inset

 or 
\begin_inset Formula $\beta_{2}=0$
\end_inset

.
\end_layout

\begin_layout Standard
Obviously we prefer to run the long regression to attain 
\begin_inset Formula $\beta_{1}$
\end_inset

 if possible.
 However, sometimes 
\begin_inset Formula $x_{2}$
\end_inset

 is simply unobservable so the long regression is infeasible.
 When only the short regression is available, in some cases we are able
 to sign the bias, meaning that we know whether 
\begin_inset Formula $\gamma_{1}$
\end_inset

 is bigger or smaller than 
\begin_inset Formula $\beta_{1}$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Subsection
Subvector Regression
\begin_inset CommandInset label
LatexCommand label
name "subvector-regression"

\end_inset


\end_layout

\begin_layout Plain Layout
Sometimes we are interested in a subvector of 
\begin_inset Formula $\beta$
\end_inset

.
 For example, when we include an intercept and some variables in 
\begin_inset Formula $x$
\end_inset

, we are often more interested in the slope coefficients—the parameters
 associated with the random regressors—as they represent the size of effect
 of these explanatory factors.
 In such a regression 
\begin_inset Formula 
\[
y=\beta_{1}+x'\beta_{2}+e,
\]

\end_inset

we take an expectation to get 
\begin_inset Formula $E\left[y\right]=\beta_{1}+E\left[x\right]'\beta_{2}.$
\end_inset

 Take the difference of the two equations, 
\begin_inset Formula 
\[
y-E\left[y\right]=\left(x-E\left[x\right]\right)'\beta_{2}.
\]

\end_inset

Therefore, we can explicitly solve 
\begin_inset Formula $\beta_{2}$
\end_inset

 as 
\begin_inset Formula 
\[
\beta_{2}=\left(E\left[\left(x-E\left[x\right]\right)\left(x-E\left[x\right]\right)'\right]\right)^{-1}E\left[\left(x-E\left[x\right]\right)\left(y-E\left[y\right]\right)\right]=\left(\mbox{var}\left(x\right)\right)^{-1}\mbox{cov}\left(x,y\right),
\]

\end_inset

This is a special case of the subvector regression.
\end_layout

\begin_layout Plain Layout
To discuss the general case, we need to know 
\emph on
the formula of the inverse of a partitioned matrix
\emph default
.
 If 
\begin_inset Formula $Q=\begin{pmatrix}Q_{11} & Q_{12}\\
Q_{21} & Q_{22}
\end{pmatrix}$
\end_inset

 is a symmetric and positive definite matrix, then 
\begin_inset Formula 
\[
Q^{-1}=\begin{pmatrix}\left(Q_{11}-Q_{12}Q_{22}Q_{21}\right)^{-1} & -\left(Q_{11}-Q_{12}Q_{22}Q_{21}\right)^{-1}Q_{12}Q_{22}^{-1}\\
-\left(Q_{22}-Q_{21}Q_{11}Q_{12}\right)^{-1}Q_{21}Q_{11}^{-1} & \left(Q_{22}-Q_{21}Q_{11}Q_{12}\right)^{-1}
\end{pmatrix}.
\]

\end_inset


\end_layout

\begin_layout Plain Layout
We apply the above formulate to the expression of 
\begin_inset Formula $\beta$
\end_inset

, and we obtain 
\begin_inset Formula $\beta_{1}=A_{11\cdot2}^{-1}A_{1y\cdot2}$
\end_inset

, where 
\begin_inset Formula 
\begin{align*}
A_{11\cdot2} & =E\left[x_{1}x_{1}'\right]-E\left[x_{1}x_{2}'\right]\left(E\left[x_{2}x_{2}'\right]\right)^{-1}E\left[x_{2}x_{1}'\right]\\
A_{1y\cdot2} & =E\left[x_{1}y\right]-E\left[x_{1}x_{2}'\right]\left(E\left[x_{2}x_{2}'\right]\right)^{-1}E\left[x_{2}y\right].
\end{align*}

\end_inset

This is a brutal force approach for the explicit expression of the subvector
 
\begin_inset Formula $\beta_{1}$
\end_inset

.
\end_layout

\begin_layout Plain Layout
Alternatively, we can proceed in two steps.
 First, we run a multiple regression
\begin_inset Foot
status open

\begin_layout Plain Layout
We do allow 
\begin_inset Formula $x_{1}$
\end_inset

 to be a vector.
 However, one may find it is easier to consider the special case that 
\begin_inset Formula $x_{1}$
\end_inset

 is a scalar random variable.
\end_layout

\end_inset

 
\begin_inset Formula 
\begin{eqnarray*}
x_{1} & = & x_{2}'\gamma+u\\
E\left[x_{2}u\right] & = & 0
\end{eqnarray*}

\end_inset

so that the regressor error
\begin_inset Formula 
\[
u=x_{1}-x_{2}'\gamma=x_{1}-x_{2}'\left(E\left[x_{2}x_{2}'\right]\right)^{-1}E\left[x_{2}x_{1}'\right]=x_{1}-E\left[x_{1}x_{2}'\right]\left(E\left[x_{2}x_{2}'\right]\right)^{-1}x_{2}.
\]

\end_inset

 We then run a simple regression of 
\begin_inset Formula $y$
\end_inset

 on 
\begin_inset Formula $u$
\end_inset

, and the coefficient is 
\begin_inset Formula 
\[
\theta=\left(E\left[uu'\right]\right)^{-1}E\left[u'y\right].
\]

\end_inset

The nominator is 
\begin_inset Formula 
\[
E\left[u'y\right]=E\left[x_{1}y\right]-E\left[x_{1}x_{2}'\right]\left(E\left[x_{2}x_{2}'\right]\right)^{-1}E\left[x_{2}y\right]=A_{1y\cdot2}
\]

\end_inset

and the denominator is 
\begin_inset Formula 
\[
E\left[uu'\right]=E\left[\left(x_{1}-E\left[x_{1}x_{2}'\right]\left(E\left[x_{2}x_{2}'\right]\right)^{-1}x_{2}\right)\left(x_{1}-E\left[x_{1}x_{2}'\right]\left(E\left[x_{2}x_{2}'\right]\right)^{-1}x_{2}\right)'\right]=A_{11\cdot2}.
\]

\end_inset

It turns out 
\begin_inset Formula $\beta_{2}=\theta$
\end_inset

.
\end_layout

\begin_layout Plain Layout
While we can derive the expression of 
\begin_inset Formula $\beta_{1}$
\end_inset

 as a subvector of 
\begin_inset Formula $\beta$
\end_inset

, why do we come up with the two-step derivation? The latter makes clear
 that the coefficient represents the 
\emph on
partial effect
\emph default
 of the associate random variable.
\end_layout

\end_inset


\end_layout

\end_body
\end_document
