#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options false
\begin_modules
theorems-ams
\end_modules
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding utf8x
\fontencoding T1
\font_roman "palatino" "default"
\font_sans "default" "default"
\font_typewriter "mathpazo" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing single
\use_hyperref true
\pdf_bookmarks false
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks false
\pdf_backref section
\pdf_pdfusetitle false
\pdf_quoted_options "urlcolor=urlcolor,linkcolor=linkcolor,citecolor=citecolor,"
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Regression and Projection
\end_layout

\begin_layout Author
Zhentao Shi
\end_layout

\begin_layout Standard

\series bold
Notation
\series default
: in this note, 
\begin_inset Formula $y$
\end_inset

 is a scale random variable, and 
\begin_inset Formula $x$
\end_inset

 is a 
\begin_inset Formula $K\times1$
\end_inset

 random vector.
\end_layout

\begin_layout Section
Conditional Expectation
\begin_inset CommandInset label
LatexCommand label
name "conditional-expectation"

\end_inset


\end_layout

\begin_layout Standard
We view the regression as a problem of supervised learning.
 Supervised learning uses a function of 
\begin_inset Formula $x$
\end_inset

, say, 
\begin_inset Formula $g\left(x\right)$
\end_inset

, to predict 
\begin_inset Formula $y$
\end_inset

.
 
\begin_inset Formula $x$
\end_inset

 cannot perfectly predict 
\begin_inset Formula $y$
\end_inset

; otherwise their relationship is deterministic.
 The prediction error
\begin_inset Formula 
\[
e=y-g\left(x\right)
\]

\end_inset

 depends on the choice of 
\begin_inset Formula $g$
\end_inset

.
 There are numerous possible choices of 
\begin_inset Formula $g$
\end_inset

.
 Which one is the best? To answer this question, we need to decide a criterion
 to compare different 
\begin_inset Formula $g$
\end_inset

.
 Such a criterion is called the 
\emph on
loss function
\emph default
 
\begin_inset Formula $L\left(y,g\left(x\right)\right)$
\end_inset

.
 A particularly convenient one is the 
\emph on
quadratic loss
\emph default
, defined as 
\begin_inset Formula 
\[
L\left(y,g\left(x\right)\right)=\left(y-g\left(x\right)\right)^{2}.
\]

\end_inset

Since the data are random, we average the loss function across the joint
 distribution of 
\begin_inset Formula $\left(y,x\right)$
\end_inset

 as 
\begin_inset Formula $R\left(y,g\left(x\right)\right)=E\left[L\left(y,g\left(x\right)\right)\right]$
\end_inset

, which is called the 
\emph on
risk
\emph default
.
 For the quadratic loss function, the corresponding risk 
\begin_inset Formula 
\[
R\left(y,g\left(x\right)\right):=E\left[L\left(y,g\left(x\right)\right)\right]=E\left[\left(y-g\left(x\right)\right)^{2}\right],
\]

\end_inset

 is called the 
\emph on
mean squared error
\emph default
 (MSE).
 MSE is a deterministic quantity since the randomness is integrated out.
\end_layout

\begin_layout Standard
What is the optimal choice of 
\begin_inset Formula $g$
\end_inset

 if we aim to minimize the MSE?
\end_layout

\begin_layout Proposition
\begin_inset CommandInset label
LatexCommand label
name "prop:CEF"

\end_inset

 The conditional mean function 
\begin_inset Formula $m\left(x\right)=E\left[y|x\right]$
\end_inset

 minimizes MSE.
\end_layout

\begin_layout Standard
Before we prove the above proposition, we first discuss some properties
 of the conditional mean function.
 Obviously
\begin_inset Formula 
\[
y=m\left(x\right)+\left(y-m\left(x\right)\right)=m\left(x\right)+\epsilon,
\]

\end_inset

 where 
\begin_inset Formula $\epsilon:=y-m\left(x\right)$
\end_inset

 is called the 
\emph on
regression error
\emph default
.
 This equation holds for 
\begin_inset Formula $\left(y,x\right)$
\end_inset

 following any joint distribution, as long as 
\begin_inset Formula $E\left[y|x\right]$
\end_inset

 exists.
 The error term 
\begin_inset Formula $\epsilon$
\end_inset

 satisfies these properties:
\end_layout

\begin_layout Itemize
\begin_inset Formula $E\left[\epsilon|x\right]=E\left[y-m\left(x\right)|x\right]=E\left[y|x\right]-m(x)=0$
\end_inset

,
\end_layout

\begin_layout Itemize
\begin_inset Formula $E\left[\epsilon\right]=E\left[E\left[\epsilon|x\right]\right]=E(0)=0$
\end_inset

,
\end_layout

\begin_layout Itemize
For any function 
\begin_inset Formula $h\left(x\right)$
\end_inset

, we have 
\begin_inset Formula $E\left[h\left(x\right)\epsilon\right]=E\left[E\left[h\left(x\right)\epsilon|x\right]\right]=E\left[h(x)E\left[\epsilon|x\right]\right]=0$
\end_inset

.
\end_layout

\begin_layout Standard
The last property implies that 
\begin_inset Formula $\epsilon$
\end_inset

 is uncorrelated with any function of 
\begin_inset Formula $x$
\end_inset

.
 In particular, when 
\begin_inset Formula $h$
\end_inset

 is the identity function 
\begin_inset Formula $h\left(x\right)=x$
\end_inset

, we have 
\begin_inset Formula $E\left[x\epsilon\right]=\mathrm{cov}\left(x,\epsilon\right)=0$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Proof
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Proof of Proposition 
\begin_inset CommandInset ref
LatexCommand ref
reference "prop:CEF"
plural "false"
caps "false"
noprefix "false"

\end_inset


\end_layout

\end_inset

 The optimality of the conditional mean can be confirmed by 
\begin_inset Quotes eld
\end_inset

guess-and-verify.
\begin_inset Quotes erd
\end_inset

 For an arbitrary 
\begin_inset Formula $g\left(x\right)$
\end_inset

, the MSE can be decomposed into three terms
\end_layout

\begin_layout Proof
\begin_inset Formula 
\[
\begin{aligned}E\left[\left(y-g\left(x\right)\right)^{2}\right] & =E\left[\left(y-m(x)+m(x)-g(x)\right)^{2}\right]\\
 & =E\left[\left(y-m\left(x\right)\right)^{2}\right]+2E\left[\left(y-m\left(x\right)\right)\left(m\left(x\right)-g\left(x\right)\right)\right]+E\left[\left(m\left(x\right)-g\left(x\right)\right)^{2}\right].
\end{aligned}
\]

\end_inset

The first term is irrelevant to 
\begin_inset Formula $g\left(x\right)$
\end_inset

.
 The second term
\begin_inset Formula 
\[
\begin{aligned}2E\left[\left(y-m\left(x\right)\right)\left(m\left(x\right)-g\left(x\right)\right)\right] & =2E\left[\epsilon\left(m\left(x\right)-g\left(x\right)\right)\right]\\
 & =2E\left[E\left[\epsilon\left(m\left(x\right)-g\left(x\right)\right)|x\right]\right]\\
 & =2E\left[\left(m\left(x\right)-g\left(x\right)\right)E\left[\epsilon|x\right]\right]=0.
\end{aligned}
\]

\end_inset

is again irrelevant of 
\begin_inset Formula $g\left(x\right)$
\end_inset

.
 The third term, obviously, is minimized at 
\begin_inset Formula $g\left(x\right)=m\left(x\right)$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
Our perspective so far deviates from mainstream econometric textbooks, most
 of which assume that the dependent variable 
\begin_inset Formula $y$
\end_inset

 is generated as 
\begin_inset Formula $g\left(x\right)+\epsilon$
\end_inset

 for some unknown function 
\begin_inset Formula $g\left(\cdot\right)$
\end_inset

 and error term 
\begin_inset Formula $\epsilon$
\end_inset

 such that 
\begin_inset Formula $E\left[\epsilon|x\right]=0$
\end_inset

.
 Instead, we take a predictive framework regardless the data generating
 process.
 What we observe are 
\begin_inset Formula $y$
\end_inset

 and 
\begin_inset Formula $x$
\end_inset

 and we are solely interested in seeking a function 
\begin_inset Formula $g\left(x\right)$
\end_inset

 to predict 
\begin_inset Formula $y$
\end_inset

 as accurately as possible under the MSE criterion.
\end_layout

\begin_layout Section
Linear Projection
\begin_inset CommandInset label
LatexCommand label
name "linear-projection"

\end_inset


\end_layout

\begin_layout Standard
As discussed in the previous section, the conditional mean function 
\begin_inset Formula $m(x)$
\end_inset

 is the function that minimizes the MSE.
 However, 
\begin_inset Formula 
\[
m\left(x\right)=E\left[y|x\right]=\int yf\left(y|x\right)\mathrm{d}y
\]

\end_inset

is a complex function of 
\begin_inset Formula $x$
\end_inset

, as it depends on the joint distribution of 
\begin_inset Formula $\left(y,x\right)$
\end_inset

, which is almost always unknown in practice.
 Now let us make the prediction task even simpler.
 How about we minimize the MSE within all linear functions in the form of
 
\begin_inset Formula $g\left(x\right)=x'b$
\end_inset

 for 
\begin_inset Formula $b\in\mathbb{R}^{K}$
\end_inset

? The minimization problem is
\begin_inset Formula 
\begin{equation}
\min_{b\in\mathbb{R}^{K}}E\left[\left(y-x'b\right)^{2}\right].\label{eq:linear_MSE}
\end{equation}

\end_inset

Take the first-order condition of the MSE 
\begin_inset Formula 
\[
\frac{\partial}{\partial b}E\left[\left(y-x'b\right)^{2}\right]=-2E\left[x\left(y-x'b\right)\right]=0.
\]

\end_inset

Rearrange the above equation and we solve the optimal 
\begin_inset Formula $b$
\end_inset

 as
\begin_inset Formula 
\[
\beta=\left(E\left[xx'\right]\right)^{-1}E\left[xy\right]
\]

\end_inset

 if 
\begin_inset Formula $E\left[xx'\right]$
\end_inset

 is invertible.
 The function 
\begin_inset Formula $x'\beta$
\end_inset

 is called the 
\emph on
best linear projection
\emph default
 of 
\begin_inset Formula $y$
\end_inset

 on 
\begin_inset Formula $x$
\end_inset

, and the vector 
\begin_inset Formula $\beta$
\end_inset

 is called the 
\emph on
linear projection coefficient
\emph default
.
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Remark
The linear function is not as restrictive as one might thought.
 It can be used to produce some nonlinear (in random variables) effect if
 we re-define 
\begin_inset Formula $x$
\end_inset

.
 For example, if 
\begin_inset Formula 
\[
y=x_{1}\beta_{1}+x_{2}\beta_{2}+x_{1}^{2}\beta_{3}+e,
\]

\end_inset

then 
\begin_inset Formula $\frac{\partial}{\partial x_{1}}m\left(x_{1},x_{2}\right)=\beta_{1}+2x_{1}\beta_{3}$
\end_inset

, which is nonlinear in 
\begin_inset Formula $x_{1}$
\end_inset

, while it is still linear in the parameter 
\begin_inset Formula $\beta=\left(\beta_{1},\beta_{2},\beta_{3}\right)$
\end_inset

 if we define a set of new regressors as 
\begin_inset Formula $\left(\tilde{x}_{1},\tilde{x}_{2},\tilde{x}_{3}\right)=\left(x_{1},x_{2},x_{1}^{2}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Remark
If 
\begin_inset Formula $\left(y,x\right)$
\end_inset

 is jointly normal in the form 
\begin_inset Formula $\begin{pmatrix}y\\
x
\end{pmatrix}\sim\mathrm{N}\left(\begin{pmatrix}\mu_{y}\\
\mu_{x}
\end{pmatrix},\begin{pmatrix}\sigma_{y}^{2} & \rho\sigma_{y}\sigma_{x}\\
\rho\sigma_{y}\sigma_{x} & \sigma_{x}^{2}
\end{pmatrix}\right)$
\end_inset

 where 
\begin_inset Formula $\rho$
\end_inset

 is the correlation coefficient, then 
\begin_inset Formula 
\[
E\left[y|x\right]=\mu_{y}+\rho\frac{\sigma_{y}}{\sigma_{x}}\left(x-\mu_{x}\right)=\left(\mu_{y}-\rho\frac{\sigma_{y}}{\sigma_{x}}\mu_{x}\right)+\rho\frac{\sigma_{y}}{\sigma_{x}}x,
\]

\end_inset

is a liner function of 
\begin_inset Formula $x$
\end_inset

.
 In this example, the conditional mean coincides with a linear function.
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Remark
Even though in general 
\begin_inset Formula $m\left(x\right)\neq x'\beta$
\end_inset

, the linear form 
\begin_inset Formula $x'\beta$
\end_inset

 is still useful in approximating 
\begin_inset Formula $m\left(x\right)$
\end_inset

.
 That is, 
\begin_inset Formula $\beta=\arg\min\limits _{b\in\mathbb{R}^{K}}E\left[\left(m(x)-x'b\right)^{2}\right]$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Proof
The first-order condition gives 
\begin_inset Formula $\frac{\partial}{\partial b}E\left[\left(m(x)-x'b\right)^{2}\right]=-2E[x(m(x)-x'b)]=0$
\end_inset

.
 Rearrange the terms and obtain 
\begin_inset Formula $E[x\cdot m(x)]=E[xx']b$
\end_inset

.
 When 
\begin_inset Formula $E[xx']$
\end_inset

 is invertible, we solve 
\begin_inset Formula 
\[
\left(E\left[xx'\right]\right){}^{-1}E[x\cdot m(x)]=\left(E\left[xx'\right]\right){}^{-1}E[E[xy|x]]=\left(E\left[xx'\right]\right){}^{-1}E[xy]=\beta.
\]

\end_inset

Thus 
\begin_inset Formula $\beta$
\end_inset

 is also the best linear approximation to 
\begin_inset Formula $m\left(x\right)$
\end_inset

 under MSE.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
We may rewrite the linear regression model, or the 
\emph on
linear projection model,
\emph default
 as
\begin_inset Formula 
\[
\begin{array}[t]{c}
y=x'\beta+e\\
E[xe]=0,
\end{array}
\]

\end_inset

where 
\begin_inset Formula $e=y-x'\beta$
\end_inset

 is called the 
\emph on
projection error
\emph default
, to be distinguished from 
\begin_inset Formula $\epsilon=y-m(x).$
\end_inset


\end_layout

\begin_layout Standard

\series bold
Exercise
\series default
: Show (a) 
\begin_inset Formula $E\left[xe\right]=0$
\end_inset

.
 (b) If 
\begin_inset Formula $x$
\end_inset

 contains a constant, then 
\begin_inset Formula $E\left[e\right]=0$
\end_inset

.
\end_layout

\begin_layout Subsection
Omitted Variable Bias 
\begin_inset CommandInset label
LatexCommand label
name "omitted-variable-bias"

\end_inset


\end_layout

\begin_layout Standard
We write the 
\emph on
long regression
\emph default
 as 
\begin_inset Formula 
\[
y=x_{1}'\beta_{1}+x_{2}'\beta_{2}+\beta_{3}+\varepsilon,
\]

\end_inset

and the 
\emph on
short regression
\emph default
 as 
\begin_inset Formula 
\[
y=x_{1}'\gamma_{1}+\gamma_{2}+u.
\]

\end_inset

If 
\begin_inset Formula $\beta_{1}$
\end_inset

 in the long regression is the parameter of interest, omitting 
\begin_inset Formula $x_{2}$
\end_inset

 as in the short regression will render 
\emph on
omitted variable bias
\emph default
 (meaning 
\begin_inset Formula $\gamma_{1}\neq\beta_{1}$
\end_inset

) unless 
\begin_inset Formula $x_{1}$
\end_inset

 and 
\begin_inset Formula $x_{2}$
\end_inset

 are uncorrelated.
\end_layout

\begin_layout Standard
We first demean all the variables in the two regressions, which is equivalent
 as if we project out the effect of the constant.
 The long regression becomes 
\begin_inset Formula 
\[
\tilde{y}=\tilde{x}_{1}'\beta_{1}+\tilde{x}_{2}'\beta_{2}+\tilde{\varepsilon},
\]

\end_inset

and the short regression becomes 
\begin_inset Formula 
\[
\tilde{y}=\tilde{x}_{1}'\gamma_{1}+\tilde{u},
\]

\end_inset

where 
\emph on
tilde
\emph default
 denotes the demeaned variable.
\end_layout

\begin_layout Standard
After demeaning, the cross-moment equals to the covariance.
 The short regression coefficient 
\begin_inset Formula 
\[
\begin{aligned}\gamma_{1} & =\left(E\left[\tilde{x}_{1}\tilde{x}_{1}'\right]\right)^{-1}E\left[\tilde{x}_{1}\tilde{y}\right]\\
 & =\left(E\left[\tilde{x}_{1}\tilde{x}_{1}'\right]\right)^{-1}E\left[\tilde{x}_{1}\left(\tilde{x}_{1}'\beta_{1}+\tilde{x}_{2}'\beta_{2}+e\right)\right]\\
 & =\left(E\left[\tilde{x}_{1}\tilde{x}_{1}'\right]\right)^{-1}E\left[\tilde{x}_{1}\tilde{x}_{1}'\right]\beta+\left(E\left[\tilde{x}_{1}\tilde{x}_{1}'\right]\right)^{-1}E\left[\tilde{x}_{1}\tilde{x}_{2}'\right]\beta_{2}\\
 & =\beta_{1}+\left(E\left[\tilde{x}_{1}\tilde{x}_{1}'\right]\right)^{-1}E\left[\tilde{x}_{1}\tilde{x}_{2}'\right]\beta_{2}.
\end{aligned}
\]

\end_inset

Therefore, 
\begin_inset Formula $\gamma_{1}=\beta_{1}$
\end_inset

 if and only if 
\begin_inset Formula $E\left[\tilde{x}_{1}\tilde{x}_{2}'\right]\beta_{2}=0$
\end_inset

, which demands either 
\begin_inset Formula $E\left[\tilde{x}_{1}\tilde{x}_{2}'\right]=0$
\end_inset

 or 
\begin_inset Formula $\beta_{2}=0$
\end_inset

.
\end_layout

\begin_layout Standard
Obviously we prefer to run the long regression to attain 
\begin_inset Formula $\beta_{1}$
\end_inset

 if possible, as it is a model general model than the short regression.
 However, sometimes 
\begin_inset Formula $x_{2}$
\end_inset

 is simply unobservable so the long regression is infeasible.
 When only the short regression is available, in some cases we are able
 to sign the bias, meaning that we know whether 
\begin_inset Formula $\gamma_{1}$
\end_inset

 is bigger or smaller than 
\begin_inset Formula $\beta_{1}$
\end_inset

.
\end_layout

\end_body
\end_document
