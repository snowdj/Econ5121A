#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding utf8x
\fontencoding T1
\font_roman "palatino" "default"
\font_sans "default" "default"
\font_typewriter "mathpazo" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing single
\use_hyperref true
\pdf_bookmarks false
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks true
\pdf_pdfborder false
\pdf_colorlinks true
\pdf_backref section
\pdf_pdfusetitle false
\pdf_quoted_options "urlcolor=urlcolor,linkcolor=linkcolor,citecolor=citecolor,"
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\bullet 1 0 9 -1
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Asymptotic Theory
\end_layout

\begin_layout Author
Zhentao Shi
\end_layout

\begin_layout Section
Asymptotics
\begin_inset CommandInset label
LatexCommand label
name "asymptotics"

\end_inset


\end_layout

\begin_layout Standard
Asymptotic theory is concerned about the behavior of statistics when the
 sample size is arbitrarily large.
 It is a useful approximation technique to simplify complicated finite-sample
 analysis.
\end_layout

\begin_layout Subsection
Modes of Convergence
\begin_inset CommandInset label
LatexCommand label
name "modes-of-convergence"

\end_inset


\end_layout

\begin_layout Standard
Convergence of a deterministic sequence 
\begin_inset Formula $z_{1},z_{2},\ldots$
\end_inset

 means that for any 
\begin_inset Formula $\varepsilon>0$
\end_inset

, there exists an 
\begin_inset Formula $N\left(\varepsilon\right)$
\end_inset

 such that for all 
\begin_inset Formula $n>N\left(\varepsilon\right)$
\end_inset

, we have 
\begin_inset Formula $\left|z_{n}-z\right|<\varepsilon$
\end_inset

.
 We say 
\begin_inset Formula $z$
\end_inset

 is the limit of 
\begin_inset Formula $z_{n}$
\end_inset

, and write as 
\begin_inset Formula $z_{n}\to z$
\end_inset

.
\end_layout

\begin_layout Standard
Instead of a deterministic sequence, here we are interested in the convergence
 of a sequence of random variables.
 Since a random variable has a probability measure associated to any event,
 we must be clear what 
\emph on
convergence
\emph default
 means.
 Several modes of convergence are often encountered.
\end_layout

\begin_layout Itemize
Convergence almost surely* 
\end_layout

\begin_layout Itemize
Convergence in probability: for any 
\begin_inset Formula $\varepsilon>0$
\end_inset

, as 
\begin_inset Formula $n\to\infty$
\end_inset

 the probability 
\begin_inset Formula $P\left(\omega:\left|z_{n}\left(\omega\right)-z\right|<\varepsilon\right)\to1$
\end_inset

 (or equivalently 
\begin_inset Formula $P\left(\omega:\left|z_{n}\left(\omega\right)-z\right|\geq\varepsilon\right)\to0$
\end_inset

.
\end_layout

\begin_layout Itemize
Squared-mean convergence: 
\begin_inset Formula $\lim_{n\to\infty}E\left[\left(z_{n}-z\right)^{2}\right]=0.$
\end_inset


\end_layout

\begin_layout Standard

\series bold
Example
\series default
: 
\begin_inset Formula $(z_{n})$
\end_inset

 is a sequence of binary random variables: 
\begin_inset Formula $z_{n}=\sqrt{n}$
\end_inset

 with probability 
\begin_inset Formula $1/n$
\end_inset

, and 
\begin_inset Formula $z_{n}=0$
\end_inset

 with probability 
\begin_inset Formula $1-1/n$
\end_inset

.
 Then 
\begin_inset Formula $z_{n}\stackrel{p}{\to}0$
\end_inset

 but 
\begin_inset Formula $z_{n}\stackrel{m.s.}{\nrightarrow}0$
\end_inset

.
\end_layout

\begin_layout Standard

\emph on
Justification: 
\emph default
For any 
\begin_inset Formula $\varepsilon>0$
\end_inset

, we have 
\begin_inset Formula $P\left(\omega:\left|z_{n}\left(\omega\right)-0\right|<\varepsilon\right)=P\left(\omega:z_{n}\left(\omega\right)=0\right)=1-1/n\rightarrow1.$
\end_inset

 Thus 
\begin_inset Formula $z_{n}\stackrel{p}{\to}0$
\end_inset

.
 On the other hand, 
\begin_inset Formula $E\left[\left(z_{n}-0\right)^{2}\right]=n\cdot1/n+0\cdot(1-1/n)=1\nrightarrow0.$
\end_inset

 Thus 
\begin_inset Formula $z_{n}\stackrel{m.s.}{\nrightarrow}0$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
Convergence in probability does not count what happens on a subset in the
 sample space of small probability.
 Squared-mean convergence deals with the average over the entire probability
 space.
 If a random variable can take a wild value, even with small probability,
 it may blow away the squared-mean convergence.
 On the contrary, such irregularity does not undermine convergence in probabilit
y.
\end_layout

\begin_layout Itemize
Convergence in distribution: 
\begin_inset Formula $x_{n}\stackrel{d}{\to}x$
\end_inset

 if 
\begin_inset Formula $F\left(x_{n}\right)\to F\left(x\right)$
\end_inset

 for each 
\begin_inset Formula $x$
\end_inset

 on which 
\begin_inset Formula $F\left(x\right)$
\end_inset

 is continuous.
\end_layout

\begin_layout Standard
Convergence in distribution is about 
\emph on
pointwise
\emph default
 convergence of CDF, not the random variables themselves.
\end_layout

\begin_layout Standard

\series bold
Example
\series default
: Let 
\begin_inset Formula $x\sim N\left(0,1\right)$
\end_inset

.
 If 
\begin_inset Formula $z_{n}=x+1/n$
\end_inset

, then 
\begin_inset Formula $z_{n}\stackrel{p}{\to}x$
\end_inset

 and of course 
\begin_inset Formula $z_{n}\stackrel{d}{\to}x$
\end_inset

.
 However, if 
\begin_inset Formula $z_{n}=-x+1/n$
\end_inset

, or 
\begin_inset Formula $z_{n}=y+1/n$
\end_inset

 where 
\begin_inset Formula $y\sim N\left(0,1\right)$
\end_inset

 is independent of 
\begin_inset Formula $x$
\end_inset

, then 
\begin_inset Formula $z_{n}\stackrel{d}{\to}x$
\end_inset

 but 
\begin_inset Formula $z_{n}\stackrel{p}{\nrightarrow}x$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Example
\series default
: 
\begin_inset Formula $(z_{n})$
\end_inset

 is a sequence of binary random variables: 
\begin_inset Formula $z_{n}=\sqrt{n}$
\end_inset

 with probability 
\begin_inset Formula $1/n$
\end_inset

, and 
\begin_inset Formula $z_{n}=0$
\end_inset

 with probability 
\begin_inset Formula $1-1/n$
\end_inset

.
 Then 
\begin_inset Formula $z_{n}\stackrel{d}{\to}z=0.$
\end_inset


\end_layout

\begin_layout Quote

\emph on
Justification:
\end_layout

\begin_layout Quote
\begin_inset Formula $F\left(z_{n}\right)=\begin{cases}
0 & z_{n}<0\\
1-1/n & 0\leq z_{n}\leq\sqrt{n}\\
1 & z_{n}\geq\sqrt{n}
\end{cases}$
\end_inset


\end_layout

\begin_layout Quote
\begin_inset Formula $F\left(z\right)=\begin{cases}
0 & z<0\\
1 & z\geq0
\end{cases}$
\end_inset

.
\end_layout

\begin_layout Quote
It is obvious that 
\begin_inset Formula $F\left(z_{n}\right)$
\end_inset

 converges to 
\begin_inset Formula $F\left(z\right)$
\end_inset

 pointwise on the set where 
\begin_inset Formula $F\left(z\right)$
\end_inset

 is continuous.
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
Squared-mean convergence implies convergence in probability.
 Convergence in probability implies convergence in distribution.
\end_layout

\begin_layout Standard

\emph on
CramÃ©r-Wold device
\emph default
 handles convergence in distribution for random vectors.
 We say a sequence of 
\begin_inset Formula $K$
\end_inset

-dimensional random vectors 
\begin_inset Formula $\left(X_{n}\right)$
\end_inset

 converge in distribution to 
\begin_inset Formula $X$
\end_inset

 if 
\begin_inset Formula $\lambda'X_{n}\stackrel{d}{\to}\lambda'X$
\end_inset

 for any 
\begin_inset Formula $\lambda\in\mathbb{R}^{K}$
\end_inset

.
\end_layout

\begin_layout Subsection
Law of Large Numbers
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "law-of-large-numbers"

\end_inset


\end_layout

\begin_layout Standard
(Weak) law of large numbers (LLN) is a collection of statements about convergenc
e in probability of the sample average to its population counterpart.
 The basic form of LLN is: 
\begin_inset Formula 
\[
\frac{1}{n}\sum_{i=1}^{n}(z_{i}-E[z_{i}])\stackrel{p}{\to}0
\]

\end_inset

as 
\begin_inset Formula $n\to\infty$
\end_inset

.
 Various versions of LLN work under different assumptions about the distribution
s and dependence of the random variables.
\end_layout

\begin_layout Itemize
Chebyshev LLN: if 
\begin_inset Formula $\left(z_{1},\ldots,z_{n}\right)$
\end_inset

 is a sample of i.i.d.
\begin_inset space ~
\end_inset

observations, 
\begin_inset Formula $E\left[z_{1}\right]=\mu$
\end_inset

 , and 
\begin_inset Formula $\sigma^{2}=\mathrm{var}\left[z_{1}\right]<\infty$
\end_inset

 exists, then 
\begin_inset Formula $\frac{1}{n}\sum_{i=1}^{n}z_{i}-\mu\stackrel{p}{\to}0.$
\end_inset

 
\end_layout

\begin_layout Standard
Chebyshev LLN utilizes 
\emph on
Chebyshev inequality
\emph default
.
\end_layout

\begin_layout Itemize

\emph on
Chebyshev inequality
\emph default
: for any random variable 
\begin_inset Formula $x$
\end_inset

 , we have 
\begin_inset Formula $P\left(\left|x\right|>\varepsilon\right)\leq E\left[x^{2}\right]/\varepsilon^{2}$
\end_inset

 for any 
\begin_inset Formula $\varepsilon>0$
\end_inset

, if 
\begin_inset Formula $E\left[x^{2}\right]<\infty$
\end_inset

.
 
\end_layout

\begin_layout Standard
Chebyshev inequality is a special case of 
\emph on
Markov inequality
\emph default
.
\end_layout

\begin_layout Itemize

\emph on
Markov inequality
\emph default
: 
\begin_inset Formula $P\left(\left|x\right|>\varepsilon\right)\leq E\left[\left|x\right|^{r}\right]/\varepsilon^{r}$
\end_inset

 for 
\begin_inset Formula $r\geq1$
\end_inset

 and any 
\begin_inset Formula $\varepsilon>0$
\end_inset

, if 
\begin_inset Formula $E\left[\left|x\right|^{r}\right]<\infty$
\end_inset

.
\end_layout

\begin_layout Standard
It is easy to verify Markov inequality.
 
\begin_inset Formula 
\[
\begin{aligned}E\left[\left|x\right|^{r}\right] & =\int_{\left|x\right|>\varepsilon}\left|x\right|^{r}dF_{X}+\int_{\left|x\right|\leq\varepsilon}\left|x\right|^{r}dF_{X}\\
 & \geq\int_{\left|x\right|>\varepsilon}\left|x\right|^{r}dF_{X}\geq\varepsilon^{r}\int_{\left|x\right|>\varepsilon}dF_{X}=\varepsilon^{r}P\left(\left|x\right|>\varepsilon\right).
\end{aligned}
\]

\end_inset


\end_layout

\begin_layout Standard
Consider a partial sum 
\begin_inset Formula $S_{n}=\sum_{i=1}^{n}x_{i}$
\end_inset

, where 
\begin_inset Formula $\mu_{i}=E\left[x_{i}\right]$
\end_inset

 and 
\begin_inset Formula $\sigma_{i}^{2}=\mathrm{var}\left[x_{i}\right]$
\end_inset

.
 We apply the Chebyshev inequality to the sample mean 
\begin_inset Formula $\overline{x}-\bar{\mu}=n^{-1}\left(S_{n}-E\left[S_{n}\right]\right)$
\end_inset

.
 
\begin_inset Formula 
\[
\begin{aligned}P\left(\left|\bar{x}-\bar{\mu}\right|\geq\varepsilon\right) & =P\left(\left|S_{n}-E\left[S_{n}\right]\right|\geq n\varepsilon\right)\\
 & \leq\left(n\varepsilon\right)^{-2}E\left[\sum_{i=1}^{n}\left(x_{i}-\mu_{i}\right)^{2}\right]\\
 & =\left(n\varepsilon\right)^{-2}\mathrm{var}\left(\sum_{i=1}^{n}x_{i}\right)\\
 & =\left(n\varepsilon\right)^{-2}\left[\sum_{i=1}^{n}\mathrm{var}\left(x_{i}\right)+\sum_{i=1}^{n}\sum_{j\neq i}\mathrm{cov}\left(x_{i},x_{j}\right)\right].
\end{aligned}
\]

\end_inset


\end_layout

\begin_layout Standard
From the above derivation, convergence in probability holds as long as the
 right-hand side shrinks to 0 as 
\begin_inset Formula $n\to\infty$
\end_inset

.
 Actually, the convergence can be maintained under much more general conditions
 than just under the i.i.d.
\begin_inset space ~
\end_inset

assumption.
 The random variables in the sample do not have to be identically distributed,
 and they do not have to be independent either.
\end_layout

\begin_layout Standard
Another useful LLN is 
\emph on
Kolmogorov LLN
\emph default
.
 Since its derivation requires advanced knowledge of probability theory,
 we state the result without proof.
\end_layout

\begin_layout Itemize
Kolmogorov LLN: if 
\begin_inset Formula $\left(z_{1},\ldots,z_{n}\right)$
\end_inset

 is a sample of i.i.d.
\begin_inset space ~
\end_inset

observations and 
\begin_inset Formula $E\left[z_{1}\right]=\mu$
\end_inset

 exists, then 
\begin_inset Formula $\frac{1}{n}\sum_{i=1}^{n}z_{i}-\mu\stackrel{p}{\to}0.$
\end_inset

 
\end_layout

\begin_layout Standard
Compared to Chebyshev LLN, Kolmogorov LLN only requires the existence of
 the population mean, but not any higher moments.
 On the other hand, i.i.d.
\begin_inset space ~
\end_inset

is essential for Kolmogorov LLN.
\end_layout

\begin_layout Subsection
Central Limit Theorem
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "central-limit-theorem"

\end_inset


\end_layout

\begin_layout Standard
The central limit theorem (CLT) is a collection of probability results about
 the convergence in distribution to a stable law, usually the normal distributio
n.
 The basic form of the CLT is: for a sample 
\begin_inset Formula $\left(z_{1},\ldots,z_{n}\right)$
\end_inset

 of 
\emph on
zero-mean
\emph default
 random variables, 
\begin_inset Formula 
\begin{equation}
\frac{1}{\sqrt{n}}\sum_{i=1}^{n}z_{i}\stackrel{d}{\to}N\left(0,\sigma^{2}\right).\label{eq:clt}
\end{equation}

\end_inset

Various versions of CLT work under different assumptions about the random
 variables.
\end_layout

\begin_layout Standard

\emph on
Lindeberg-Levy CLT
\emph default
 is the simplest CLT.
\end_layout

\begin_layout Itemize
If the sample is i.i.d., 
\begin_inset Formula $E\left[x_{1}\right]=0$
\end_inset

 and 
\begin_inset Formula $\mathrm{var}\left[x_{1}^{2}\right]=\sigma^{2}<\infty$
\end_inset

, then CLT holds.
 
\end_layout

\begin_layout Standard
Lindeberg-Levy CLT is easy to verify by the characteristic function.
 For any random variable 
\begin_inset Formula $x$
\end_inset

, the function 
\begin_inset Formula $\varphi_{x}\left(t\right)=E\left[\exp\left(ixt\right)\right]$
\end_inset

 is called its 
\emph on
characteristic function
\emph default
.
 The characteristic function fully describes a distribution, just like PDF
 or CDF.
 For example, the characteristic function of 
\begin_inset Formula $N\left(\mu,\sigma^{2}\right)$
\end_inset

 is 
\begin_inset Formula $\exp\left(it\mu-\frac{1}{2}\sigma^{2}t^{2}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
Here is a very heuristic argument.
 If 
\begin_inset Formula $E\left[\left|x\right|^{k}\right]<\infty$
\end_inset

 for a positive integer 
\begin_inset Formula $k$
\end_inset

, then 
\begin_inset Formula 
\[
\varphi_{X}\left(t\right)=1+itE\left[X\right]+\frac{\left(it\right)^{2}}{2}E\left[X^{2}\right]+\ldots\frac{\left(it\right)^{k}}{k!}E\left[X^{k}\right]+o\left(t^{k}\right).
\]

\end_inset

Under the assumption of Lindeberg-Levy CLT, 
\begin_inset Formula 
\[
\varphi_{X_{i}/\sqrt{n}}\left(t\right)=1-\frac{t^{2}}{2n}\sigma^{2}+o\left(\frac{t^{2}}{n}\right)
\]

\end_inset

for all 
\begin_inset Formula $i$
\end_inset

, and by independence we have 
\begin_inset Formula 
\[
\begin{aligned}\varphi_{\frac{1}{\sqrt{n}}\sum_{i=1}^{n}x_{i}}\left(t\right) & =\prod_{i=1}^{n}\varphi_{x_{i}/\sqrt{n}}\left(t\right)=\left(1+i\cdot0-\frac{t^{2}}{2n}\sigma^{2}+o\left(\frac{t^{2}}{n}\right)\right)^{n}\\
 & \to\exp\left(-\frac{\sigma^{2}}{2}t^{2}\right),
\end{aligned}
\]

\end_inset

where the limit is exactly the characteristic function of 
\begin_inset Formula $N\left(0,\sigma^{2}\right)$
\end_inset

.
\end_layout

\begin_layout Itemize
Lindeberg-Feller CLT: i.n.i.d., and 
\emph on
Lindeberg condition
\emph default
: for any fixed 
\begin_inset Formula $\varepsilon>0$
\end_inset

, 
\begin_inset Formula 
\[
\frac{1}{s_{n}^{2}}\sum_{i=1}^{n}E\left[x_{i}^{2}\cdot\boldsymbol{1}\left\{ \left|x_{i}\right|\geq\varepsilon s_{n}\right\} \right]\to0
\]

\end_inset

where 
\begin_inset Formula $s_{n}=\left(\sum_{i=1}^{n}\sigma_{i}^{2}\right)^{1/2}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Lyapunov CLT: i.n.i.d.: 
\begin_inset Formula $\max_{i\leq n}E\left[\left|x_{i}\right|^{3}\right]<C<\infty$
\end_inset

.
\end_layout

\begin_layout Subsection
Tools for Transformations
\begin_inset CommandInset label
LatexCommand label
name "tools-for-transformations"

\end_inset


\end_layout

\begin_layout Standard
In their original forms, LLN deals with the sample mean, and CLT handles
 the scaled (by 
\begin_inset Formula $\sqrt{n}$
\end_inset

) and/or standardized (by standard deviation) sample mean.
 However, most of the econometric estimators of interest are functions of
 sample means.
 Therefore, we need tools to handle transformations.
\end_layout

\begin_layout Itemize
Small op: 
\begin_inset Formula $x_{n}=o_{p}\left(r_{n}\right)$
\end_inset

 if 
\begin_inset Formula $x_{n}/r_{n}\stackrel{p}{\to}0$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Big Op: 
\begin_inset Formula $x_{n}=O_{p}\left(r_{n}\right)$
\end_inset

 if for any 
\begin_inset Formula $\varepsilon>0$
\end_inset

, there exists a 
\begin_inset Formula $c>0$
\end_inset

 such that 
\begin_inset Formula $P\left(\left|x_{n}\right|/r_{n}>c\right)<\varepsilon$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Continuous mapping theorem 1: If 
\begin_inset Formula $x_{n}\stackrel{p}{\to}a$
\end_inset

 and 
\begin_inset Formula $f\left(\cdot\right)$
\end_inset

 is continuous at 
\begin_inset Formula $a$
\end_inset

, then 
\begin_inset Formula $f\left(x_{n}\right)\stackrel{p}{\to}f\left(a\right)$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Continuous mapping theorem 2: If 
\begin_inset Formula $x_{n}\stackrel{d}{\to}x$
\end_inset

 and 
\begin_inset Formula $f\left(\cdot\right)$
\end_inset

 is continuous almost surely on the support of 
\begin_inset Formula $x$
\end_inset

, then 
\begin_inset Formula $f\left(x_{n}\right)\stackrel{d}{\to}f\left(x\right)$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Slutsky's Theorem: If 
\begin_inset Formula $x_{n}\stackrel{d}{\to}x$
\end_inset

 and 
\begin_inset Formula $y_{n}\stackrel{p}{\to}a$
\end_inset

, then
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $x_{n}+y_{n}\stackrel{d}{\to}x+a$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $x_{n}y_{n}\stackrel{d}{\to}ax$
\end_inset

 
\end_layout

\begin_layout Itemize
\begin_inset Formula $x_{n}/y_{n}\stackrel{d}{\to}x/a$
\end_inset

 if 
\begin_inset Formula $a\neq0$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Itemize
Delta method: if 
\begin_inset Formula $\sqrt{n}\left(\widehat{\theta}-\theta_{0}\right)\stackrel{d}{\to}N\left(0,\Omega\right)$
\end_inset

, and 
\begin_inset Formula $f\left(\cdot\right)$
\end_inset

 is continuously differentiable at 
\begin_inset Formula $\theta_{0}$
\end_inset

, then 
\begin_inset Formula 
\[
\sqrt{n}\left(f\left(\widehat{\theta}\right)-f\left(\theta_{0}\right)\right)\stackrel{d}{\to}N\left(0,\frac{\partial f}{\partial\theta'}\left(\theta_{0}\right)\Omega\left(\frac{\partial f}{\partial\theta}\left(\theta_{0}\right)\right)'\right).
\]

\end_inset


\end_layout

\begin_layout Section
Asymptotic Properties of OLS
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "asymptotic-properties-of-ols"

\end_inset


\end_layout

\begin_layout Standard
We apply large sample theory to study the OLS estimator 
\begin_inset Formula $\widehat{\beta}=\left(X'X\right)^{-1}X'Y.$
\end_inset


\end_layout

\begin_layout Subsection
Consistency
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "consistency"

\end_inset


\end_layout

\begin_layout Standard
We say 
\begin_inset Formula $\widehat{\beta}$
\end_inset

 is 
\emph on
consistent
\emph default
 if 
\begin_inset Formula $\widehat{\beta}\stackrel{p}{\to}\beta$
\end_inset

 as 
\begin_inset Formula $n\to\infty$
\end_inset

.
 To verify consistency, we write 
\begin_inset Formula 
\begin{equation}
\widehat{\beta}-\beta=\left(X'X\right)^{-1}X'e=\left(\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}'\right)^{-1}\frac{1}{n}\sum_{i=1}^{n}x_{i}e_{i}.\label{eq:ols_d}
\end{equation}

\end_inset

The first term 
\begin_inset Formula 
\[
\widehat{Q}=\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}'\stackrel{p}{\to}Q=E\left[x_{i}x_{i}'\right],
\]

\end_inset

and the second term 
\begin_inset Formula 
\[
\frac{1}{n}\sum_{i=1}^{n}x_{i}e_{i}\stackrel{p}{\to}0.
\]

\end_inset

No matter whether 
\begin_inset Formula $\left(y_{i},x_{i}\right)_{i=1}^{n}$
\end_inset

 is an i.i.d., i.n.i.d., or dependent sample, as long as the convergence in probability
 holds for the above two expressions and 
\begin_inset Formula $Q$
\end_inset

 is an invertible matrix, we have 
\begin_inset Formula $\widehat{\beta}-\beta\stackrel{p}{\to}Q^{-1}0=0$
\end_inset

 by the continuous mapping theorem.
 In other words, 
\begin_inset Formula $\widehat{\beta}$
\end_inset

 is a consistent estimator of 
\begin_inset Formula $\beta$
\end_inset

.
\end_layout

\begin_layout Subsection
Asymptotic Normality
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "asymptotic-normality"

\end_inset


\end_layout

\begin_layout Standard
In finite sample, 
\begin_inset Formula $\widehat{\beta}$
\end_inset

 is a random variable.
 We have shown the distribution of 
\begin_inset Formula $\widehat{\beta}$
\end_inset

 under normality in the previous lecture.
 Without the restrictive normality assumption, how can we characterize the
 randomness of the OLS estimator?
\end_layout

\begin_layout Standard
We know from the previous section that 
\begin_inset Formula $\hat{\beta}-\beta\stackrel{p}{\to}0$
\end_inset

 degenerates to a constant.
 To study its distribution, we must scale it up by a proper multiplier so
 that in the limit it neither degenerates nor explodes.
 The suitable scaling factor is 
\begin_inset Formula $\sqrt{n}$
\end_inset

:
\begin_inset Formula 
\[
\sqrt{n}\left(\widehat{\beta}-\beta\right)=\left(\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}'\right)^{-1}\frac{1}{\sqrt{n}}\sum_{i=1}^{n}x_{i}e_{i}.
\]

\end_inset

Since 
\begin_inset Formula $E\left[x_{i}e_{i}\right]=0$
\end_inset

, we apply a CLT to obtain 
\begin_inset Formula 
\[
n^{-1/2}\sum_{i=1}^{n}x_{i}e_{i}\stackrel{d}{\to}N\left(0,\Sigma\right)
\]

\end_inset

where 
\begin_inset Formula $\Sigma=E\left[x_{i}x_{i}'e_{i}^{2}\right]$
\end_inset

.
 By the continuous mapping theorem, 
\begin_inset Formula 
\[
\sqrt{n}\left(\widehat{\beta}-\beta\right)\stackrel{d}{\to}Q^{-1}\times N\left(0,\Sigma\right)\sim N\left(0,\Omega\right)
\]

\end_inset

where 
\begin_inset Formula $\Omega=Q^{-1}\Sigma Q^{-1}$
\end_inset

 is called the 
\emph on
asymptotic variance
\emph default
.
 This is the 
\emph on
asymptotic normality
\emph default
 of the OLS estimator.
\end_layout

\begin_layout Standard
Up to now we have derived the asymptotic distribution of 
\begin_inset Formula $\widehat{\beta}$
\end_inset

.
 However, to make it feasible, we still have to estimator the asymptotic
 variance 
\begin_inset Formula $\Omega$
\end_inset

.
 If 
\begin_inset Formula $\widehat{\Sigma}$
\end_inset

 is a consistent estimator of 
\begin_inset Formula $\Sigma$
\end_inset

, then 
\begin_inset Formula $\widehat{\Omega}=\widehat{Q}^{-1}\widehat{\Sigma}\widehat{Q}^{-1}$
\end_inset

 is a consistent estimator of 
\begin_inset Formula $\Omega$
\end_inset

.
 (Of course, there are other ways to estimate the asymptotic variance.) A
 feasible version about the distribution of 
\begin_inset Formula $\widehat{\beta}$
\end_inset

 is 
\begin_inset Formula 
\[
\widehat{\Omega}^{-1/2}\sqrt{n}\left(\widehat{\beta}-\beta\right)\stackrel{d}{\to}N\left(0,I_{K}\right).
\]

\end_inset


\end_layout

\begin_layout Standard
In the special case of homoskedasticity, 
\begin_inset Formula $\Sigma=E\left[x_{i}x_{i}'e_{i}^{2}\right]=E\left[x_{i}x_{i}'E\left[e_{i}^{2}|X\right]\right]=\sigma^{2}E\left[x_{i}x_{i}'\right]=\sigma^{2}Q$
\end_inset

 so that 
\begin_inset Formula 
\[
n^{-1/2}\sum_{i=1}^{n}x_{i}e_{i}\stackrel{d}{\to}N\left(0,\sigma^{2}Q\right).
\]

\end_inset

Again by the continuous mapping theorem,
\begin_inset Formula 
\[
\sqrt{n}\left(\widehat{\beta}-\beta\right)\stackrel{d}{\to}Q^{-1}\times N\left(0,\sigma^{2}Q\right)\sim N\left(0,\sigma^{2}Q^{-1}\right).
\]

\end_inset

We can estimate the unknown parameter 
\begin_inset Formula $\sigma^{2}$
\end_inset

 by either 
\begin_inset Formula $\widehat{\sigma}^{2}=\widehat{e}'\widehat{e}/\left(n-K\right)$
\end_inset

 or 
\begin_inset Formula $\widehat{\sigma}^{2}=\widehat{e}'\widehat{e}/n$
\end_inset

.
\end_layout

\begin_layout Subsection
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
texorpdfstring{
\end_layout

\end_inset

Estimation of the Variance*
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

}{
\end_layout

\end_inset

Estimation of the Variance*
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "estimation-of-the-variance"

\end_inset


\end_layout

\begin_layout Standard
We show all elements of 
\begin_inset Formula $\Sigma=E\left[x_{i}x_{i}'e_{i}^{2}\right]$
\end_inset

 is finite.
 That is, 
\begin_inset Formula $\left\Vert \Sigma\right\Vert _{\infty}:=\max_{i,j\leq K}\left|\sigma_{ij}\right|<\infty$
\end_inset

.
 Let 
\begin_inset Formula $z_{i}=x_{i}e_{i}$
\end_inset

, so 
\begin_inset Formula $\Sigma=E\left[z_{i}z_{i}'\right]$
\end_inset

.
 Because of the Cachy-Schwarz inequality, 
\begin_inset Formula 
\[
\left\Vert \Sigma\right\Vert _{\infty}=\max_{k=1,\ldots,K}E\left[z_{ik}^{2}\right].
\]

\end_inset

For each 
\begin_inset Formula $k$
\end_inset

, 
\begin_inset Formula $E\left[z_{ik}^{2}\right]=E\left[x_{ik}^{2}e_{i}^{2}\right]\leq\left(E\left[x_{ik}^{4}\right]E\left[e_{i}^{4}\right]\right)^{1/2}$
\end_inset

.
\end_layout

\begin_layout Standard
For the estimation of variance, if the error is homoskedastic, 
\begin_inset Formula 
\[
\begin{aligned}\frac{1}{n}\sum_{i=1}^{n}\widehat{e}_{i}^{2} & =\frac{1}{n}\sum_{i=1}^{n}\left(e_{i}+x_{i}'\left(\widehat{\beta}-\beta\right)\right)^{2}\\
 & =\frac{1}{n}\sum_{i=1}^{n}e_{i}^{2}+\left(\frac{2}{n}\sum_{i=1}^{n}e_{i}x_{i}\right)'\left(\widehat{\beta}-\beta\right)+\frac{1}{n}\sum_{i=1}^{n}e_{i}^{2}\left(\widehat{\beta}-\beta\right)'x_{i}x_{i}'\left(\widehat{\beta}-\beta\right).
\end{aligned}
\]

\end_inset

The second term 
\begin_inset Formula 
\[
\left(\frac{2}{n}\sum_{i=1}^{n}e_{i}x_{i}\right)'\left(\widehat{\beta}-\beta\right)=o_{p}\left(1\right)o_{p}\left(1\right)=o_{p}\left(1\right).
\]

\end_inset

The third term 
\begin_inset Formula 
\[
\left(\widehat{\beta}-\beta\right)\left(\frac{1}{n}\sum_{i=1}^{n}e_{i}^{2}x_{i}x'_{i}\right)\left(\widehat{\beta}-\beta\right)=o_{p}\left(1\right)O_{p}\left(1\right)o_{p}\left(1\right)=o_{p}\left(1\right).
\]

\end_inset

As 
\begin_inset Formula $\frac{1}{n}\sum_{i=1}^{n}\widehat{e}_{i}^{2}=\frac{1}{n}\sum_{i=1}^{n}\widehat{e}_{i}^{2}+o_{p}\left(1\right)$
\end_inset

 and 
\begin_inset Formula $\frac{1}{n}\sum_{i=1}^{n}e_{i}^{2}=\sigma_{e}^{2}+o_{p}\left(1\right)$
\end_inset

, we have 
\begin_inset Formula $\frac{1}{n}\sum_{i=1}^{n}\widehat{e}_{i}^{2}=\sigma_{e}^{2}+o_{p}\left(1\right)$
\end_inset

.
 In other words, 
\begin_inset Formula $\frac{1}{n}\sum_{i=1}^{n}\widehat{e}_{i}^{2}\stackrel{p}{\to}\sigma_{e}^{2}$
\end_inset

.
\end_layout

\begin_layout Standard
For general heteroskedasticity, 
\begin_inset Formula 
\[
\begin{aligned}\frac{1}{n}\sum_{i=1}^{n}\widehat{e}_{i}^{2} & =\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}'\left(e_{i}+x_{i}'\left(\widehat{\beta}-\beta\right)\right)^{2}\\
 & =\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}'e_{i}^{2}+\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}e_{i}x_{i}'\left(\widehat{\beta}-\beta\right)+\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}'\left(\left(\widehat{\beta}-\beta\right)'x_{i}\right)^{2}.
\end{aligned}
\]

\end_inset

The third term is bounded by 
\begin_inset Formula 
\[
\begin{aligned} & \mbox{trace}\left(\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}'\left(\left(\widehat{\beta}-\beta\right)'x_{i}\right)^{2}\right)\\
 & \leq K\max_{k}\frac{1}{n}\sum_{i=1}^{n}x_{ik}^{2}\left[\left(\widehat{\beta}-\beta\right)'x_{i}\right]^{2}\\
 & \leq K\left\Vert \widehat{\beta}-\beta\right\Vert _{2}^{2}\max_{k}\frac{1}{n}\sum_{i=1}^{n}x_{ik}^{2}\left\Vert x_{i}\right\Vert _{2}^{2}\\
 & \leq K\left\Vert \widehat{\beta}-\beta\right\Vert _{2}^{2}\frac{1}{n}\sum_{i=1}^{n}\left\Vert x_{i}\right\Vert _{2}^{2}\left\Vert x_{i}\right\Vert _{2}^{2}\\
 & =K\left\Vert \widehat{\beta}-\beta\right\Vert _{2}^{2}\frac{1}{n}\sum_{i=1}^{n}\left(\sum_{k=1}^{K}x_{ik}^{2}\right)^{2}\\
 & \leq K\left\Vert \widehat{\beta}-\beta\right\Vert _{2}^{2}K\sum_{k=1}^{K}\frac{1}{n}\sum_{i=1}^{n}x_{ik}^{4}=o_{p}\left(1\right)O_{p}\left(1\right)=o_{p}\left(1\right).
\end{aligned}
\]

\end_inset

where the third inequality follows by 
\begin_inset Formula $\left(a_{1}+\cdots+a_{K}\right)^{2}\leq K\left(a_{1}^{2}+\cdots+a_{K}^{2}\right)$
\end_inset

.
 The second term is bounded by 
\begin_inset Formula 
\[
\begin{aligned} & \left|\frac{1}{n}\sum_{i=1}^{n}x_{ik}x_{ik'}e_{i}x_{i}'\left(\widehat{\beta}-\beta\right)\right|\\
 & \leq\max_{k}\left|\widehat{\beta}_{k}-\beta_{k}\right|K\max_{k,k',k''}\left|\frac{1}{n}\sum_{i=1}^{n}e_{i}x_{ik}x_{ik'}x_{ik''}\right|\\
 & \leq\left\Vert \widehat{\beta}-\beta\right\Vert _{2}\left(\frac{1}{n}\sum_{i=1}^{n}e_{i}^{4}\right)^{1/4}K\max_{k,k',k''}\left(\frac{1}{n}\sum_{i=1}^{n}\left(x_{ik}x_{ik'}x_{ik''}\right)^{4/3}\right)^{3/4}\\
 & \leq\left\Vert \widehat{\beta}-\beta\right\Vert _{2}K\max_{k}\left(\frac{1}{n}\sum_{i=1}^{n}x_{ik}^{4}\right)^{3/4}=o_{p}\left(1\right)O_{p}\left(1\right)
\end{aligned}
\]

\end_inset

where the second and the third inequality hold by the Holder's inequality.
\end_layout

\end_body
\end_document
