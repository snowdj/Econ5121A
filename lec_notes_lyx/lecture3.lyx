#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{Lecture Notes}
\rhead{Zhentao Shi}
\end_preamble
\use_default_options false
\begin_modules
theorems-ams
theorems-ams-extended
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format dvi
\output_sync 1
\output_sync_macro "\synctex=-1"
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing onehalf
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 0
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 0
\use_package mhchem 1
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\branch abc
\selected 1
\filename_suffix 0
\color #faf0e6
\end_branch
\branch pf of gamma = 0
\selected 0
\filename_suffix 0
\color #faf0e6
\end_branch
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes true
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
This version: 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
today
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Notation: 
\begin_inset Formula $y_{i}$
\end_inset

 is a scalar, and 
\begin_inset Formula $x_{i}$
\end_inset

 is a 
\begin_inset Formula $K\times1$
\end_inset

 vector.
 
\begin_inset Formula $Y$
\end_inset

 is an 
\begin_inset Formula $n\times1$
\end_inset

 vector, and 
\begin_inset Formula $X$
\end_inset

 is an 
\begin_inset Formula $n\times K$
\end_inset

 matrix.
\end_layout

\begin_layout Section
Algebra of Least Squares
\end_layout

\begin_layout Subsection
OLS estimator
\end_layout

\begin_layout Standard
As we have learned from the linear project model, the parameter 
\begin_inset Formula $\beta$
\end_inset

 
\begin_inset Formula 
\begin{eqnarray*}
y_{i} & = & x'_{i}\beta+e_{i}\\
E[x_{i}e_{i}] & = & 0
\end{eqnarray*}

\end_inset

can be written as 
\begin_inset Formula $\beta=\left(E\left[x_{i}x_{i}'\right]\right)^{-1}E\left[x_{i}y_{i}\right].$
\end_inset

 
\end_layout

\begin_layout Standard
While population is something imaginary, in reality we possess a sample
 of 
\begin_inset Formula $n$
\end_inset

 observations.
 We thus replace the population mean 
\begin_inset Formula $E\left[\cdot\right]$
\end_inset

 by the sample mean, and the resulting estimator is
\begin_inset Formula 
\[
\widehat{\beta}=\left(\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}'\right)^{-1}\frac{1}{n}\sum_{i=1}^{n}x_{i}y_{i}=\left(X'X\right)^{-1}X'y.
\]

\end_inset

This is one way to motivate the OLS estimator.
 
\end_layout

\begin_layout Standard
Alternatively, we can derive the OLS estimator from minimizing the sum of
 squared residuals 
\begin_inset Formula 
\[
Q\left(\beta\right)=\sum_{i=1}^{n}\left(y_{i}-x_{i}'\beta\right)^{2}=\left(Y-X\beta\right)'\left(Y-X\beta\right).
\]

\end_inset

 By the first-order condition
\begin_inset Formula 
\[
\frac{\partial}{\partial\beta}Q\left(\beta\right)=-2X'\left(Y-X\beta\right),
\]

\end_inset

the optimality condition gives exactly the same 
\begin_inset Formula $\widehat{\beta}$
\end_inset

.
 Moreover, the second-order condition 
\begin_inset Formula 
\[
\frac{\partial^{2}}{\partial\beta\partial\beta'}Q\left(\beta\right)=2X'X
\]

\end_inset

shows that 
\begin_inset Formula $Q\left(\beta\right)$
\end_inset

 is convex in 
\begin_inset Formula $\beta$
\end_inset

.
 (
\begin_inset Formula $Q\left(\beta\right)$
\end_inset

 is strictly convex in 
\begin_inset Formula $\beta$
\end_inset

 if 
\begin_inset Formula $X'X$
\end_inset

 is positive definite.)
\end_layout

\begin_layout Standard
Here we introduce some definitions and properties in OLS estimation.
\end_layout

\begin_layout Itemize
Fitted value: 
\begin_inset Formula $\widehat{Y}=X\widehat{\beta}$
\end_inset

.
\end_layout

\begin_layout Itemize
Projector: 
\begin_inset Formula $P_{X}=X\left(X'X\right)^{-1}X$
\end_inset

; Annihilator: 
\begin_inset Formula $M_{X}=I_{n}-P_{X}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
\begin_inset Formula $P_{X}M_{X}=M_{X}P_{X}=0$
\end_inset

.
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $AA=A$
\end_inset

, we call it an idempotent matrix.
 Both 
\begin_inset Formula $P_{X}$
\end_inset

 and 
\begin_inset Formula $M_{X}$
\end_inset

 are idempotent.
 
\end_layout

\begin_layout Itemize
Residual: 
\begin_inset Formula $\widehat{e}=Y-\widehat{Y}=Y-X\widehat{\beta}=M_{X}Y=M_{X}\left(X\beta+e\right)=M_{X}e$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $X'\widehat{e}=XM_{X}e=0$
\end_inset

.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\frac{1}{n}\sum_{i=1}^{n}\widehat{e}_{i}=0$
\end_inset

 if 
\begin_inset Formula $x_{i}$
\end_inset

 contains a constant.
\end_layout

\begin_layout Subsection
Goodness of Fit
\end_layout

\begin_layout Standard
The so-called R-square is the most popular measure of goodness-of-fit in
 the linear regression.
 R-square is well defined only when a constant is included in the regressors.
 Let 
\begin_inset Formula $M_{\iota}=I_{n}-\frac{1}{n}\iota\iota'$
\end_inset

, where 
\begin_inset Formula $\iota$
\end_inset

 is an 
\begin_inset Formula $n\times1$
\end_inset

 vector of 1's.
 
\begin_inset Formula $M_{\iota}$
\end_inset

 is the 
\emph on
demeaner
\emph default
, in the sense that 
\begin_inset Formula $M_{\iota}\left(z_{1},\ldots,z_{n}\right)'=\left(z_{1}-\overline{z},\ldots,z_{n}-\overline{z}\right)'$
\end_inset

, where 
\begin_inset Formula $\bar{z}=\frac{1}{n}\sum_{i=1}^{n}z_{i}$
\end_inset

.
 For any 
\begin_inset Formula $X$
\end_inset

, we can decompose 
\begin_inset Formula $Y=P_{X}Y+M_{X}Y=\widehat{Y}+\widehat{e}$
\end_inset

.
 The total variation is 
\begin_inset Formula 
\[
Y'M_{\iota}Y=\left(\widehat{Y}+\widehat{e}\right)'M_{\iota}\left(\widehat{Y}+\widehat{e}\right)=\widehat{Y}'M_{\iota}\widehat{Y}+2\widehat{Y}'M_{\iota}\widehat{e}+\widehat{e}'M_{\iota}\widehat{e}=\widehat{Y}'M_{\iota}\widehat{Y}+\widehat{e}'\widehat{e}
\]

\end_inset

where the last equality follows by 
\begin_inset Formula $M_{\iota}\widehat{e}=\widehat{e}$
\end_inset

 as 
\begin_inset Formula $\frac{1}{n}\sum_{i=1}^{n}\widehat{e}_{i}=0$
\end_inset

, and 
\begin_inset Formula $\widehat{Y}'\widehat{e}=Y'P_{X}M_{X}e=0$
\end_inset

.
 R-square is defined as 
\begin_inset Formula $\widehat{Y}'M_{\iota}\widehat{Y}/Y'M_{\iota}Y$
\end_inset

.
\end_layout

\begin_layout Subsection
Frish-Waugh-Lovell Theorem
\end_layout

\begin_layout Standard
This theorem is the sample version of the subvector regression.
\end_layout

\begin_layout Standard
If 
\begin_inset Formula $Y=X_{1}\beta_{1}+X_{2}\beta_{2}+e$
\end_inset

, then 
\begin_inset Formula $\widehat{\beta}_{1}=\left(X_{1}'M_{X_{2}}X_{1}\right)^{-1}X_{1}'M_{X_{2}}Y.$
\end_inset


\end_layout

\begin_layout Section
Statistical Properties of Least Squares
\end_layout

\begin_layout Standard
To talk about the statistical properties in finite sample, we impose the
 following assumptions.
\end_layout

\begin_layout Enumerate
The data 
\begin_inset Formula $\left(y_{i},x_{i}\right)_{i=1}^{n}$
\end_inset

 is a random sample from the same data generating process 
\begin_inset Formula $y_{i}=x_{i}'\beta+e_{i}$
\end_inset

.
\end_layout

\begin_layout Enumerate
\begin_inset Formula $e_{i}|x_{i}\sim N\left(0,\sigma^{2}\right)$
\end_inset

.
\end_layout

\begin_layout Subsection
Maximum Likelihood Estimation*
\end_layout

\begin_layout Standard
Under the normality assumption, 
\begin_inset Formula $y_{i}|x_{i}\sim N\left(x_{i}'\beta,\gamma\right)$
\end_inset

, where 
\begin_inset Formula $\gamma=\sigma^{2}$
\end_inset

.
 The 
\emph on
conditional
\emph default
 likelihood of observing a sample 
\begin_inset Formula $\left(y_{i},x_{i}\right)_{i=1}^{n}$
\end_inset

 is 
\begin_inset Formula 
\[
\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi\gamma}}\exp\left(-\frac{1}{2\gamma}\left(y_{i}-x_{i}'\beta\right)^{2}\right),
\]

\end_inset

 and the (conditional) log-likelihood function is 
\begin_inset Formula 
\[
L\left(\beta,\gamma\right)=-\frac{n}{2}\log2\pi-\frac{n}{2}\log\gamma-\frac{1}{2\gamma}\sum_{i=1}^{n}\left(y_{i}-x_{i}'\beta\right)^{2}.
\]

\end_inset

Therefore, the maximum likelihood estimator (MLE) coincides with the OLS
 estimator, and 
\begin_inset Formula $\widehat{\gamma}_{\mathrm{MLE}}=\widehat{e}'\widehat{e}/n$
\end_inset

.
\end_layout

\begin_layout Subsection
Finite Sample Distribution
\end_layout

\begin_layout Standard
We can show the finite-sample exact distribution of 
\begin_inset Formula $\widehat{\beta}$
\end_inset

.
 
\emph on
Finite sample distribution
\emph default
 means that the distribution holds for any 
\begin_inset Formula $n$
\end_inset

; it is in contrast to 
\emph on
asymptotic distribution
\emph default
, which holds only when 
\begin_inset Formula $n$
\end_inset

 is arbitrarily large.
 
\end_layout

\begin_layout Standard
Since 
\begin_inset Formula 
\[
\widehat{\beta}=\left(X'X\right)^{-1}X'y=\left(X'X\right)^{-1}X'\left(X'\beta+e\right)=\beta+\left(X'X\right)^{-1}X'e,
\]

\end_inset

 we have the estimator 
\begin_inset Formula $\widehat{\beta}|X\sim N\left(\beta,\sigma^{2}\left(X'X\right)^{-1}\right)$
\end_inset

, and 
\begin_inset Formula 
\[
\widehat{\beta}_{k}|X\sim N\left(\beta_{k},\sigma^{2}\eta_{k}'\left(X'X\right)^{-1}\eta_{k}\right)\sim N\left(\beta_{k},\sigma^{2}\left(X'X\right)_{kk}^{-1}\right),
\]

\end_inset

 where 
\begin_inset Formula $\eta_{k}=\left(1\left\{ l=k\right\} \right)_{l=1,\ldots,K}$
\end_inset

 is the selector of the 
\begin_inset Formula $k$
\end_inset

-th element.
 
\end_layout

\begin_layout Standard
In reality, 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is an unknown parameter, and 
\begin_inset Formula 
\[
s^{2}=\widehat{e}'\widehat{e}/\left(n-K\right)=e'M_{X}e/\left(n-K\right)
\]

\end_inset

 is an unbiased estimator of 
\begin_inset Formula $\sigma^{2}$
\end_inset

.
 Consider the 
\begin_inset Formula $T$
\end_inset

-statistic
\begin_inset Formula 
\[
T_{k}=\frac{\widehat{\beta}_{k}-\beta_{k}}{\sqrt{s^{2}\left[\left(X'X\right)^{-1}\right]_{kk}}}=\frac{\left(\widehat{\beta}_{k}-\beta_{k}\right)/\sqrt{\sigma^{2}\left[\left(X'X\right)^{-1}\right]_{kk}}}{\sqrt{\frac{e'}{\sigma}M_{X}\frac{e}{\sigma}/\left(n-K\right)}}.
\]

\end_inset

The numerator follows a standard normal, and the denominator follows 
\begin_inset Formula $\frac{1}{n-K}\chi^{2}\left(n-K\right)$
\end_inset

.
 Moreover, the numerator and the denominator are independent.
 As a result, 
\begin_inset Formula $T_{k}\sim t\left(n-K\right)$
\end_inset

.
\end_layout

\begin_layout Subsection
Mean and Variance
\end_layout

\begin_layout Standard
Now we relax the normality assumption and statistical independence.
 Instead, we assume a random sample and 
\begin_inset Formula 
\begin{eqnarray}
y_{i} & = & x_{i}'\beta+e_{i}\nonumber \\
E[e_{i}|x_{i}] & = & 0\label{eq:mean_indep}\\
E\left[e_{i}^{2}|x_{i}\right] & = & \sigma^{2}.\label{eq:homo}
\end{eqnarray}

\end_inset

(
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:mean_indep"

\end_inset

) is the 
\emph on
mean independence
\emph default
 assumption, and (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:homo"

\end_inset

) is the 
\emph on
homoskedasticity
\emph default
 assumption.
\end_layout

\begin_layout Example*
(Heteroskedasticity) If 
\begin_inset Formula $e_{i}=x_{i}u_{i}$
\end_inset

, where 
\begin_inset Formula $x_{i}$
\end_inset

 is a scalar random variable, 
\begin_inset Formula $u_{i}$
\end_inset

 is independent of 
\begin_inset Formula $x_{i}$
\end_inset

, 
\begin_inset Formula $E\left[u_{i}\right]=0$
\end_inset

 and 
\begin_inset Formula $E\left[u_{i}^{2}\right]=\sigma^{2}$
\end_inset

.
 Then 
\begin_inset Formula $E\left[e_{i}|x_{i}\right]=0$
\end_inset

 but 
\begin_inset Formula $E\left[e_{i}^{2}|x_{i}\right]=\sigma_{i}^{2}x_{i}^{2}$
\end_inset

 is a function of 
\begin_inset Formula $x_{i}$
\end_inset

.
 We say 
\begin_inset Formula $e_{i}^{2}$
\end_inset

 is a heteroskedastic error.
\end_layout

\begin_layout Standard
These assumptions are about the first and second moment of 
\begin_inset Formula $e_{i}$
\end_inset

 conditional on 
\begin_inset Formula $x_{i}$
\end_inset

.
 Unlike the normality assumption, they do not restrict the entire distribution
 of 
\begin_inset Formula $e_{i}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Unbiasedness: 
\begin_inset Formula 
\[
E\left[\widehat{\beta}|X\right]=E\left[\left(X'X\right)^{-1}XY|X\right]=E\left[\left(X'X\right)^{-1}X\left(X'\beta+e\right)|X\right]=\beta.
\]

\end_inset

 Unbiasedness does not rely on homoskedasticity.
\end_layout

\begin_layout Itemize
Variance: 
\begin_inset Formula 
\begin{eqnarray*}
\mathrm{var}\left(\widehat{\beta}|X\right) & = & E\left[\left(\widehat{\beta}-E\widehat{\beta}\right)\left(\widehat{\beta}-E\widehat{\beta}\right)'|X\right]\\
 & = & E\left[\left(\widehat{\beta}-\beta\right)\left(\widehat{\beta}-\beta\right)'|X\right]\\
 & = & E\left[\left(X'X\right)^{-1}X'ee'X\left(X'X\right)^{-1}|X\right]\\
 & = & \left(X'X\right)^{-1}X'E\left[ee'|X\right]X\left(X'X\right)^{-1}\\
 & = & \left(X'X\right)^{-1}X'\left(\sigma^{2}I_{n}\right)X\left(X'X\right)^{-1}\\
 & = & \sigma^{2}\left(X'X\right)^{-1}.
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
Gauss-Markov Theorem*
\end_layout

\begin_layout Standard
Gauss-Markov theorem justifies the OLS estimator as the efficient estimator
 among all linear unbiased ones.
 
\emph on
Efficient
\emph default
 here means that it enjoys the smallest variance in a family of estimators.
\end_layout

\begin_layout Standard
There are numerous linearly unbiased estimators.
 For example, 
\begin_inset Formula $\left(Z'X\right)^{-1}Z'y$
\end_inset

 for 
\begin_inset Formula $z_{i}=x_{i}^{2}$
\end_inset

 is unbiased because 
\begin_inset Formula $E\left[\left(Z'X\right)^{-1}Z'y\right]=E\left[\left(Z'X\right)^{-1}Z'\left(X\beta+e\right)\right]=\beta$
\end_inset

.
 
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $\tilde{\beta}=A'y$
\end_inset

 be a generic linear estimator, where 
\begin_inset Formula $A$
\end_inset

 is any 
\begin_inset Formula $n\times K$
\end_inset

 functions of 
\begin_inset Formula $X$
\end_inset

.
 As 
\begin_inset Formula 
\[
E\left[A'y|X\right]=E\left[A'\left(X\beta+e\right)|X\right]=A'X\beta.
\]

\end_inset

So the linearity and unbiasedness of 
\begin_inset Formula $\tilde{\beta}$
\end_inset

 implies 
\begin_inset Formula $A'X=I_{n}$
\end_inset

.
 Moreover, the variance 
\begin_inset Formula 
\[
\mbox{var}\left(A'y|X\right)=E\left[\left(A'y-\beta\right)\left(A'y-\beta\right)'|X\right]=E\left[A'ee'A|X\right]=\sigma^{2}A'A.
\]

\end_inset

 Let 
\begin_inset Formula $C=A-X\left(X'X\right)^{-1}.$
\end_inset


\begin_inset Formula 
\begin{eqnarray*}
 &  & A'A-\left(X'X\right)^{-1}\\
 & = & \left(C+X\left(X'X\right)^{-1}\right)'\left(C+X\left(X'X\right)^{-1}\right)-\left(X'X\right)^{-1}\\
 & = & C'C+\left(X'X\right)^{-1}X'C+C'X\left(X'X\right)^{-1}=C'C,
\end{eqnarray*}

\end_inset

where the last equality follows as 
\begin_inset Formula 
\[
\left(X'X\right)^{-1}X'C=\left(X'X\right)^{-1}X'\left(A-X\left(X'X\right)^{-1}\right)=\left(X'X\right)^{-1}-\left(X'X\right)^{-1}=0.
\]

\end_inset

Therefore 
\begin_inset Formula $A'A-\left(X'X\right)^{-1}$
\end_inset

 is a positive semi-definite matrix.
 The variance of any 
\begin_inset Formula $\tilde{\beta}$
\end_inset

 is no smaller than the OLS estimator 
\begin_inset Formula $\widehat{\beta}$
\end_inset

.
 
\end_layout

\begin_layout Standard
Homoskedasticity is a restrictive assumption.
 Under homoskedasticity, 
\begin_inset Formula $\mathrm{var}\left(\widehat{\beta}\right)=\sigma^{2}\left(X'X\right)^{-1}$
\end_inset

.
 Popular estimator of 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is the sample mean of the residuals 
\begin_inset Formula $\widehat{\sigma}^{2}=\frac{1}{n}\widehat{e}'\widehat{e}$
\end_inset

 or the unbiased one 
\begin_inset Formula $s^{2}=\frac{1}{n-K}\widehat{e}'\widehat{e}$
\end_inset

.
 Under heteroskedasticity, Gauss-Markov theorem does not apply.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Heteroskedasticity
\end_layout

\begin_layout Plain Layout
Under heteroskedasticity, 
\begin_inset Formula 
\[
\mathrm{var}\left(\widehat{\beta}\right)=\left(X'X\right)^{-1}X'DX\left(X'X\right)^{-1}
\]

\end_inset

where 
\begin_inset Formula $D=\mbox{diag}\left(\sigma_{1}^{2},\ldots,\sigma_{n}^{2}\right)$
\end_inset

.
 We propose to estimate 
\begin_inset Formula $\widehat{\sigma}_{i}^{2}=\widehat{e}_{i}^{2}$
\end_inset

 so that 
\begin_inset Formula $X'DX=\sum_{i=1}^{n}x_{i}x_{i}'\widehat{e}_{i}^{2}$
\end_inset

.
 
\end_layout

\end_inset


\end_layout

\end_body
\end_document
