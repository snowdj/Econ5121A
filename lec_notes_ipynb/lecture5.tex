
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \renewcommand{\hat}{\widehat}
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Lecture 5: Statistical Inference}
    \author{Zhentao Shi}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    
    \maketitle
    
    

    
    Notation: \(\mathbf{X}\) denotes a random variable or random vector.
\(\mathbf{x}\) is its realization.

\section{Hypothesis Testing}\label{hypothesis-testing}

\begin{itemize}
\item
  A \emph{hypothesis} is a statement about the parameter space
  \(\Theta\).
\item
  The \emph{null hypothesis} \(\Theta_{0}\) is a subset of \(\Theta\) of
  interest, ideally suggested by scientific theory.
\item
  The \emph{alternative hypothesis}
  \(\Theta_{1}=\Theta\backslash\Theta_{0}\) is the complement of
  \(\Theta_{0}\). 
\item \emph{Hypothesis testing} is a decision whether to
  accept the null hypothesis or to reject it according to the observed
  evidence.
\item
  If \(\Theta_0\) is a singleton, we call it a \emph{simple hypothesis};
  otherwise we call it a \emph{composite hypothesis}.
\item
  A \emph{test function} is a mapping
  \[\phi:\mathcal{X}^{n}\mapsto\left\{ 0,1\right\},\] where
  \(\mathcal{X}\) is the sample space. We accept the null hypothesis if
  \(\phi\left(\mathbf{x}\right)=0\), or reject it if
  \(\phi\left(\mathbf{x}\right)=1\).
\item
  The \emph{acceptance region} is defined as
  \(A_{\phi}=\left\{ \mathbf{x}\in\mathcal{X}^{n}:\phi\left(\mathbf{x}\right)=0\right\} ,\)
  and the \emph{rejection region} is
  \(R_{\phi}=\left\{ \mathbf{x}\in\mathcal{X}^{n}:\phi\left(\mathbf{x}\right)=1\right\} .\)
\item
  The \emph{power function} of the test \(\phi\) is
  \[\beta_{\phi}\left(\theta\right)=P_{\theta}\left(\phi\left(\mathbf{X}\right)=1\right)=E_{\theta}\left(\phi\left(\mathbf{X}\right)\right).\]
  The power function measures, at a given point \(\theta\), the
  probability that the test function rejects the null.
\item
  The \emph{power} of \(\phi\) at \(\theta\) for some
  \(\theta\in\Theta_{1}\) is defined as the value of
  \(\beta_{\phi}\left(\theta\right)\). 
  The \emph{size} of the test \(\phi\) is define as
  \(\alpha=\sup_{\theta\in\Theta_{0}}\beta_{\phi}\left(\theta\right).\)
  Notice that the definition of power depends on a \(\theta\) in the
  alternative, whereas that of size is independent of \(\theta\) as it
  takes the supremum over the set of null \(\Theta_0\).
\item
  The \emph{level} of the test \(\phi\) is a value
  \(\alpha\in\left(0,1\right)\) such that
  \(\alpha\geq\sup_{\theta\in\Theta_{0}}\beta_{\phi}\left(\theta\right)\),
  which is often used when it is difficult to attain the exact supremum.
  A test of size $\alpha$ is also of level $\alpha$ or bigger; while a test of level $\alpha$
  must have size smaller or equal to $\alpha$.
\end{itemize}

    \begin{verbatim}
     | decision     |  reject $H_{1}$  | reject $H_{0}$
     |--------------|------------------| ---------------
     | $H_{0}$ true |     correct      | Type I error
     | $H_{0}$ false| Type II error    |   correct
\end{verbatim}

    \begin{itemize}
\tightlist
\item
  size = \emph{P}(reject \(H_{0}\) when \(H_{0}\) true)
\item
  power = \emph{P}(reject \(H_{0}\) when \(H_{0}\) false)
\item
  The \emph{probability of committing Type I error} is
  \(\beta_{\phi}\left(\theta\right)\) for some \(\theta\in\Theta_{0}\).
\item
  The \emph{probability of committing Type II error} is
  \(1-\beta_{\phi}\left(\theta\right)\) for \(\theta\in\Theta_{1}\).
\end{itemize}

    The philosophy on the hypothesis testing has been debated for centuries.
At present the prevailing framework in statistics textbooks is the
frequentist perspective. A frequentist views the parameter as a fixed
constant, and they keep a conservative attitude about the Type I error.
Only if overwhelming evidence is demonstrated should a researcher reject
the null. Under the philosophy of protecting the null hypothesis, a
desirable test should have a small level. Conventionally we take
\(\alpha=0.01,\) 0.05 or 0.1. There can be many tests of the correct
size.

\bigskip 

\textbf{Example} A trivial test function,
\(\phi(\mathbf{X})=1\left\{ 0\leq U\leq\alpha\right\}\), where \(U\) is
a random variable from a uniform distribution on \(\left[0,1\right]\),
has correct size $\alpha$ but no power. Another trivial test function
\(\phi\left(\mathbf{X}\right)=1\) has the biggest power but useless
size.

\bigskip 

    Usually, we design a test by proposing a test statistic
\(T_{n}:\mathcal{X}^{n}\mapsto\mathbb{R}^{+}\) and a critical value
\(c_{1-\alpha}\). Given \(T_n\) and \(c_{1-\alpha}\), we write the test
function as
\[\phi\left(\mathbf{X}\right)=1\left\{ T_{n}\left(\mathbf{X}\right)>c_{1-\alpha}\right\}.\]
To ensure such a \(\phi\left(\mathbf{x}\right)\) has correct size, we
figure out the distribution of \(T_{n}\) under the null hypothesis
(called the \emph{null distribution}), and choose a critical value
\(c_{1-\alpha}\) according to the null distribution and the desirable
size or level \(\alpha\).

The concept of \emph{level} is useful if we do not have sufficient information to
derive the exact size of a test.

\bigskip

\textbf{Example} If \(\left(X_{1i},X_{2i}\right)_{i=1}^{n}\) are
randomly drawn from some unknown joint distribution, but we know the
marginal distribution is \(X_{ji}\sim N\left(\theta_{j},1\right)\), for
\(j=1,2\). In order to test the joint hypothesis
\(\theta_{1}=\theta_{2}=0\), we can construct a test function
\[\phi\left(\mathbf{X}_{1},\mathbf{X}_{2}\right)=1\left\{ \left\{ \sqrt{n}\left|\overline{X}_{1}\right|\geq c_{1-\alpha/4}\right\} \cup\left\{ \sqrt{n}\left|\overline{X}_{2}\right|\geq c_{1-\alpha/4}\right\} \right\} ,\]
where \(c_{1-\alpha/4}\) is the \(\left(1-\alpha/4\right)\)-th quantile
of the standard normal distribution. The level of this test is
\[\begin{aligned}
P_{\theta_{1}=\theta_{2}=0}\left(\phi\left(\mathbf{X}_{1},\mathbf{X}_{2}\right)\right) & \leq P_{\theta_{1}=0}\left(\sqrt{n}\left|\overline{X}_{1}\right|\geq c_{1-\alpha/4}\right)+P_{\theta_{2}=0}\left(\sqrt{n}\left|\overline{X}_{2}\right|\geq c_{1-\alpha/4}\right)\\
 & =\alpha/2+\alpha/2=\alpha.\end{aligned}\] where the inequality
follows by the \emph{Bonferroni inequality}
\(P\left(A\cup B\right)\leq P\left(A\right)+P\left(B\right)\).
Therefore, the level of
\(\phi\left(\mathbf{X}_{1},\mathbf{X}_{2}\right)\) is \(\alpha\), but
the exact size is unknown without the knowledge of the joint
distribution. (Even if we know the correlation of \(X_{1i}\) and
\(X_{2i}\), putting two marginally normal distributions together does
not make a jointly normal vector in general.)

\bigskip


Denote the class of test
functions of level \(\alpha\) as
\(\Psi_{\alpha}=\left\{ \phi:\sup_{\theta\in\Theta_{0}}\beta_{\phi}\left(\theta\right)\leq\alpha\right\}\).
A \emph{uniformly most powerful test} \(\phi^{*}\in\Psi_{\alpha}\) is a
test function such that, for every \(\phi\in\Psi_{\alpha},\)
\[\beta_{\phi^{*}}\left(\theta\right)\geq\beta_{\phi}\left(\theta\right)\]
uniformly over \(\theta\in\Theta_{1}\).

\textbf{Example} Suppose a random sample of size 6 is generated from
\[\left(X_{1},\ldots,X_{6}\right)\sim\text{i.i.d.}N\left(\theta,1\right),\]
where \(\theta\) is unknown. We want to infer the population mean of the
normal distribution. The null hypothesis is \(H_{0}\): \(\theta\leq0\)
and the alternative is \(H_{1}\): \(\theta>0\). All tests in
\[\Psi=\left\{ 1\left\{ \bar{X}\geq c/\sqrt{6}\right\} :c\geq1.64\right\}\]
has the correct level. Since
\(\bar{X}=N\left(\theta,1/6 \right)\), the power function for
those in \(\Psi\) is
\[\beta_{\phi}\left(\theta\right)=P\left(\bar{X}\geq\frac{c}{\sqrt{6}}\right)=P\left(\sqrt{6}\left(\bar{X}-\theta\right)\geq c-\sqrt{6}\theta\right)=1-\Phi\left(c-\sqrt{6}\theta\right)\]
where \(\Phi\) is the cdf of standard normal. It is clear that
\(\beta_{\phi}\left(\theta\right)\) is monotonically decreasing in
\(c\). Thus the test function
\[\phi\left(\mathbf{X}\right)=1\left\{ \bar{X}\geq 1.64/\sqrt{6}\right\}\]
is the most powerful test in \(\Psi\), as \(c=1.64\) is the lower bound
that \(\Psi\) allows.

    Another commonly used indicator in hypothesis testing is \(p\)-value:
\[\sup_{\theta\in\Theta_{0}}P_{\theta}\left(T_{n}\left(\mathbf{x}\right)\leq T_{n}\left(\mathbf{X}\right)\right).\]
In the above expression, \(T_{n}\left(\mathbf{x}\right)\) is the
realized value of the test statistic \(T_{n}\), while
\(T_{n}\left(\mathbf{X}\right)\) is the random variable generated by
\(\mathbf{X}\) under the null \(\theta\in\Theta_{0}\). The
interpretation of the \(p\)-value is tricky. \(p\)-value is the
probability that we observe \(T_n (\mathbf{X})\) being greater than the
realized \(T_n (\mathbf{x} )\) if the null hypothesis is true.
\(p\)-value is \emph{not} the probability that the null hypothesis is
true. Under the frequentist perspective, the null hypothesis is either
true or false, with certainty. The randomness of a test comes only from
sampling, not from the hypothesis.

It measures whether the data is consistent with the null hypothesis, or
whether the evidence from the data is compatible with the null
hypothesis. \(p\)-value is closely related to the corresponding test.
When \(p\)-value is smaller than the specified test size \(\alpha\), the
test rejects the null hypothesis.

    \section{Confidence Interval}\label{confidence-interval}

An \emph{interval estimate} is a function
\(C:\mathcal{X}^{n}\mapsto\left\{ \Theta':\Theta'\subseteq\Theta\right\}\)
that maps a point in the sample space to a subset of the parameter
space. The \emph{coverage probability} of an \emph{interval estimator}
\(C\left(\mathbf{X}\right)\) is defined as
\(P_{\theta}\left(\theta\in C\left(\mathbf{X}\right)\right)\). The
coverage probability is the frequency that the interval estimator
captures the true parameter that generates the sample (From the
frequentist perspective, the parameter is fixed while the region is
random). It is \emph{not} the probability that \(\theta\) is inside the
given region (From the Bayesian perspective, the parameter is random
while the region is fixed conditional on \(\mathbf{X}\).)

Suppose a random sample of size 6 is generated from
\[\left(X_{1},\ldots,X_{6}\right)\sim\text{i.i.d. }N\left(\theta,1\right).\]
Find the coverage probability of the random interval
\[\left[\bar{X}-1.96/\sqrt{6},\bar{X}+1.96/\sqrt{6}\right].\]

Hypothesis testing and confidence interval are closely related.
Sometimes it is difficult to directly construct the confidence interval,
but easier to test a hypothesis. One way to construct confidence
interval is by \emph{inverting a corresponding test}. Suppose \(\phi\)
is a test of size \(\alpha\). If \(C\left(\mathbf{X}\right)\) is
constructed as
\[C\left(\mathbf{x}\right)=\left\{ \theta\in\Theta:\phi_{\theta}\left(\mathbf{x}\right)=0\right\},\]
then its coverage probability
\[P_{\theta}\left(\theta\in C\left(\mathbf{X}\right)\right)=1-P_{\theta}\left(\phi_{\theta}\left(\mathbf{X}\right)=1\right)=1-\alpha.\]





\section{Bayesian Credible Set}



The Bayesian framework offers a coherent and natural language for statistical decision.
However, the major criticism against Bayesian statistics is the arbitrariness of the choice
of the prior. 

In the Bayesian framework, both the data $\mathbf{X}_n$ and the parameter $\theta$ are random variables.
Before she observes the data, she holds a \emph{prior distribution} $\pi$ about $\theta$.
After observing the data, she updates the prior distribution to a
\emph{posterior distribution} $p(\theta | \mathbf{X}_n)$. 
The \emph{Bayes Theorem} connects the prior and the posterior as
\[
p( \theta| \mathbf{X}_n ) \propto f( \mathbf{X}_n | \theta ) \pi(\theta)
\]
where $f( \mathbf{X}_n | \theta )$ is the likelihood function.




Here is a classical example to illustrate the Bayesian approach of statistical inference.
Suppose we have an iid sample $(X_1,\ldots,X_n)$ drawn from a normal distribution with 
unknown $\theta$ and known $\sigma$. For a researcher with a prior distribution 
$\theta \sim N(\theta_0, \sigma_0^2)$, her posterior distribution is,
by some routine calculation, the posterior is also a normal distribution
\[
p(\theta | \mathbf{x}) \sim N\left( \tilde{\theta}, \tilde{\sigma}^2   \right),
\]
where $ \tilde{\theta} = \frac{\sigma^2}{n \sigma_0^2 + \sigma^2} \theta_0 
+ \frac{n\sigma_0^2}{n\sigma_0^2 + \sigma^2} \bar{x} $ 
and $\tilde{\sigma}^2 = \frac{\sigma_0^2 \sigma^2}{n\sigma_0^2 + \sigma^2}$.
Thus the Bayesian credible set is 
\[
\left( \tilde{\theta} - z_{1-\alpha/2 } \cdot \tilde{\sigma},  \tilde{\theta} + z_{1-\alpha/2 }\cdot  \tilde{\sigma}  \right).
\]
This posterior distribution depends on $\theta_0$ and $\sigma_0^2$ from the posterior. 
When the sample size is sufficiently large 
the posterior can be approximated by $N( \bar{x}, \sigma^2 / n )$,
where the prior information is overwhelmed by the information accumulated from the data.

In contrast, a frequentist estimates $\hat{\theta} = \bar{x} \sim N(\theta, \sigma^2 / n) $. Her 
confidence interval is 
$$
\left( \bar{x} - z_{1-\alpha/2 } \cdot \sigma/\sqrt{n},  
\bar{x} - z_{1-\alpha/2 } \cdot \sigma/\sqrt{n} \right).
$$



\section{Application in OLS}\label{application-in-ols}

\subsection{Wald Test}\label{wald-test}

Suppose the OLS estimator \(\widehat{\beta}\) is asymptotic normal, i.e.
\[\sqrt{n}\left(\widehat{\beta}-\beta\right)\stackrel{d}{\to}N\left(0,\Omega\right)\]
where \(\Omega\) is a \(K\times K\) positive definite covariance matrix
and \(R\) is a \(q\times K\) constant matrix, then
\(R\sqrt{n}\left(\widehat{\beta}-\beta\right)\stackrel{d}{\to}N\left(0,R\Omega R'\right)\).
Moreover, if \(\mbox{rank}\left(R\right)=q\), then
\[n\left(\widehat{\beta}-\beta\right)'R'\left(R\Omega R'\right)^{-1}R\left(\widehat{\beta}-\beta\right)\stackrel{d}{\to}\chi_{q}^{2}.\]
Now we intend to test the null hypothesis \(R\beta=r\). Under the null,
the Wald statistic
\[W_{n}=n\left(R\widehat{\beta}-r\right)'\left(R\widehat{\Omega}R'\right)^{-1}\left(R\widehat{\beta}-r\right)\stackrel{d}{\to}\chi_{q}^{2}\]
where \(\widehat{\Omega}\) is a consistent estimator of \(\Omega\).

    \textbf{Example} (Single test) In a linear regression \[\begin{aligned}
y & =  x_{i}'\beta+e_{i}=\sum_{k=1}^{5}\beta_{k}x_{ik}+e_{i}.\nonumber \\
E\left[e_{i}x_{i}\right] & =  \mathbf{0}_{5},\label{eq:example}\end{aligned}
\] where \(y\) is wage and
\[x=\left(\mbox{edu},\mbox{age},\mbox{experience},\mbox{experience}^{2},1\right)'.\]
To test whether \emph{education} affects \emph{wage}, we specify the
null hypothesis \(\beta_{1}=0\). Let \(R=\left(1,0,0,0,0\right)\).
\[\sqrt{n}\widehat{\beta}_{1}=\sqrt{n}\left(\widehat{\beta}_{1}-\beta_{1}\right)=\sqrt{n}R\left(\widehat{\beta}-\beta\right)\stackrel{d}{\to}N\left(0,R\Omega R'\right)\sim N\left(0,\Omega_{11}\right),\label{eq:R11}\]
where \(\Omega{}_{11}\) is the \(\left(1,1\right)\) (scalar) element of
\(\Omega\). Therefore,
\[\sqrt{n}\frac{\widehat{\beta}_{1}}{\widehat{\Omega}_{11}^{1/2}}=\sqrt{\frac{\Omega_{11}}{\widehat{\Omega}_{11}}}\sqrt{n}\frac{\widehat{\beta}_{1}}{\Omega_{11}^{1/2}}\]
If \(\widehat{\Omega}\stackrel{p}{\to}\Omega\), then
\(\left(\Omega_{11}/\widehat{\Omega}_{11}\right)^{1/2}\stackrel{p}{\to}1\)
by the continuous mapping theorem. As
\(\sqrt{n}\widehat{\beta}_{1}/\Omega_{11}^{1/2}\stackrel{d}{\to}N\left(0,1\right)\),
we conclude
\(\sqrt{n}\widehat{\beta}_{1}/\widehat{\Omega}_{11}^{1/2}\stackrel{d}{\to}N\left(0,1\right).\)

    The above example is a test about a single coefficient, and the test
statistic is essentially a \emph{t}-statistic. The following example
gives a test about a joint hypothesis.

\textbf{Example} (Joint test) We want to simultaneously test
\(\beta_{1}=1\) and \(\beta_{3}+\beta_{4}=2\) in the above example. The
null hypothesis can be expressed in the general form \(R\beta=r\), where
the restriction matrix \(R\) is \[R=\begin{pmatrix}1 & 0 & 0 & 0 & 0\\
0 & 0 & 1 & 1 & 0
\end{pmatrix}\] and \(r=\left(1,2\right)'\). Once we figure out \(R\),
it is routine to construct the test.

    These two examples are linear restrictions. In order to test a nonlinear
regression, we need the so-called \emph{delta method}.

\textbf{Delta method} If
\(\sqrt{n}\left(\widehat{\theta}-\theta_{0}\right)\stackrel{d}{\to}N\left(0,\Omega_{K\times K}\right)\),
and \(f:\mathbb{R}^{K}\mapsto\mathbb{R}^{q}\) is a continuously
differentiable function for some \(q\leq K\), then
\[\sqrt{n}\left(f\left(\widehat{\theta}\right)-f\left(\theta_{0}\right)\right)\stackrel{d}{\to}N\left(0,\frac{\partial f}{\partial\theta}\left(\theta_{0}\right)\Omega\frac{\partial f}{\partial\theta}\left(\theta_{0}\right)'\right).\]
This result can be easily shown by a mean-value expansion
\[
f(\hat{\theta} ) - f(\theta_0) = \frac{ \partial f(\tilde{\theta}) }{\partial \theta} 
(\hat{\theta} - \theta_0)
\]
where $\tilde{\theta}$ lies on the line segment connecting $\hat{\theta}$ and $\theta_0$.
Multiply both sides by $\sqrt{n}$ and notice $\tilde{\theta} \stackrel{p}{\to} \theta_0$,
by Slutsky theorem we have 
$\sqrt{n} (f(\hat{\theta} ) - f(\theta_0) ) \stackrel{d}{\to}
\frac{\partial f}{\partial\theta}\left(\theta_{0}\right) N(0,\Omega).
 $


    In the example of linear regression, the optimal experience level can be
found by setting the first order condition with respective to experience
to set, \(\beta_{3}+2\beta_{4}\mbox{experience}^{*}=0\). We test the
hypothesis that the optimal experience level is 20 years; in other
words, \[\mbox{experience}^{*}=-\frac{\beta_{3}}{2\beta_{4}}=20.\] This
is a nonlinear hypothesis. If \(q\leq K\) where \(q\) is the number of
restrictions, we have
\[n\left(f\left(\widehat{\theta}\right)-f\left(\theta_{0}\right)\right)'\left(\frac{\partial f}{\partial\theta}\left(\theta_{0}\right)\Omega\frac{\partial f}{\partial\theta}\left(\theta_{0}\right)'\right)^{-1}\left(f\left(\widehat{\theta}\right)-f\left(\theta_{0}\right)\right)\stackrel{d}{\to}\chi_{q}^{2},\]
where in this example, \(\theta=\beta\),
\(f\left(\beta\right)=-\beta_{3}/\left(2\beta_{4}\right)\). The gradient
\[\frac{\partial f}{\partial\beta}\left(\beta\right)=\left(0,0,-\frac{1}{2\beta_{4}},\frac{\beta_{3}}{2\beta_{4}^{2}}\right)\]
Since \(\widehat{\beta}\stackrel{p}{\to}\beta_{0}\), by the continuous
mapping theorem theorem, if \(\beta_{0,4}\neq0\), we have
\(\frac{\partial}{\partial\beta}f\left(\widehat{\beta}\right)\stackrel{p}{\to}\frac{\partial}{\partial\beta}f\left(\beta_{0}\right)\).
Therefore, the (nonlinear) Wald test is
\[W_{n}=n\left(f\left(\widehat{\beta}\right)-20\right)'\left(\frac{\partial f}{\partial\beta}\left(\widehat{\beta}\right)\widehat{\Omega}\frac{\partial f}{\partial\beta}\left(\widehat{\beta}\right)'\right)^{-1}\left(f\left(\widehat{\beta}\right)-20\right)\stackrel{d}{\to}\chi_{1}^{2}.\]
This is a valid test with correct asymptotic size.

However, we can equivalently state the null hypothesis as
\(\beta_{3}+40\beta_{4}=0\) and we can construct a Wald statistic
accordingly. In general, a linear hypothesis is preferred to a nonlinear
one, due to the approximation error in the delta method under the null
and more importantly the invalidity of the Taylor expansion under the
alternative. It also highlights the problem of Wald test being
\emph{variant} for re-parametrization.

    \subsection{Lagrangian Multiplier
Test*}\label{lagrangian-multiplier-test}

Restricted least square
\[\min_{\beta}\left(y-X\beta\right)'\left(y-X\beta\right)\mbox{ s.t. }R\beta=r.\]
Turn it into an unrestricted problem
\[L\left(\beta,\lambda\right)=\frac{1}{2n}\left(y-X\beta\right)'\left(y-X\beta\right)+\lambda'\left(R\beta-r\right).\]
The first-order condition 
\begin{align*}
\frac{\partial}{\partial\beta}L & =  -\frac{1}{n}X'\left(y-X\tilde{\beta}\right)+\tilde{\lambda}R=-\frac{1}{n}X'e+\frac{1}{n}X'X\left(\tilde{\beta}-\beta^{*}\right)+R'\tilde{\lambda}=0.\\
\frac{\partial}{\partial\lambda}L & =  R\tilde{\beta}-r=R\left(\tilde{\beta}-\beta^{*}\right)=0
\end{align*}
Combine these two equations into a linear system,
\[
\begin{pmatrix}
\widehat{Q} & R'\\
R & 0
\end{pmatrix}\begin{pmatrix}\tilde{\beta}-\beta^{*}\\
\tilde{\lambda}
\end{pmatrix}=\begin{pmatrix}\frac{1}{n}X'e\\
0
\end{pmatrix},\]
where $\hat{Q} = X'X/n$.

Thus we can explicitly express the estimator as
\[\begin{aligned}
  \begin{pmatrix}\tilde{\beta}-\beta^{*}\\
\tilde{\lambda}
\end{pmatrix}
& =\begin{pmatrix}\widehat{Q} & R'\\
R & 0
\end{pmatrix}^{-1}
\begin{pmatrix}\frac{1}{n}X'e\\
0
\end{pmatrix}\\
 & =  \begin{pmatrix}\widehat{Q}^{-1}-\widehat{Q}^{-1}R'\left(R\widehat{Q}^{-1}R'\right)^{-1}R\widehat{Q}^{-1} & \widehat{Q}^{-1}R'\left(R\widehat{Q}^{-1}R'\right)^{-1}\\
\left(R\widehat{Q}^{-1}R'\right)^{-1}R\widehat{Q}^{-1}  & (R'Q^{-1}R)^{-1}
\end{pmatrix}
\begin{pmatrix}\frac{1}{n}X'e\\
0
\end{pmatrix}.\end{aligned}\]
We conclude that
\[
\sqrt{n}\tilde{\lambda}=\left(R\widehat{Q}^{-1}R'\right)^{-1}R\widehat{Q}^{-1}\frac{1}{\sqrt{n}}X'e \stackrel{d}{\to} N\left(0,\left(RQ^{-1}R'\right)^{-1}RQ^{-1}\Omega Q^{-1}R'\left(RQ^{-1}R'\right)^{-1}\right).\]
Let
\(W=\left(RQ^{-1}R'\right)^{-1}RQ^{-1}\Omega Q^{-1}R'\left(RQ^{-1}R'\right)^{-1}\),
we have
\[n\tilde{\lambda}'W^{-1}\tilde{\lambda}\stackrel{d}{\to} \chi_{q}^{2}.\] If
homoskedastic, then
\(W=\sigma^{2}\left(RQ^{-1}R'\right)^{-1}RQ^{-1}QQ^{-1}R'\left(RQ^{-1}R'\right)^{-1}=\sigma^{2}\left(RQ^{-1}R'\right)^{-1}.\)
Replace $W$ with the estimated $\hat{W}$,
\[\begin{aligned}
\frac{n\tilde{\lambda}'R\hat{Q}^{-1}R'\tilde{\lambda}}{\hat{\sigma}^{2}} 
& =\frac{1}{n\hat{\sigma}^{2}}\left(y-X\tilde{\beta}\right)'X \hat{Q}^{-1} R'
(R \hat{Q}^{-1} R')^{-1}
R \hat{Q}^{-1}
X'\left(y-X\tilde{\beta}\right)\\
 & =\frac{1}{n\hat{\sigma}^{2}}\left(y-X\tilde{\beta}\right)'P_{X \hat{Q}^{-1} R'}\left(y-X\tilde{\beta}\right).\end{aligned}\]

    \subsection{Likelihood-Ratio test*}\label{likelihood-ratio-test}

For likelihood ratio test, the starting point can be a criterion
function
\(L\left(\beta\right)=\left(y-X\beta\right)'\left(y-X\beta\right)\). It
does not have to be the likelihood function. \[\begin{aligned}
L\left(\tilde{\beta}\right)-L\left(\widehat{\beta}\right) & =\frac{\partial L}{\partial\beta}\left(\widehat{\beta}\right)+\frac{1}{2}\left(\tilde{\beta}-\widehat{\beta}\right)'\frac{\partial L}{\partial\beta\partial\beta}\left(\dot{\beta}\right)\left(\tilde{\beta}-\widehat{\beta}\right)\\
 & =0+\frac{1}{2}\left(\tilde{\beta}-\widehat{\beta}\right)'\widehat{Q}\left(\tilde{\beta}-\widehat{\beta}\right).\end{aligned}\]
From the derivation of LM test, we have \[\begin{aligned}
\sqrt{n}\left(\tilde{\beta}-\beta^{*}\right) 
 & =  \left(\widehat{Q}^{-1}-\widehat{Q}^{-1}R'\left(R\widehat{Q}^{-1}R'\right)^{-1}R\widehat{Q}^{-1}\right)\frac{1}{\sqrt{n}}X'e\\
 & =  \frac{1}{\sqrt{n}}\left(X'X\right)X'e-\widehat{Q}^{-1}R'\left(R\widehat{Q}^{-1}R'\right)^{-1}R\widehat{Q}^{-1}\frac{1}{\sqrt{n}}X'e\\
 & =  \sqrt{n}\left(\widehat{\beta}-\beta^{*}\right)-\widehat{Q}^{-1}R'\left(R\widehat{Q}^{-1}R'\right)^{-1}R\widehat{Q}^{-1}\frac{1}{\sqrt{n}}X'e
 \end{aligned}\] Therefore
\[\sqrt{n}\left(\tilde{\beta}-\widehat{\beta}\right)=-\widehat{Q}^{-1}R'\left(R\widehat{Q}^{-1}R'\right)^{-1}R\widehat{Q}^{-1}\frac{1}{\sqrt{n}}X'e\]
and \[\begin{aligned}
 &   n\left(\tilde{\beta}-\beta\right)'\widehat{Q}\left(\tilde{\beta}-\widehat{\beta}\right)\\
 & =  \frac{1}{\sqrt{n}}e'X\widehat{Q}^{-1}R'\left(R\widehat{Q}^{-1}R'\right)^{-1}R\widehat{Q}^{-1}\widehat{Q}\widehat{Q}^{-1}R'\left(R\widehat{Q}^{-1}R'\right)^{-1}R\widehat{Q}^{-1}\frac{1}{\sqrt{n}}X'e\\
 & =  \frac{1}{\sqrt{n}}e'X\widehat{Q}^{-1}R'\left(R\widehat{Q}^{-1}R'\right)^{-1}R\widehat{Q}^{-1}\frac{1}{\sqrt{n}}X'e
\end{aligned}\] In general, it is a quadratic form of normal
distributions. If homoskedastic, then
\[\left(R\widehat{Q}^{-1}R'\right)^{-1/2}R\widehat{Q}^{-1}\frac{1}{\sqrt{n}}X'e\]
has variance
\[\sigma^{2}\left(RQ^{-1}R'\right)^{-1/2}RQ^{-1}QQ^{-1}R'\left(RQ^{-1}R'\right)^{-1/2}=\sigma^{2}I_{q}.\]

We can view the optimization of the log-likelihood as a two-step
optimization with the inner step \(\sigma=\sigma\left(\beta\right)\). By
the envelop theorem, when we take derivative with respect to \(\beta\),
we can ignore the indirect effect of
\(\partial\sigma\left(\beta\right)/\partial\beta\).


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
