{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability\n",
    "\n",
    "Human beings are awed by uncertainty in daily life. In the old days, Egyptians\n",
    "consulted oracles, Hebrews inquired prophets, and Chinese counted on diviners\n",
    "to interpret tortoise shell or bone cracks.\n",
    "Even in today's Hong Kong fortunetellers are abundant.\n",
    "\n",
    "Probability theory is a philosophy about uncertainty.\n",
    "Over centuries, mathematicians strove to contribute to the understanding\n",
    "of randomness. As measure theory matured in the early 20th century,\n",
    "Russian mathematician Andrey Kolmogorov (1903-1987) laid the foundation of\n",
    "modern probability theory in his book published in 1933.\n",
    "The formal mathematical language is a system that allows rigorous explorations\n",
    "that have made fruitful advancements, and is now widely accepted as scientific\n",
    "standard in academic and industrial research.\n",
    "\n",
    "With the advent of big data, computer scientists have come up with a plethora of new\n",
    "algorithms that are aimed at revealing patterns from seemingly random data. *Machine learning*\n",
    "and *artificial intelligence* (AI) become buzz words. They\n",
    "defeat best human Go players, automate\n",
    "manufacturers, power self-driving vehicles, recognize human faces, and recommend\n",
    "online purchases. Behind their industrial success, statistics sheds\n",
    "light on the behavior of these algorithms.\n",
    "While statistical theory is built on modern probability theory,\n",
    "the latter is so far the most promising paradigm to rationalize\n",
    "existing algorithms and engineer new ones.\n",
    "\n",
    "Economics has been an empirical social science since Adam Smith (1723-1790). Many\n",
    "numerical anecdotes appear in his *Wealth of Nations* published in 1776.\n",
    "Ragnar Frisch\n",
    "(1895-1973) and Jan Tinbergen (1903-1994), two pioneers econometricians, were awarded\n",
    " in 1969 the first Nobel Prize in economics. Econometrics provides quantitative\n",
    "insights about economic data. It flourishes in real-world management practices, from households and\n",
    "firms up to governance at the global level. Today, the AI revolution is pumping fresh energy into\n",
    "research and exercise of econometric methods, while its very foundation is again modern probability theory.\n",
    "\n",
    "In this preparatory course, we will have a brief introduction of the axiomatic\n",
    "probability theory along with familiar results covered in\n",
    "undergraduate *probability and statistics*. This\n",
    "lecture note is at the level\n",
    "\n",
    "* Stachurski (2016): A Primer in Econometric Theory, or\n",
    "* Casella and Berger (2002): Statistical Inference (second edition)\n",
    "\n",
    "Interested readers may want to read this textbook for more examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Space\n",
    "\n",
    "\n",
    " A *sample space* $\\Omega$ is a collection of all possible outcomes. It is a set\n",
    " of things.\n",
    "\n",
    "An *event* $A$ is a subset of $\\Omega$. It is something of interest on the sample space.\n",
    "\n",
    "A $\\sigma$-*field*, denoted by $\\mathcal{F}$, is a collection of\n",
    "  $(A_i \\subseteq \\Omega)_{i \\in \\mathbb{N}}$  events such that\n",
    "\n",
    "1. $\\emptyset\\in\\mathcal{F}$;\n",
    "2. if an event $A\\in\\mathcal{F}$, then $A^{c}\\in\\mathcal{F}$;\n",
    "3. if $A_{i}\\in\\mathcal{F}$ for $i\\in\\mathbb{N}$, then\n",
    "    $\\bigcup_{i\\in\\mathbb{N}}A_{i}\\in\\mathcal{F}$.\n",
    "\n",
    "\n",
    "\n",
    "It is easy to show that $\\Omega \\in \\mathcal{F}$ and $\\bigcap_{i\\in\\mathbb{N}}A_{i}\\in\\mathcal{F}$.\n",
    "The $\\sigma$-field can be viewed as a well-organized structure built on the ground of the sample space. The pair $\\left(\\Omega,\\mathcal{F}\\right)$ is called a *measure space*.\n",
    "\n",
    "\n",
    "\n",
    "Let $\\mathcal{G} = \\{B_1, B_2,\\ldots\\}$ be an arbitrary collection of sets, not necessarily a $\\sigma$-field. We say $\\mathcal{F}$ is\n",
    "the smallest $\\sigma$-field generated by $\\mathcal{G}$ if $\\mathcal{G}\\subseteq \\mathcal{F}$, and  $\\mathcal{F}\\subseteq \\mathcal{\\tilde{F}}$ for any  $\\mathcal{\\tilde{F}}$ such that $\\mathcal{G}\\subseteq \\mathcal{\\tilde{F}}$.\n",
    "A *Borel $\\sigma$-field* $\\mathcal{R}$ is the\n",
    "smallest $\\sigma$-field generated by the open sets on the real line $\\mathbb{R}$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "A function $\\mu:(\\Omega, \\mathcal{F})\\mapsto\\left[0,\\infty\\right]$ is called a\n",
    "    *measure* if it satisfies\n",
    "\n",
    "1. (positiveness) $\\mu\\left(A\\right)\\geq 0$ for all $A\\in\\mathcal{F}$;\n",
    "2. (countable additivity) if $A_{i}\\in\\mathcal{F}$, $i\\in\\mathbb{N}$,\n",
    "are mutually disjoint, then\n",
    "$$\\mu\\left(\\bigcup_{i\\in\\mathbb{N}}A_{i}\\right)=\\sum_{i\\in\\mathbb{N}}\\mu\\left(A_{i}\\right).$$\n",
    "\n",
    "Measure can be understand as weight\n",
    "    or length.\n",
    "    In particular, we call $\\mu$ a *probability\n",
    "    measure* if $\\mu\\left(\\Omega\\right)=1$. A probability measure is often denoted as $P$.\n",
    "    The triple $\\left(\\Omega,\\mathcal{F},P\\right)$ is called a *probability space*.\n",
    "\n",
    "\n",
    "So far we have answered the question: \"What is a well-defined probability?\", but we have not yet\n",
    "answered \"How to assign the probability?\"\n",
    "\n",
    "There are two major schools of thinking on probability assignment. One is\n",
    "*frequentist*, who considers probability as the average chance of occurrence if a large number of experiments\n",
    "are carried out. The other is *Bayesian*, who deems probability as a subjective brief.\n",
    "The principles of these two schools are largely incompatible, while each school has\n",
    "peculiar merit under different context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Variable\n",
    "\n",
    "The terminology *random variable* somewhat belies its formal definition of a deterministic mapping. It is a link between\n",
    "two measurable spaces such that any event in the $\\sigma$-field installed on the range\n",
    "can be traced back to an event in the $\\sigma$-field installed\n",
    "on the domain.\n",
    "\n",
    "Formally, a function $X:\\Omega\\mapsto\\mathbb{R}$ is\n",
    "$\\left(\\Omega,\\mathcal{F}\\right)\\backslash\\left(\\mathbb{R},\\mathcal{R}\\right)$\n",
    "*measurable* if\n",
    "$$X^{-1}\\left(B\\right)=\\left\\{ \\omega\\in\\Omega:X\\left(\\omega\\right)\\in B\\right\\} \\in\\mathcal{F}$$\n",
    "for any $B\\in\\mathcal{R}.$  *Random variable* is an alternative,\n",
    "and somewhat romantic, name for a measurable function. We say a measurable is a\n",
    "*discrete random variable* if the set\n",
    "$\\left\\{X\\left(\\omega\\right):\\omega\\in\\Omega\\right\\}$ is finite\n",
    "or countable. We say it is a\n",
    "*continuous random variable* if the set\n",
    "$\\left\\{ X\\left(\\omega\\right):\\omega\\in\\Omega\\right\\}$\n",
    "is uncountable.\n",
    "\n",
    "A measurable function connects two measurable spaces.\n",
    "No probability is involved in its definition yet. While if a probability measure $P$\n",
    "is installed on  $(\\Omega, \\mathcal{F})$, the measurable function $X$\n",
    " will induce a probability measure on $(\\mathbb{R},\\mathcal{R})$.\n",
    "It is easy to verify that $P_{X}:(\\mathbb{R},\\mathcal{R})\\mapsto\\left[0,1\\right]$ is also a probability measure if defined as\n",
    "$$P_{X}\\left(B\\right)=P\\left(X^{-1}\\left(B\\right)\\right)$$\n",
    "for any $B\\in\\mathcal{R}$. This $P_{X}$ is called the probability measure\n",
    "    *induced* by the measurable function $X$.\n",
    "The induced probability measure $P_X$ is an offspring of the parent\n",
    "probability measure $P$ though the channel of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution Function\n",
    "\n",
    "We go back to some terms that we have learned in a undergraduate\n",
    "probability course. A *(cumulative) distribution function*\n",
    "$F:\\mathbb{R}\\mapsto [0,1]$ is defined as\n",
    "$$F\\left(x\\right)=P\\left(X\\leq x\\right)=\n",
    "P\\left(\\{X\\leq x\\}\\right)=P\\left(\\left\\{ \\omega\\in\\Omega:X\\left(\\omega\\right)\\leq x\\right\\} \\right).$$\n",
    "It is often abbreviated as CDF, and it has the following properties.\n",
    "\n",
    "(i) $\\lim_{x\\to-\\infty}F\\left(x\\right)=0$,\n",
    "(ii) $\\lim_{x\\to\\infty}F\\left(x\\right)=1$,\n",
    "(iii) non-decreasing,\n",
    "(iv)  right-continuous $\\lim_{y\\to x^{+}}F\\left(y\\right)=F\\left(x\\right).$\n",
    "\n",
    "For continuous distribution, if there exists a function $f$ such that for all $x$,\n",
    "$$F\\left(x\\right)=\\int_{-\\infty}^{x}f\\left(y\\right)\\mathrm{d}y,$$\n",
    " then $f$ is\n",
    "    called the *probability density function* of $X$, often abbreviated as PDF.\n",
    "It is easy to show that $f\\left(x\\right)\\geq0$ and\n",
    "    $\\int_{a}^{b}f\\left(x\\right)dx=F\\left(b\\right)-F\\left(a\\right)$.\n",
    "\n",
    "**Example** We have learned many parametric distributions like the binary distribution, the Poisson distribution,\n",
    "the uniform distribution, the normal distribution, $\\chi^{2}$, $t$, $F$ and so on.\n",
    "They are parametric distributions, meaning that the CDF or PDF can be completely\n",
    "characterized by a few parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected Value\n",
    "\n",
    "\n",
    "## Integration\n",
    "\n",
    "Integration is one of the most fundamental operations in mathematical analysis.\n",
    "We have studied Riemann's integral in the undergraduate calculus.\n",
    "Riemann's integral is intuitive, but Lebesgue integral is a more general approach to defining integration.\n",
    "\n",
    "Lebesgue integral is\n",
    "constructed by the following steps. $X$ is called a *simple function* on a measurable space\n",
    "    $\\left(\\Omega,\\mathcal{F}\\right)$ if\n",
    "    $X=\\sum_{i}a_{i}\\cdot 1\\left\\{ A_{i}\\right\\}$ and this summation is finite, where\n",
    "    $a_{i}\\in\\mathbb{R}$ and $\\{ A_i\\in\\mathcal{F} \\}_{i\\in \\mathbb{N}}$ is a partition\n",
    "of $\\Omega$. A simple function is measurable.\n",
    "\n",
    "\n",
    "1. Let $\\left(\\Omega,\\mathcal{F},\\mu\\right)$ be a measure space. The integral of the simple function $X$ with respect to $\\mu$\n",
    "is\n",
    "$$\\int X\\mathrm{d}\\mu=\\sum_{i}a_{i}\\mu\\left(A_{i}\\right).$$\n",
    "Unlike the Rieman integral, this definition of integration does not\n",
    "partition the domain into splines of equal length. Instead, it tracks\n",
    "the distinctive values of the function and the corresponding measure.\n",
    "\n",
    "2. Let $X$ be a non-negative measurable function. The integral of $X$\n",
    "    with respect to $\\mu$ is\n",
    "    $$\\int X\\mathrm{d}\\mu=\\sup\\left\\{ \\int Y\\mathrm{d}\\mu:0\\leq Y\\leq X,\\text{ }Y\\text{ is simple}\\right\\} .$$\n",
    "3. Let $X$ be a measurable function. Define\n",
    "    $X^{+}=\\max\\left\\{ X,0\\right\\}$ and\n",
    "    $X^{-}=-\\min\\left\\{ X,0\\right\\}$. Both $X^{+}$ and $X^{-}$ are\n",
    "    non-negative functions. The integral of $X$ with respect to $\\mu$ is\n",
    "    $$\\int X\\mathrm{d}\\mu=\\int X^{+}\\mathrm{d}\\mu-\\int X^{-}\\mathrm{d}\\mu.$$\n",
    "\n",
    "The Step 1 above defines the integral of a simple function.\n",
    "Step 2 defines the integral of a non-negative function as the approximation of\n",
    "steps functions from below.\n",
    "Step 3 defines the integral of a general function as the difference of the\n",
    "integral of two non-negative parts.\n",
    "\n",
    "If the measure $\\mu$ is a probability measure $P$, then the integral\n",
    "    $\\int X\\mathrm{d}P$ is called the *expected value,* or\n",
    "    *expectation,* of $X$. We often use the notation\n",
    "    $E\\left[X\\right]$, instead of $\\int X\\mathrm{d}P$, for convenience.\n",
    "\n",
    "Expectation provides the average of a random variable,\n",
    "despite that we cannot foresee the realization of a random variable in a particular trial\n",
    "(otherwise the study of uncertainty is trivial). In the frequentist's view,\n",
    "the expectation is the average outcome if we carry out a large number of independent\n",
    "trials.\n",
    "\n",
    "If we know the probability mass function of a discrete random variable, its expectation\n",
    "is calculated as $E\\left[X\\right]=\\sum_{x}xP\\left(X=x\\right)$, which is\n",
    "the integral of a simple function.\n",
    "If a continuous random variable has a PDF $f(x)$, its expectation\n",
    "can be computed as  $E\\left[X\\right]=\\int xf\\left(x\\right)\\mathrm{d}x$.\n",
    "These two expressions are unified as\n",
    "$E[X] = \\int X \\mathrm{d} P$ by  the Lebesgue integral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some properties of the expectation.\n",
    "\n",
    "\n",
    "-  The probability of an event $A$ is the expectation\n",
    "of an indicator function. $E\\left[1\\left\\{ A\\right\\}  \\right]= 1\\times P(A) + 0 \\times P(A^c) =P\\left(A\\right)$.\n",
    "\n",
    "-   $E\\left[X^{r}\\right]$ is call the $r$-moment of $X$. The *mean* of a random variable is the first moment $\\mu=E\\left[X\\right]$, and\n",
    "the second *centered* moment is called the *variance*\n",
    "$\\mathrm{var}\\left[X\\right]=E\\left[\\left(X-\\mu\\right)^{2}\\right]$.\n",
    "The third centered moment $E\\left[\\left(X-\\mu\\right)^{3}\\right]$,\n",
    "called *skewness*, is a measurement of the\n",
    "symmetry of a random variable, and the fourth centered moment\n",
    "    $E\\left[\\left(X-\\mu\\right)^{4}\\right]$, called *kurtosis*, is\n",
    "     a measurement of the tail thickness.\n",
    "\n",
    "- Moments do not always exist. For example, the mean of the Cauchy distribution does not exist,\n",
    "and the variance of the $t(2)$ distribution does not exist.\n",
    "\n",
    "- $E[\\cdot]$ is a linear operation. If $\\phi(\\cdot)$ is a linear function, then $E[\\phi(X)] = \\phi(E[X]).$\n",
    "\n",
    "-   *Jensen's inequality* is an important fact.\n",
    "A function $\\varphi(\\cdot)$ is convex if\n",
    "$\\varphi( a x_1 + (1-a) x_2 ) \\leq a \\varphi(x_1) + (1-a) \\varphi(x_2)$ for all $x_1,x_2$\n",
    "in the domain and $a\\in[0,1]$. For instance, $x^2$ is a convex function.\n",
    "Jensen's inequality says that if $\\varphi\\left(\\cdot\\right)$ is a convex\n",
    "    function, then\n",
    "    $\\varphi\\left(E\\left[X\\right]\\right)\\leq E\\left[\\varphi\\left(X\\right)\\right].$ \n",
    "\n",
    "-   *Markov inequality* is another simple but important fact. If $E\\left[\\left|X\\right|^{r}\\right]$ exists,\n",
    "    then\n",
    "    $P\\left(\\left|X\\right|>\\epsilon\\right)\\leq E\\left[\\left|X\\right|^{r}\\right]/\\epsilon^{r}$\n",
    "    for all $r\\geq1$. *Chebyshev inequality* $P\\left(\\left|X\\right|>\\epsilon\\right)\\leq E\\left[X^{2}\\right]/\\epsilon^{2}$\n",
    "    is a special case of the Markov inequality when $r=2$.\n",
    "\n",
    "- The distribution of a random variable is completely characterized by its\n",
    "CDF or PDF. Moment is a function of the distribution. To back out the underlying distribution\n",
    "from moments, we need to know the moment-generating function (mgf) $M_X(t) = E [ e^{tX}]$ for $t\\in \\mathbb{R}$\n",
    "whenever the expectation exists. The $r$th moment can be computed from mgf as\n",
    "$$\n",
    "E[X^r ] = \\frac{ \\mathrm{d}^r M_X(t) }{ \\mathrm{d} t^r } \\big \\vert_{t=0}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Random Variable\n",
    "\n",
    "A bivariate random variable is a\n",
    "measurable function $X:\\Omega\\mapsto\\mathbb{R}^{2}$, and more generally a multivariate random\n",
    "variable is a measurable function $X:\\Omega\\mapsto\\mathbb{R}^{n}$.\n",
    "We can define the *joint CDF* as\n",
    "$F\\left(x_{1},\\ldots,x_{n}\\right)=P\\left(X_{1}\\leq x_{1},\\ldots,X_{n}\\leq x_{n}\\right)$.\n",
    "Joint PDF is defined similarly.\n",
    "\n",
    "It is sufficient to introduce the joint distribution, conditional distribution\n",
    "and marginal distribution in the simple bivariate case, and these definitions\n",
    "can be extended to multivariate distributions. Suppose\n",
    "a bivariate random variable $(X,Y)$ has a joint density\n",
    "$f(\\cdot,\\cdot)$.\n",
    "The  *conditional density* can be roughly written as  $f\\left(y|x\\right)=f\\left(x,y\\right)/f\\left(x\\right)$ if we do not formally deal\n",
    "with the case $f(x)=0$.\n",
    "The *marginal density* $f\\left(y\\right)=\\int f\\left(x,y\\right)dx$ integrates out\n",
    "the coordinate that is not interested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independence\n",
    "\n",
    "In a probability space $(\\Omega, \\mathcal{F}, P)$, for two events $A_1,A_2\\in \\mathcal{F}$ the *conditional probability* is\n",
    "$$P\\left(A_1|A_2\\right)=\\frac{P\\left(A_1 A_2\\right)}{P\\left(A_2\\right)}$$\n",
    "if $P(A_2) \\neq 0$. If $P(A_2) =0$, the conditional probability can still be valid\n",
    "in some cases,\n",
    "but we need to introduce the *dominance* between two measures,\n",
    "which I choose not to do at this time.\n",
    "In the definition of conditional probability, $A_2$ plays\n",
    "the role of the outcome space  so that\n",
    " $P(A_1 A_2)$ is standardized by the total mass $P(A_2)$.\n",
    "\n",
    "Since $A_1$ and $A_2$ are symmetric, we also have $P(A_1 A_2) = P(A_2|A_1)P(A_1)$.\n",
    "It implies\n",
    "$$P(A_1 | A_2)=\\frac{P\\left(A_2| A_1\\right)P\\left(A_1\\right)}{P\\left(A_2\\right)}$$\n",
    "This formula is the well-known *Bayes' Theorem*.\n",
    "\n",
    "**Example:** $A_1$ is the event \"a student can survive CUHK's postgraduate program\", and $A_2$ is\n",
    "his or her application profile.\n",
    "\n",
    "We say two events $A_1$ and $A_2$ are *independent* if $P(A_1A_2) = P(A_1)P(A_2)$.\n",
    "If $P(A_2) \\neq 0$, it is equivalent to $P(A_1 | A_2 ) = P(A_1)$.\n",
    "In words, knowing $A_2$ does not change the probability of $A_1$.\n",
    "\n",
    "Regarding the independence of two random variables,\n",
    "$X$ and $Y$ are *independent* if\n",
    "$P\\left(X\\in B_1,Y\\in B_2\\right)=P\\left(X\\in B_1\\right)P\\left(Y\\in B_2\\right)$\n",
    "for any two Borel sets $B_1$ and $B_2$.\n",
    "\n",
    "If $X$ and $Y$ are independent, $E[XY] = E[X]E[Y]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Law of Iterated Expectations\n",
    "\n",
    "\n",
    "Given a probability space $\\left(\\Omega,\\mathcal{F},P\\right)$, a sub\n",
    "    $\\sigma$-algebra $\\mathcal{G}\\subseteq \\mathcal{F}$ and a\n",
    "    $\\mathcal{F}$-measurable function $X$ with $E\\left|X\\right|<\\infty$,\n",
    "    the *conditional expectation* $E\\left[X|\\mathcal{G}\\right]$ is\n",
    "    defined as a $\\mathcal{G}$-measurable function such that\n",
    "    $\\int_{A}X \\mathrm{d} P=\\int_{A}E\\left[X|\\mathcal{G}\\right] \\mathrm{d} P$ for all\n",
    "    $A\\in\\mathcal{G}$. *Law of iterated expectation* is a trivial fact if we take\n",
    "    $A=\\Omega$.\n",
    "\n",
    "\n",
    "\n",
    "In the bivariate case, if the conditional density exists, the conditional expectation can be computed as\n",
    "    $E\\left[Y|X\\right]=\\int y f\\left(y|X\\right) \\mathrm{d} y$.\n",
    "The law of iterated expectation implies $E\\left[E\\left[Y|X\\right]\\right]=E\\left[Y\\right]$.\n",
    "\n",
    "\n",
    "Below are some properties of conditional expectations\n",
    "\n",
    "1.  $E\\left[E\\left[Y|X_{1},X_{2}\\right]|X_{1}\\right]=E\\left[Y|X_{1}\\right];$\n",
    "2.  $E\\left[E\\left[Y|X_{1}\\right]|X_{1},X_{2}\\right]=E\\left[Y|X_{1}\\right];$\n",
    "3.  $E\\left[h\\left(X\\right)Y|X\\right]=h\\left(X\\right)E\\left[Y|X\\right].$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.0"
  },
  "latex_metadata": {
   "affiliation": "CUHK",
   "author": "Zhentao Shi",
   "title": "Lecture 1: Probability"
  },
  "name": "_merged"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
