
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Lecture 1}
    \author{Zhentao Shi}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Probability}\label{probability}

Human beings are awed by uncertainty in daily life. In the old days,
Egyptians consulted oracles, Hebrews inquired prophets, and Chinese
counted on diviners to interpret tortoise shell or bone cracks. Even in
today's Hong Kong fortunetellers are abundant.

Probability theory is a philosophy about uncertainty. Over centuries,
mathematicians strove to contribute to the understanding of randomness.
As measure theory matured in the early 20th century, Russian
mathematician Andrey Kolmogorov (1903-1987) laid the foundation of
modern probability theory in his book published in 1933. The formal
mathematical language is a system that allows rigorous explorations that
have made fruitful advancements, and is now widely accepted as
scientific standard in academic and industrial research.

With the advent of big data, computer scientists have come up with a
plethora of new algorithms that are aimed at revealing patterns from
seemingly random data. \emph{Machine learning} and \emph{artificial
intelligence} (AI) become buzz words. They defeat best human Go players,
automate manufacturers, power self-driving vehicles, recognize human
faces, and recommend online purchases. Behind their industrial success,
statistics sheds light on the behavior of these algorithms. While
statistical theory is built on modern probability theory, the latter is
so far the most promising paradigm to rationalize existing algorithms
and engineer new ones.

Economics has been an empirical social science since Adam Smith
(1723-1790). Many numerical anecdotes appear in his \emph{Wealth of
Nations} published in 1776. Ragnar Frisch (1895-1973) and Jan Tinbergen
(1903-1994), two pioneers econometricians, were awarded in 1969 the
first Nobel Prize in economics. Econometrics provides quantitative
insights about economic data. It flourishes in real-world management
practices, from households and firms up to governance at the global
level. Today, the AI revolution is pumping fresh energy into research
and exercise of econometric methods, while its very foundation is again
modern probability theory.

In this preparatory course, we will have a brief introduction of the
axiomatic probability theory along with familiar results covered in
undergraduate \emph{probability and statistics}. This
lecture note is at the level

\begin{itemize}
\tightlist
\item
  Stachurski (2016): A Primer in Econometric Theory, or
\item
  Casella and Berger (2002): Statistical Inference (second edition)
\end{itemize}

Interested readers may want to read this textbook for more examples.

    \subsection{Probability Space}\label{probability-space}

A \emph{sample space} \(\Omega\) is a collection of all possible
outcomes. It is a set of things.

An \emph{event} \(A\) is a subset of \(\Omega\). It is something of
interest on the sample space.

A \(\sigma\)-\emph{field}, denoted by \(\mathcal{F}\), is a collection
of \((A_i \subseteq \Omega)_{i \in \mathbb{N}}\) events such that

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(\emptyset\in\mathcal{F}\);
\item
  if an event \(A\in\mathcal{F}\), then \(A^{c}\in\mathcal{F}\);
\item
  if \(A_{i}\in\mathcal{F}\) for \(i\in\mathbb{N}\), then
  \(\bigcup_{i\in\mathbb{N}}A_{i}\in\mathcal{F}\).
\end{enumerate}

It is easy to show that \(\Omega \in \mathcal{F}\) and
\(\bigcap_{i\in\mathbb{N}}A_{i}\in\mathcal{F}\). The \(\sigma\)-field
can be viewed as a well-organized structure built on the ground of the
sample space. The pair \(\left(\Omega,\mathcal{F}\right)\) is called a
\emph{measure space}.

Let \(\mathcal{G} = \{B_1, B_2,\ldots\}\) be an arbitrary collection of
sets, not necessarily a \(\sigma\)-field. We say \(\mathcal{F}\) is the
smallest \(\sigma\)-field generated by \(\mathcal{G}\) if
\(\mathcal{G}\subseteq \mathcal{F}\), and
\(\mathcal{F}\subseteq \mathcal{\tilde{F}}\) for any
\(\mathcal{\tilde{F}}\) such that
\(\mathcal{G}\subseteq \mathcal{\tilde{F}}\). A \emph{Borel
\(\sigma\)-field} \(\mathcal{R}\) is the smallest \(\sigma\)-field
generated by the open sets on the real line \(\mathbb{R}\).

A function \(\mu:(\Omega, \mathcal{F})\mapsto\left[0,\infty\right]\) is
called a \emph{measure} if it satisfies

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  (positiveness) \(\mu\left(A\right)\geq 0\) for all
  \(A\in\mathcal{F}\);
\item
  (countable additivity) if \(A_{i}\in\mathcal{F}\), \(i\in\mathbb{N}\),
  are mutually disjoint, then
  \[\mu\left(\bigcup_{i\in\mathbb{N}}A_{i}\right)=\sum_{i\in\mathbb{N}}\mu\left(A_{i}\right).\]
\end{enumerate}

Measure can be understand as weight or length. In particular, we call
\(\mu\) a \emph{probability measure} if \(\mu\left(\Omega\right)=1\). A
probability measure is often denoted as \(P\). The triple
\(\left(\Omega,\mathcal{F},P\right)\) is called a \emph{probability
space}.

So far we have answered the question: "What is a well-defined
probability?", but we have not yet answered "How to assign the
probability?"

There are two major schools of thinking on probability assignment. One
is \emph{frequentist}, who considers probability as the average chance
of occurrence if a large number of experiments are carried out. The
other is \emph{Bayesian}, who deems probability as a subjective brief.
The principles of these two schools are largely incompatible, while each
school has peculiar merit under different context.

    \subsection{Random Variable}\label{random-variable}

The terminology \emph{random variable} somewhat belies its formal
definition of a deterministic mapping. It is a link between two
measurable spaces such that any event in the \(\sigma\)-field installed
on the range can be traced back to an event in the \(\sigma\)-field
installed on the domain.

Formally, a function \(X:\Omega\mapsto\mathbb{R}\) is
\(\left(\Omega,\mathcal{F}\right)\backslash\left(\mathbb{R},\mathcal{R}\right)\)
\emph{measurable} if
\[X^{-1}\left(B\right)=\left\{ \omega\in\Omega:X\left(\omega\right)\in B\right\} \in\mathcal{F}\]
for any \(B\in\mathcal{R}.\) \emph{Random variable} is an alternative,
and somewhat romantic, name for a measurable function. We say a
measurable is a \emph{discrete random variable} if the set
\(\left\{X\left(\omega\right):\omega\in\Omega\right\}\) is finite or
countable. We say it is a \emph{continuous random variable} if the set
\(\left\{ X\left(\omega\right):\omega\in\Omega\right\}\) is uncountable.

A measurable function connects two measurable spaces. No probability is
involved in its definition yet. While if a probability measure \(P\) is
installed on \((\Omega, \mathcal{F})\), the measurable function \(X\)
will induce a probability measure on \((\mathbb{R},\mathcal{R})\). It is
easy to verify that
\(P_{X}:(\mathbb{R},\mathcal{R})\mapsto\left[0,1\right]\) is also a
probability measure if defined as
\[P_{X}\left(B\right)=P\left(X^{-1}\left(B\right)\right)\] for any
\(B\in\mathcal{R}\). This \(P_{X}\) is called the probability measure
\emph{induced} by the measurable function \(X\). The induced probability
measure \(P_X\) is an offspring of the parent probability measure \(P\)
though the channel of \(X\).

    \subsection{Distribution Function}\label{distribution-function}

We go back to some terms that we have learned in a undergraduate
probability course. A \emph{(cumulative) distribution function}
\(F:\mathbb{R}\mapsto [0,1]\) is defined as
\[F\left(x\right)=P\left(X\leq x\right)=
P\left(\{X\leq x\}\right)=P\left(\left\{ \omega\in\Omega:X\left(\omega\right)\leq x\right\} \right).\]
It is often abbreviated as CDF, and it has the following properties.

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\tightlist
\item
  \(\lim_{x\to-\infty}F\left(x\right)=0\),
\item
  \(\lim_{x\to\infty}F\left(x\right)=1\),
\item
  non-decreasing,
\item
  right-continuous \(\lim_{y\to x^{+}}F\left(y\right)=F\left(x\right).\)
\end{enumerate}

For continuous distribution, if there exists a function \(f\) such that
for all \(x\),
\[F\left(x\right)=\int_{-\infty}^{x}f\left(y\right)\mathrm{d}y,\] then
\(f\) is called the \emph{probability density function} of \(X\), often
abbreviated as PDF. It is easy to show that \(f\left(x\right)\geq0\) and
\(\int_{a}^{b}f\left(x\right)dx=F\left(b\right)-F\left(a\right)\).

\textbf{Example} We have learned many parametric distributions like the
binary distribution, the Poisson distribution, the uniform distribution,
the normal distribution, \(\chi^{2}\), \(t\), \(F\) and so on. They are
parametric distributions, meaning that the CDF or PDF can be completely
characterized by a few parameters.

    \section{Expected Value}\label{expected-value}

\subsection{Integration}\label{integration}

Integration is one of the most fundamental operations in mathematical
analysis. We have studied Riemann's integral in the undergraduate
calculus. Riemann's integral is intuitive, but Lebesgue integral is a
more general approach to defining integration.

Lebesgue integral is constructed by the following steps. \(X\) is called
a \emph{simple function} on a measurable space
\(\left(\Omega,\mathcal{F}\right)\) if
\(X=\sum_{i}a_{i}\cdot 1\left\{ A_{i}\right\}\) and this summation is
finite, where \(a_{i}\in\mathbb{R}\) and
\(\{ A_i\in\mathcal{F} \}_{i\in \mathbb{N}}\) is a partition of
\(\Omega\). A simple function is measurable.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Let \(\left(\Omega,\mathcal{F},\mu\right)\) be a measure space. The
  integral of the simple function \(X\) with respect to \(\mu\) is
  \[\int X\mathrm{d}\mu=\sum_{i}a_{i}\mu\left(A_{i}\right).\] Unlike the
  Rieman integral, this definition of integration does not partition the
  domain into splines of equal length. Instead, it tracks the
  distinctive values of the function and the corresponding measure.
\item
  Let \(X\) be a non-negative measurable function. The integral of \(X\)
  with respect to \(\mu\) is
  \[\int X\mathrm{d}\mu=\sup\left\{ \int Y\mathrm{d}\mu:0\leq Y\leq X,\text{ }Y\text{ is simple}\right\} .\]
\item
  Let \(X\) be a measurable function. Define
  \(X^{+}=\max\left\{ X,0\right\}\) and
  \(X^{-}=-\min\left\{ X,0\right\}\). Both \(X^{+}\) and \(X^{-}\) are
  non-negative functions. The integral of \(X\) with respect to \(\mu\)
  is
  \[\int X\mathrm{d}\mu=\int X^{+}\mathrm{d}\mu-\int X^{-}\mathrm{d}\mu.\]
\end{enumerate}

The Step 1 above defines the integral of a simple function. Step 2
defines the integral of a non-negative function as the approximation of
steps functions from below. Step 3 defines the integral of a general
function as the difference of the integral of two non-negative parts.

If the measure \(\mu\) is a probability measure \(P\), then the integral
\(\int X\mathrm{d}P\) is called the \emph{expected value,} or
\emph{expectation,} of \(X\). We often use the notation
\(E\left[X\right]\), instead of \(\int X\mathrm{d}P\), for convenience.

Expectation provides the average of a random variable, despite that we
cannot foresee the realization of a random variable in a particular
trial (otherwise the study of uncertainty is trivial). In the
frequentist's view, the expectation is the average outcome if we carry
out a large number of independent trials.

If we know the probability mass function of a discrete random variable,
its expectation is calculated as
\(E\left[X\right]=\sum_{x}xP\left(X=x\right)\), which is the integral of
a simple function. If a continuous random variable has a PDF \(f(x)\),
its expectation can be computed as
\(E\left[X\right]=\int xf\left(x\right)\mathrm{d}x\). These two
expressions are unified as \(E[X] = \int X \mathrm{d} P\) by the
Lebesgue integral.

    Here are some properties of the expectation.

\begin{itemize}
\item
  The probability of an event \(A\) is the expectation of an indicator
  function.
  \(E\left[1\left\{ A\right\} \right]= 1\times P(A) + 0 \times P(A^c) =P\left(A\right)\).
\item
  \(E\left[X^{r}\right]\) is call the \(r\)-moment of \(X\). The
  \emph{mean} of a random variable is the first moment
  \(\mu=E\left[X\right]\), and the second \emph{centered} moment is
  called the \emph{variance}
  \(\mathrm{var}\left[X\right]=E\left[\left(X-\mu\right)^{2}\right]\).
  The third centered moment \(E\left[\left(X-\mu\right)^{3}\right]\),
  called \emph{skewness}, is a measurement of the symmetry of a random
  variable, and the fourth centered moment
  \(E\left[\left(X-\mu\right)^{4}\right]\), called \emph{kurtosis}, is a
  measurement of the tail thickness.
\item
  Moments do not always exist. For example, the mean of the Cauchy
  distribution does not exist, and the variance of the \(t(2)\)
  distribution does not exist.
\item
  \(E[\cdot]\) is a linear operation. If \(\phi(\cdot)\) is a linear
  function, then \(E[\phi(X)] = \phi(E[X]).\)
\item
  \emph{Jensen's inequality} is an important fact. A function
  \(\varphi(\cdot)\) is convex if
  \(\varphi( a x_1 + (1-a) x_2 ) \leq a \varphi(x_1) + (1-a) \varphi(x_2)\)
  for all \(x_1,x_2\) in the domain and \(a\in[0,1]\). For instance,
  \(x^2\) is a convex function. Jensen's inequality says that if
  \(\varphi\left(\cdot\right)\) is a convex function, then
  \(\varphi\left(E\left[X\right]\right)\leq E\left[\varphi\left(X\right)\right].\)
\item
  \emph{Markov inequality} is another simple but important fact. If
  \(E\left[\left|X\right|^{r}\right]\) exists, then
  \(P\left(\left|X\right|>\epsilon\right)\leq E\left[\left|X\right|^{r}\right]/\epsilon^{r}\)
  for all \(r\geq1\). \emph{Chebyshev inequality}
  \(P\left(\left|X\right|>\epsilon\right)\leq E\left[X^{2}\right]/\epsilon^{2}\)
  is a special case of the Markov inequality when \(r=2\).
\item
  The distribution of a random variable is completely characterized by
  its CDF or PDF. Moment is a function of the distribution. To back out
  the underlying distribution from moments, we need to know the
  moment-generating function (mgf) \(M_X(t) = E [ e^{tX}]\) for
  \(t\in \mathbb{R}\) whenever the expectation exists. The \(r\)th
  moment can be computed from mgf as \[
  E[X^r ] = \frac{ \mathrm{d}^r M_X(t) }{ \mathrm{d} t^r } \big \vert_{t=0}.
  \]
\end{itemize}

    \section{Multivariate Random
Variable}\label{multivariate-random-variable}

A bivariate random variable is a measurable function
\(X:\Omega\mapsto\mathbb{R}^{2}\), and more generally a multivariate
random variable is a measurable function
\(X:\Omega\mapsto\mathbb{R}^{n}\). We can define the \emph{joint CDF} as
\(F\left(x_{1},\ldots,x_{n}\right)=P\left(X_{1}\leq x_{1},\ldots,X_{n}\leq x_{n}\right)\).
Joint PDF is defined similarly.

It is sufficient to introduce the joint distribution, conditional
distribution and marginal distribution in the simple bivariate case, and
these definitions can be extended to multivariate distributions. Suppose
a bivariate random variable \((X,Y)\) has a joint density
\(f(\cdot,\cdot)\). The \emph{conditional density} can be roughly
written as \(f\left(y|x\right)=f\left(x,y\right)/f\left(x\right)\) if we
do not formally deal with the case \(f(x)=0\). The \emph{marginal
density} \(f\left(y\right)=\int f\left(x,y\right)dx\) integrates out the
coordinate that is not interested.

    \subsection{Independence}\label{independence}

In a probability space \((\Omega, \mathcal{F}, P)\), for two events
\(A_1,A_2\in \mathcal{F}\) the \emph{conditional probability} is
\[P\left(A_1|A_2\right)=\frac{P\left(A_1 A_2\right)}{P\left(A_2\right)}\]
if \(P(A_2) \neq 0\). If \(P(A_2) =0\), the conditional probability can
still be valid in some cases, but we need to introduce the
\emph{dominance} between two measures, which I choose not to do at this
time. In the definition of conditional probability, \(A_2\) plays the
role of the outcome space so that \(P(A_1 A_2)\) is standardized by the
total mass \(P(A_2)\).

Since \(A_1\) and \(A_2\) are symmetric, we also have
\(P(A_1 A_2) = P(A_2|A_1)P(A_1)\). It implies
\[P(A_1 | A_2)=\frac{P\left(A_2| A_1\right)P\left(A_1\right)}{P\left(A_2\right)}\]
This formula is the well-known \emph{Bayes' Theorem}.

\textbf{Example:} \(A_1\) is the event "a student can survive CUHK's
postgraduate program", and \(A_2\) is his or her application profile.

We say two events \(A_1\) and \(A_2\) are \emph{independent} if
\(P(A_1A_2) = P(A_1)P(A_2)\). If \(P(A_2) \neq 0\), it is equivalent to
\(P(A_1 | A_2 ) = P(A_1)\). In words, knowing \(A_2\) does not change
the probability of \(A_1\).

Regarding the independence of two random variables, \(X\) and \(Y\) are
\emph{independent} if
\(P\left(X\in B_1,Y\in B_2\right)=P\left(X\in B_1\right)P\left(Y\in B_2\right)\)
for any two Borel sets \(B_1\) and \(B_2\).

If \(X\) and \(Y\) are independent, \(E[XY] = E[X]E[Y]\).

    \subsection{Law of Iterated
Expectations}\label{law-of-iterated-expectations}

Given a probability space \(\left(\Omega,\mathcal{F},P\right)\), a sub
\(\sigma\)-algebra \(\mathcal{G}\subseteq \mathcal{F}\) and a
\(\mathcal{F}\)-measurable function \(X\) with
\(E\left|X\right|<\infty\), the \emph{conditional expectation}
\(E\left[X|\mathcal{G}\right]\) is defined as a
\(\mathcal{G}\)-measurable function such that
\(\int_{A}X \mathrm{d} P=\int_{A}E\left[X|\mathcal{G}\right] \mathrm{d} P\)
for all \(A\in\mathcal{G}\). \emph{Law of iterated expectation} is a
trivial fact if we take \(A=\Omega\).

In the bivariate case, if the conditional density exists, the
conditional expectation can be computed as
\(E\left[Y|X\right]=\int y f\left(y|X\right) \mathrm{d} y\). The law of
iterated expectation implies
\(E\left[E\left[Y|X\right]\right]=E\left[Y\right]\).

Below are some properties of conditional expectations

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(E\left[E\left[Y|X_{1},X_{2}\right]|X_{1}\right]=E\left[Y|X_{1}\right];\)
\item
  \(E\left[E\left[Y|X_{1}\right]|X_{1},X_{2}\right]=E\left[Y|X_{1}\right];\)
\item
  \(E\left[h\left(X\right)Y|X\right]=h\left(X\right)E\left[Y|X\right].\)
\end{enumerate}


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
