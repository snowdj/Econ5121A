{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notation**: in this note, $y$ is a scale random variable, and $x$ is a\n",
    "$K\\times1$ random vector.\n",
    "\n",
    "Conditional Expectation\n",
    "===========================================================\n",
    "\n",
    "A regression model can be written as $$y=m\\left(x\\right)+\\epsilon,$$\n",
    "where $m(x)=E[y|x]$ is called the *conditional mean function*, and\n",
    "$\\epsilon=y-m\\left(x\\right)$ is called the *regression error*. Such an\n",
    "equation holds for $\\left(y,x\\right)$ that follows any joint\n",
    "distribution, as long as $E\\left[y|x\\right]$ exists. The error term\n",
    "$\\epsilon$ satisfies these properties:\n",
    "\n",
    "-   $E\\left[\\epsilon|x\\right]=0$,\n",
    "\n",
    "-   $E\\left[\\epsilon\\right]=0$,\n",
    "\n",
    "-   $E\\left[h\\left(x\\right)\\epsilon\\right]=0$, where $h$ is a function\n",
    "    of $x$.\n",
    "\n",
    "The last property implies that $\\epsilon$ is uncorrelated with any\n",
    "function of $x$.\n",
    "\n",
    "If we are interested in predicting $y$ given $x$, then the conditional\n",
    "mean function $E\\left[y|x\\right]$ is “optimal” in terms of the *mean\n",
    "squared error* (MSE).\n",
    "\n",
    "As $y$ is not a deterministic function of $x$, we cannot predict it with\n",
    "certainty. In order to evaluate different methods of prediction, we must\n",
    "have a criterion for comparison. For an arbitrary\n",
    "prediction method $g\\left(x\\right)$, we employ a *loss function*\n",
    "$L\\left(y,g\\left(x\\right)\\right)$ to measure how wrong is the\n",
    "prediction, and the expected value of the loss function is called the\n",
    "*risk*, and is denoted as $R\\left(y,g\\left(x\\right)\\right)$. \n",
    "\n",
    "There are many choices of loss functions. A particularly convenient one is\n",
    "the *quadratic loss function*,\n",
    "defined as\n",
    "$$L\\left(y,g\\left(x\\right)\\right)=\\left(y-g\\left(x\\right)\\right)^{2}.$$\n",
    "The risk corresponding to the quadratic loss is\n",
    "$$R\\left(y,g\\left(x\\right)\\right)=E\\left[\\left(y-g\\left(x\\right)\\right)^{2}\\right],$$\n",
    "and it is called the MSE.\n",
    "\n",
    "Due to its operational ease, MSE is one of the most widely used\n",
    "criterion. Under MSE, the conditional expectation function happens to be\n",
    "the best prediction method for $y$ given $x$. In other words, the\n",
    "conditional mean function $m\\left(x\\right)$ minimizes the MSE.\n",
    "\n",
    "The claimed optimality can be confirmed by \"guess-and-verify.\" \n",
    "For an\n",
    "arbitrary $g\\left(x\\right)$, the risk is decomposed into three terms\n",
    "$$E\\left[\\left(y-g\\left(x\\right)\\right)^{2}\\right] = \n",
    " E\\left[\\left(y-m\\left(x\\right)\\right)^{2}\\right]+2E\\left[\\left(y-m\\left(x\\right)\\right)\\left(m\\left(x\\right)-g\\left(x\\right)\\right)\\right]+E\\left[\\left(m\\left(x\\right)-g\\left(x\\right)\\right)^{2}\\right].$$\n",
    "The first term is irrelevant to $g\\left(x\\right)$. The second term\n",
    "$2E\\left[\\epsilon\\left(m\\left(x\\right)-g\\left(x\\right)\\right)\\right]=0$\n",
    "is again irrelevant of $g\\left(x\\right)$. The third term, obviously, is\n",
    "minimized at $g\\left(x\\right)=m\\left(x\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Projection\n",
    "============================================\n",
    "\n",
    "As discussed in the previous section, we are interested in the\n",
    "conditional mean function $m(x)$. However, remind that\n",
    "$$m\\left(x\\right)=E\\left[y|x\\right]=\\int y f\\left(y|x\\right)\\mathrm{d}y$$\n",
    "is a complex function of $x$, as it depends on the joint distribution of\n",
    "$\\left(y,x\\right)$.\n",
    "\n",
    "A particular form of the conditional mean function is a linear function\n",
    "$$m\\left(x\\right)=x'\\beta.$$\n",
    "\n",
    "The linear function is not as restrictive as one might thought. It can\n",
    "be used to generate some nonlinear (in random variables) effect if we\n",
    "re-define $x$. For example, if\n",
    "$$y=x_{1}\\beta_{2}+x_{2}\\beta_{2}+x_{1}x_{2}\\beta_{3}+e,$$ then\n",
    "$\\frac{\\partial}{\\partial x_{1}}m\\left(x_{1},x_{2}\\right)=\\beta_{1}+x_{2}\\beta_{3}$,\n",
    "which is nonlinear in $x_{1}$, while it is still linear in the parameter\n",
    "$\\beta$ if we define a set of new regressors as\n",
    "$\\left(\\tilde{x}_{1},\\tilde{x}_{2},\\tilde{x}_{3}\\right)=\\left(x_{1},x_{2},x_{1}x_{2}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "If $\\begin{pmatrix}y\\\\\n",
    "x\n",
    "\\end{pmatrix}\\sim\\mathrm{N}\\left(\\begin{pmatrix}\\mu_{y}\\\\\n",
    "\\mu_{x}\n",
    "\\end{pmatrix},\\begin{pmatrix}\\sigma_{y}^{2} & \\rho\\sigma_{y}\\sigma_{x}\\\\\n",
    "\\rho\\sigma_{y}\\sigma_{x} & \\sigma_{x}^{2}\n",
    "\\end{pmatrix}\\right)$, then\n",
    "$$E\\left[y|x\\right]=\\mu_{y}+\\rho\\frac{\\sigma_{y}}{\\sigma_{x}}\\left(x-\\mu_{x}\\right)=\\left(\\mu_{y}-\\rho\\frac{\\sigma_{y}}{\\sigma_{x}}\\mu_{x}\\right)+\\rho\\frac{\\sigma_{y}}{\\sigma_{x}}x.$$\n",
    "\n",
    "Even though in general $m\\left(x\\right)\\neq x'\\beta$, the linear form\n",
    "$x'\\beta$ is still useful as an approximation, as will be clear soon.\n",
    "Therefore, we may write the linear regression model, or the *linear\n",
    "projection model*, as \n",
    "$$\n",
    "\\begin{aligned}\n",
    "y & =  x'\\beta+e\\\\\n",
    "E[x e] & =  0,\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $e$ is called the *projection\n",
    "error*, to be distinguished from $\\varepsilon=y-m\\left(x\\right)$.\n",
    "\n",
    "If a constant is included in $x$ as a regressor, we have\n",
    "$E\\left[e\\right]=0$.\n",
    "\n",
    "The coefficient $\\beta$ in the linear projection model has a\n",
    "straightforward closed-form. Multiplying $x$ on both sides and taking\n",
    "expectation, we have $E[xy]=E[xx']\\beta$. If $E[xx']$ is invertible, we\n",
    "can explicitly solve\n",
    "$$\\beta=\\left(E\\left[xx'\\right]\\right)^{-1}E\\left[xy\\right].$$\n",
    "\n",
    "Now we justify $x'\\beta$ as an approximation to $m\\left(x\\right)$.\n",
    "Indeed, $x'\\beta$ is the optimal *linear* predictor in terms of MSE; in\n",
    "other words,\n",
    "$$\\beta=\\arg\\min_{b\\in\\mathbb{R}^{K}}E\\left[\\left(y-x'b\\right)^{2}\\right].\\label{eq:min_MSE}$$\n",
    "This fact can be verified by taking the first-order condition of the\n",
    "above minimization problem\n",
    "$$\\frac{\\partial}{\\partial\\beta}E\\left[\\left(y-x'\\beta\\right)^{2}\\right]=2E\\left[x\\left(y-x'\\beta\\right)\\right]=0.$$\n",
    "\n",
    "In the meantime, $x'\\beta$ is also the best *linear* approximation to\n",
    "$m(x)$. If we replace $y$ in the optimization problem by $m\\left(x\\right)$, we\n",
    "solve the minimizer as\n",
    "$$\\left(E\\left[xx'\\right]\\right)^{-1}E\\left[xm\\left(x\\right)\\right]=\\left(E\\left[xx'\\right]\\right)^{-1}E\\left[E\\left[xy|x\\right]\\right]=\\left(E\\left[xx'\\right]\\right)^{-1}E\\left[xy\\right]=\\beta.$$\n",
    "Thus $\\beta$ is also the best linear approximation to\n",
    "$m\\left(x\\right)$ in terms of MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Omitted Variable Bias\n",
    "----------------------------------------------\n",
    "\n",
    "We write the *long regression* as\n",
    "$$y=x_{1}'\\beta_{1}+x_{2}'\\beta_{2}+\\beta_{3}+e,$$ and the *short\n",
    "regression* as $$y=x_{1}'\\gamma_{1}+\\gamma_{2}+u.$$ If $\\beta_{1}$ in\n",
    "the long regression is the parameter of interest, omitting $x_{2}$ as in\n",
    "the short regression will render *omitted variable bias* (meaning\n",
    "$\\gamma_{1}\\neq\\beta_{1}$) unless $x_{1}$ and $x_{2}$ are uncorrelated.\n",
    "\n",
    "We first demean all the variables in the two regressions, which is\n",
    "equivalent as if we project out the effect of the constant. The long\n",
    "regression becomes\n",
    "$$\\tilde{y} = \\tilde{x}_{1}'\\beta_{1}+\\tilde{x}_{2}'\\beta_{2}+\\tilde{e},$$ and the short regression becomes\n",
    "$$\\tilde{y}=\\tilde{x}_{1}'\\gamma_{1}+\\tilde{u},$$ where *tilde* denotes\n",
    "the demeaned variable.\n",
    "\n",
    "After demeaning, the cross-moment equals to the covariance. The short\n",
    "regression coefficient \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\gamma_{1} & =  \\left(E\\left[\\tilde{x}_{1}\\tilde{x}_{1}'\\right]\\right)^{-1}E\\left[\\tilde{x}_{1}\\tilde{y}\\right]\\\\\n",
    " & =  \\left(E\\left[\\tilde{x}_{1}\\tilde{x}_{1}'\\right]\\right)^{-1}E\\left[\\tilde{x}_{1}\\left(\\tilde{x}_{1}'\\beta_{1}+\\tilde{x}_{2}'\\beta_{2}+e\\right)\\right]\\\\\n",
    " & =  \\beta_{1}+\\left(E\\left[\\tilde{x}_{1}\\tilde{x}_{1}'\\right]\\right)^{-1}E\\left[\\tilde{x}_{1}\\tilde{x}_{2}'\\right]\\beta_{2}.\\end{aligned}\n",
    "$$\n",
    "Therefore, $\\gamma_{1}=\\beta_{1}$ if and only if\n",
    "$E\\left[\\tilde{x}_{1}\\tilde{x}_{2}'\\right]\\beta_{2}=0$, which demands\n",
    "either $E\\left[\\tilde{x}_{1}\\tilde{x}_{2}'\\right]=0$ or $\\beta_{2}=0$.\n",
    "\n",
    "Obviously we prefer to run the long regression to attain $\\beta_{1}$ if\n",
    "possible, as it is a model general model than the short regression. \n",
    "However, sometimes $x_{2}$ is simply unobservable so the long\n",
    "regression is infeasible. When only the short regression is available,\n",
    "in some cases we are able to sign the bias, meaning that we know whether\n",
    "$\\gamma_{1}$ is bigger or smaller than $\\beta_{1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
