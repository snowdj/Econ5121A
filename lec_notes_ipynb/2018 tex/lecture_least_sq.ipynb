{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notation: $y_{i}$ is a scalar, and $x_{i}$ is a $K\\times1$ vector. $Y$\n",
    "is an $n\\times 1$ vector, and $X$ is an $n\\times K$ matrix.\n",
    "\n",
    "Algebra of Least Squares\n",
    "========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLS estimator\n",
    "-------------\n",
    "\n",
    "As we have learned from the linear project model, the parameter $\\beta$\n",
    "$$\\begin{aligned}\n",
    "y_{i} & =  x'_{i}\\beta+e_{i}\\\\\n",
    "E[x_{i}e_{i}] & =  0\\end{aligned}$$ can be written as\n",
    "$\\beta=\\left(E\\left[x_{i}x_{i}'\\right]\\right)^{-1}E\\left[x_{i}y_{i}\\right].$\n",
    "\n",
    "While population is something imaginary, in reality we possess a sample\n",
    "of $n$ observations. We thus replace the population mean\n",
    "$E\\left[\\cdot\\right]$ by the sample mean, and the resulting estimator is\n",
    "$$\\widehat{\\beta}=\\left(\\frac{1}{n}\\sum_{i=1}^{n}x_{i}x_{i}'\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^{n}x_{i}y_{i}=\\left(X'X\\right)^{-1}X'y.$$\n",
    "This is one way to motivate the OLS estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "beta0 = c(1.0, 1.0, 0.0) \n",
    "X = cbind(rnorm(n), rpois(n, 3) )\n",
    "e = rlogis(n) # the error term does not have to be normally distributed\n",
    "\n",
    "y = cbind(1, X ) %*% beta0 + e # generate data\n",
    "# in reality, we observe y and X but not e and beta0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can derive the OLS estimator from minimizing the sum\n",
    "of squared residuals\n",
    "$$Q\\left(\\beta\\right)=\\sum_{i=1}^{n}\\left(y_{i}-x_{i}'\\beta\\right)^{2}=\\left(Y-X\\beta\\right)'\\left(Y-X\\beta\\right).$$\n",
    "By the first-order condition\n",
    "$$\\frac{\\partial}{\\partial\\beta}Q\\left(\\beta\\right)=-2X'\\left(Y-X\\beta\\right),$$\n",
    "the optimality condition gives exactly the same $\\widehat{\\beta}$.\n",
    "Moreover, the second-order condition\n",
    "$$\\frac{\\partial^{2}}{\\partial\\beta\\partial\\beta'}Q\\left(\\beta\\right)=2X'X$$\n",
    "shows that $Q\\left(\\beta\\right)$ is convex in $\\beta$.\n",
    "($Q\\left(\\beta\\right)$ is strictly convex in $\\beta$ if $X'X$ is\n",
    "positive definite.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Call:\n",
      "lm(formula = y ~ X)\n",
      "\n",
      "Coefficients:\n",
      "(Intercept)           X1           X2  \n",
      "    0.71285      0.63639      0.08951  \n",
      "\n",
      "           [,1]\n",
      "[1,] 0.71285448\n",
      "[2,] 0.63638941\n",
      "[3,] 0.08950762\n"
     ]
    }
   ],
   "source": [
    "reg1 = lm( y ~ X ) # OLS regression\n",
    "print(reg1)\n",
    "\n",
    "X1 = cbind(1, X) # the first column of X is a constant\n",
    "bhat = solve(t(X1)%*%X1, t(X1) %*% y )\n",
    "print(bhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we introduce some definitions and properties in OLS estimation.\n",
    "\n",
    "-   Fitted value: $\\widehat{Y}=X\\widehat{\\beta}$.\n",
    "\n",
    "-   Projector: $P_{X}=X\\left(X'X\\right)^{-1}X$; Annihilator:\n",
    "    $M_{X}=I_{n}-P_{X}$.\n",
    "\n",
    "-   $P_{X}M_{X}=M_{X}P_{X}=0$.\n",
    "\n",
    "-   If $AA=A$, we call it an idempotent matrix. Both $P_{X}$ and $M_{X}$\n",
    "    are idempotent.\n",
    "\n",
    "-   Residual:\n",
    "    $\\widehat{e}=Y-\\widehat{Y}=Y-X\\widehat{\\beta}=M_{X}Y=M_{X}\\left(X\\beta+e\\right)=M_{X}e$.\n",
    "\n",
    "-   $X'\\widehat{e}=XM_{X}e=0$.\n",
    "\n",
    "-   $\\frac{1}{n}\\sum_{i=1}^{n}\\widehat{e}_{i}=0$ if $x_{i}$ contains a\n",
    "    constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtAAAAJYCAMAAAB8aiEbAAAAM1BMVEUAAABNTU1oaGh8fHyM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\n7RKBCKw0IgAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "yhat = predict( reg1, data  = X ) # predicted value from the OLS regression\n",
    "matplot( x = X[,1], y = cbind(y, yhat), pch = 1:2, xlab = \"x\", ylab = \"y\") # a graph between x1 and y and yhat\n",
    "\n",
    "library(repr)\n",
    "options(repr.plot.width=6, repr.plot.height=5)\n",
    "legend(x = 1.2, y = -2, pch = 1:2, col = 1:2, legend = c(\"y\", \"predicted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           [,1]       [,2]       [,3]\n",
      "[1,]  0.7406951  0.7406951  0.7406951\n",
      "[2,]  0.1507748  0.1507748  0.1507748\n",
      "[3,]  0.3846270  0.3846270  0.3846270\n",
      "[4,]  0.1888849  0.1888849  0.1888849\n",
      "[5,] -0.3220290 -0.3220290 -0.3220290\n",
      "[6,]  0.4483823  0.4483823  0.4483823\n",
      "              [,1]\n",
      "[1,]  2.442491e-15\n",
      "[2,] -1.539737e-15\n",
      "[3,] -1.163514e-13\n"
     ]
    }
   ],
   "source": [
    "# check the orthogonality of ehat and X1\n",
    "ehat = y - X1 %*% bhat\n",
    "MX = diag(rep(1,n) ) - X1 %*% solve( t(X1)%*% X1 ) %*% t(X1)\n",
    "print( head ( cbind(ehat, MX%*%y, MX%*%e ) ) )\n",
    "\n",
    "print( t(X1) %*% ehat )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of the residual is  6.82115e-17 and the sum is 6.827872e-15 \n",
      "But the mean of the true error term is 0.01046438 \n"
     ]
    }
   ],
   "source": [
    "cat(\"The mean of the residual is \", mean(ehat), \"and the sum is\", sum(ehat), \n",
    "    \"\\nBut the mean of the true error term is\", mean(e), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Real Data Example**\n",
    "\n",
    "We check the relationship between *health status* and three control variables: *the number of doctor visits*, *the number of children in the household*, and\n",
    "*access to health care*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: 'Ecfun'\n",
      "\n",
      "The following object is masked from 'package:base':\n",
      "\n",
      "    sign\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>doctor</th><th scope=col>children</th><th scope=col>access</th><th scope=col>health</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td> 0    </td><td>1     </td><td>0.50  </td><td> 0.495</td></tr>\n",
       "\t<tr><td> 1    </td><td>3     </td><td>0.17  </td><td> 0.520</td></tr>\n",
       "\t<tr><td> 0    </td><td>4     </td><td>0.42  </td><td>-1.227</td></tr>\n",
       "\t<tr><td> 0    </td><td>2     </td><td>0.33  </td><td>-1.524</td></tr>\n",
       "\t<tr><td>11    </td><td>1     </td><td>0.67  </td><td> 0.173</td></tr>\n",
       "\t<tr><td> 3    </td><td>1     </td><td>0.25  </td><td>-0.905</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       " doctor & children & access & health\\\\\n",
       "\\hline\n",
       "\t  0     & 1      & 0.50   &  0.495\\\\\n",
       "\t  1     & 3      & 0.17   &  0.520\\\\\n",
       "\t  0     & 4      & 0.42   & -1.227\\\\\n",
       "\t  0     & 2      & 0.33   & -1.524\\\\\n",
       "\t 11     & 1      & 0.67   &  0.173\\\\\n",
       "\t  3     & 1      & 0.25   & -0.905\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "doctor | children | access | health | \n",
       "|---|---|---|---|---|---|\n",
       "|  0     | 1      | 0.50   |  0.495 | \n",
       "|  1     | 3      | 0.17   |  0.520 | \n",
       "|  0     | 4      | 0.42   | -1.227 | \n",
       "|  0     | 2      | 0.33   | -1.524 | \n",
       "| 11     | 1      | 0.67   |  0.173 | \n",
       "|  3     | 1      | 0.25   | -0.905 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  doctor children access health\n",
       "1  0     1        0.50    0.495\n",
       "2  1     3        0.17    0.520\n",
       "3  0     4        0.42   -1.227\n",
       "4  0     2        0.33   -1.524\n",
       "5 11     1        0.67    0.173\n",
       "6  3     1        0.25   -0.905"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(Ecdat, quietly = TRUE, warn.conflicts = FALSE)\n",
    "\n",
    "data(Doctor)\n",
    "head(Doctor) # display the data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Call:\n",
      "lm(formula = health ~ doctor + children + access, data = Doctor)\n",
      "\n",
      "Coefficients:\n",
      "(Intercept)       doctor     children       access  \n",
      "   -0.02810      0.12059      0.03323     -0.63320  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "reg = lm(health ~ doctor + children + access, data = Doctor)\n",
    "print(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = health ~ doctor + children + access, data = Doctor)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-3.3370 -1.0085 -0.3261  0.6938  6.1266 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept) -0.02810    0.18281  -0.154    0.878    \n",
       "doctor       0.12059    0.01884   6.399 3.71e-10 ***\n",
       "children     0.03323    0.04771   0.697    0.486    \n",
       "access      -0.63320    0.33724  -1.878    0.061 .  \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 1.378 on 481 degrees of freedom\n",
       "Multiple R-squared:  0.08221,\tAdjusted R-squared:  0.07649 \n",
       "F-statistic: 14.36 on 3 and 481 DF,  p-value: 5.628e-09\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goodness of Fit\n",
    "---------------\n",
    "\n",
    "The so-called R-square is the most popular measure of goodness-of-fit in\n",
    "the linear regression. R-square is well defined only when a constant is\n",
    "included in the regressors. Let\n",
    "$M_{\\iota}=I_{n}-\\frac{1}{n}\\iota\\iota'$, where $\\iota$ is an $n\\times1$\n",
    "vector of 1’s. $M_{\\iota}$ is the *demeaner*, in the sense that\n",
    "$M_{\\iota}\\left(z_{1},\\ldots,z_{n}\\right)'=\\left(z_{1}-\\overline{z},\\ldots,z_{n}-\\overline{z}\\right)'$,\n",
    "where $\\bar{z}=\\frac{1}{n}\\sum_{i=1}^{n}z_{i}$. For any $X$, we can\n",
    "decompose $Y=P_{X}Y+M_{X}Y=\\widehat{Y}+\\widehat{e}$. The total variation\n",
    "is\n",
    "$$Y'M_{\\iota}Y=\\left(\\widehat{Y}+\\widehat{e}\\right)'M_{\\iota}\\left(\\widehat{Y}+\\widehat{e}\\right)=\\widehat{Y}'M_{\\iota}\\widehat{Y}+2\\widehat{Y}'M_{\\iota}\\widehat{e}+\\widehat{e}'M_{\\iota}\\widehat{e}=\\widehat{Y}'M_{\\iota}\\widehat{Y}+\\widehat{e}'\\widehat{e}$$\n",
    "where the last equality follows by $M_{\\iota}\\widehat{e}=\\widehat{e}$ as\n",
    "$\\frac{1}{n}\\sum_{i=1}^{n}\\widehat{e}_{i}=0$, and\n",
    "$\\widehat{Y}'\\widehat{e}=Y'P_{X}M_{X}e=0$. R-square is defined as\n",
    "$\\widehat{Y}'M_{\\iota}\\widehat{Y}/Y'M_{\\iota}Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frish-Waugh-Lovell Theorem\n",
    "--------------------------\n",
    "\n",
    "The FWL theorem is an algebraic fact about the formula of a subvector\n",
    "of the OLS estimator. To derive the FWL theorem we need to use the inverse of partitioned matrix.\n",
    "For a positive definite symmetric matrix $A=\\begin{pmatrix}A_{11} & A_{12}\\\\\n",
    "A_{12}' & A_{22}\n",
    "\\end{pmatrix}$, the inverse can be written as \n",
    "$$\n",
    "A^{-1}=\\begin{pmatrix}\\left(A_{11}-A_{12}A_{22}^{-1}A_{12}'\\right)^{-1} & -\\left(A_{11}-A_{12}A_{22}^{-1}A_{12}'\\right)^{-1}A_{12}A_{22}^{-1}\\\\\n",
    "-A_{22}^{-1}A_{12}'\\left(A_{11}-A_{12}A_{22}^{-1}A_{12}'\\right)^{-1} & \\left(A_{22}-A_{12}'A_{11}^{-1}A_{12}\\right)^{-1}\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "In our context of the OLS estimator, \n",
    "\\begin{align*}\n",
    "\\widehat{\\beta} & =\\begin{pmatrix}\\widehat{\\beta}_{1}\\\\\n",
    "\\widehat{\\beta}_{2}\n",
    "\\end{pmatrix}=\\begin{pmatrix}X_{1}'X_{1} & X_{1}'X_{2}\\\\\n",
    "X_{2}'X_{1} & X_{2}'X_{2}\n",
    "\\end{pmatrix}^{-1}\\begin{pmatrix}X_{1}y\\\\\n",
    "X_{2}y\n",
    "\\end{pmatrix}\\\\\n",
    " & =\\begin{pmatrix}\\left(X_{1}M_{X_{2}}'X_{1}\\right)^{-1} & -\\left(X_{1}M_{X_{2}}'X_{1}\\right)^{-1}X_{1}'X_{2}\\left(X_{2}'X_{2}\\right)^{-1}\\\\\n",
    "\\cdot & \\cdot\n",
    "\\end{pmatrix}\\begin{pmatrix}X_{1}y\\\\\n",
    "X_{2}y\n",
    "\\end{pmatrix}.\n",
    "\\end{align*}\n",
    "The subvector\n",
    "\\begin{align*}\n",
    "\\widehat{\\beta}_{1} & =\\left(X_{1}M_{X_{2}}'X_{1}\\right)^{-1}X_{1}y-\\left(X_{1}M_{X_{2}}'X_{1}\\right)^{-1}X_{1}'X_{2}\\left(X_{2}'X_{2}\\right)^{-1}X_{2}y\\\\\n",
    " & =\\left(X_{1}M_{X_{2}}'X_{1}\\right)^{-1}\\left(X_{1}y-X_{1}'P_{X_{2}}y\\right)\\\\\n",
    " & =\\left(X_{1}M_{X_{2}}'X_{1}\\right)^{-1}X_{1}M_{X_{2}}y.\n",
    "\\end{align*}\n",
    "\n",
    "Similar derivation can also be carried out in the population linear\n",
    "projection. See Hansen's Chapter 2.21-23."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           [,1]\n",
      "[1,] 0.08950762\n"
     ]
    }
   ],
   "source": [
    "X2 = X1[,1:2]\n",
    "PX2 = X2 %*% solve( t(X2) %*% X2) %*% t(X2)\n",
    "MX2 = diag(rep(1,n)) - PX2\n",
    "\n",
    "X3 = X1[,3]\n",
    "\n",
    "bhat3 =  solve(t(X3)%*% MX2 %*% X3, t(X3) %*% MX2 %*% y )\n",
    "print(bhat3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical Properties of Least Squares\n",
    "=======================================\n",
    "\n",
    "To talk about the statistical properties in finite sample, we impose the\n",
    "following assumptions.\n",
    "\n",
    "1.  The data $\\left(y_{i},x_{i}\\right)_{i=1}^{n}$ is a random sample\n",
    "    from the same data generating process $y_{i}=x_{i}'\\beta+e_{i}$.\n",
    "\n",
    "2.  $e_{i}|x_{i}\\sim N\\left(0,\\sigma^{2}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum Likelihood Estimation\n",
    "-----------------------------------\n",
    "\n",
    "Under the normality assumption,\n",
    "$y_{i}|x_{i}\\sim N\\left(x_{i}'\\beta,\\gamma\\right)$, where\n",
    "$\\gamma=\\sigma^{2}$. The *conditional* likelihood of observing a sample\n",
    "$\\left(y_{i},x_{i}\\right)_{i=1}^{n}$ is\n",
    "$$\\prod_{i=1}^{n}\\frac{1}{\\sqrt{2\\pi\\gamma}}\\exp\\left(-\\frac{1}{2\\gamma}\\left(y_{i}-x_{i}'\\beta\\right)^{2}\\right),$$\n",
    "and the (conditional) log-likelihood function is\n",
    "$$L\\left(\\beta,\\gamma\\right)=-\\frac{n}{2}\\log2\\pi-\\frac{n}{2}\\log\\gamma-\\frac{1}{2\\gamma}\\sum_{i=1}^{n}\\left(y_{i}-x_{i}'\\beta\\right)^{2}.$$\n",
    "Therefore, the maximum likelihood estimator (MLE) coincides with the OLS\n",
    "estimator, and\n",
    "$\\widehat{\\gamma}_{\\mathrm{MLE}}=\\widehat{e}'\\widehat{e}/n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finite Sample Distribution\n",
    "--------------------------\n",
    "\n",
    "We can show the finite-sample exact distribution of $\\widehat{\\beta}$.\n",
    "*Finite sample distribution* means that the distribution holds for any\n",
    "$n$; it is in contrast to *asymptotic distribution*, which is a large sample\n",
    "approximation to the finite sample distribution.\n",
    "\n",
    "Since\n",
    "$$\\widehat{\\beta}=\\left(X'X\\right)^{-1}X'y=\\left(X'X\\right)^{-1}X'\\left(X'\\beta+e\\right)=\\beta+\\left(X'X\\right)^{-1}X'e,$$\n",
    "we have the estimator\n",
    "$\\widehat{\\beta}|X\\sim N\\left(\\beta,\\sigma^{2}\\left(X'X\\right)^{-1}\\right)$,\n",
    "and\n",
    "$$\\widehat{\\beta}_{k}|X\\sim N\\left(\\beta_{k},\\sigma^{2}\\eta_{k}'\\left(X'X\\right)^{-1}\\eta_{k}\\right)\\sim N\\left(\\beta_{k},\\sigma^{2}\\left(X'X\\right)_{kk}^{-1}\\right),$$\n",
    "where $\\eta_{k}=\\left(1\\left\\{ l=k\\right\\} \\right)_{l=1,\\ldots,K}$ is\n",
    "the selector of the $k$-th element.\n",
    "\n",
    "In reality, $\\sigma^{2}$ is an unknown parameter, and\n",
    "$$s^{2}=\\widehat{e}'\\widehat{e}/\\left(n-K\\right)=e'M_{X}e/\\left(n-K\\right)$$\n",
    "is an unbiased estimator of $\\sigma^{2}$. Consider the $T$-statistic\n",
    "$$T_{k}=\\frac{\\widehat{\\beta}_{k}-\\beta_{k}}{\\sqrt{s^{2}\\left[\\left(X'X\\right)^{-1}\\right]_{kk}}}=\\frac{\\left(\\widehat{\\beta}_{k}-\\beta_{k}\\right)/\\sqrt{\\sigma^{2}\\left[\\left(X'X\\right)^{-1}\\right]_{kk}}}{\\sqrt{\\frac{e'}{\\sigma}M_{X}\\frac{e}{\\sigma}/\\left(n-K\\right)}}.$$\n",
    "The numerator follows a standard normal, and the denominator follows\n",
    "$\\frac{1}{n-K}\\chi^{2}\\left(n-K\\right)$. Moreover, the numerator and the\n",
    "denominator are independent. As a result, $T_{k}\\sim t\\left(n-K\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean and Variance\n",
    "-----------------\n",
    "\n",
    "Now we relax the normality assumption and statistical independence.\n",
    "Instead, we assume a regression model $y_{i} =  x_{i}'\\beta+e_{i}$ and \n",
    "$$\\begin{aligned}\n",
    "E[e_{i}|x_{i}] & =  0 \\\\\n",
    "E\\left[e_{i}^{2}|x_{i}\\right] & =  \\sigma^{2}.\n",
    "\\end{aligned}$$\n",
    "where the first condition is the *mean independence* assumption, and\n",
    "the second condition is the *homoskedasticity* assumption.\n",
    "\n",
    "**Example**\n",
    "(Heteroskedasticity) If $e_{i}=x_{i}u_{i}$, where $x_{i}$ is a scalar\n",
    "random variable, $u_{i}$ is independent of $x_{i}$,\n",
    "$E\\left[u_{i}\\right]=0$ and $E\\left[u_{i}^{2}\\right]=\\sigma^{2}$. Then\n",
    "$E\\left[e_{i}|x_{i}\\right]=0$ but\n",
    "$E\\left[e_{i}^{2}|x_{i}\\right]=\\sigma_{i}^{2}x_{i}^{2}$ is a function of\n",
    "$x_{i}$. We say $e_{i}^{2}$ is a heteroskedastic error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtAAAAJYCAMAAAB8aiEbAAAAMFBMVEUAAABNTU1oaGh8fHyM\njIyampqnp6eysrK9vb3Hx8fQ0NDZ2dnh4eHp6enw8PD////QFLu4AAAACXBIWXMAABJ0AAAS\ndAHeZh94AAAXbklEQVR4nO2di3qiShAGB29rEpX3f9tVRAVF5TLT0/xUfefEJMI0MkXTNK4J\nJYAQIfcGAMQEoUEKhAYpEBqkQGiQAqFBCoQGKRAapEBokAKhQQqEBikQGqRAaJACoUEKhAYp\nEBqkQGiQAqFBCoQGKRAapEBokAKhQQqEBikQGqRAaJACoUEKhAYpEBqkQGiQAqFBCoQGKRAa\npEBokAKhQQqEBikQGqRAaJACoUEKhAYpEBqkQGiQAqF7E0LUnfV9uB4Bt30XXAzsid74E/qv\nCD2HWg7sid74E/q2AEI/YE/0xq/Q8IA90puLPn/rELbH+he/2/Ovtr+PZ/ersPory30R1n9d\ny5z+nVcPm5/7CufnQygu4512RSh29cjHbRFW+7uvP5vzd6vrk40hwpXyIfZptwph/Zt0L3gH\noXtz1mZXKVRcvVvXRm1uz15/cbwu9Pe6zLGof1rXK5x/VY92e6pa669eqvb0NsblyeYQL0Lf\nntyZ7hZnIHRvwp2qt7C5/7hpPVs0ftle5pysz5n1dBZ0f13houD16Lh5WjR/uHq6P9t7Kstd\nNUZziBeh7+stOUcjdG8uvv1VZl20O9cKYX86lwC1QKHybn8pDg7Vw+syl/R9/u3pvMh1hdNZ\nwcNl6Ku0p6uuP+cA56V/i+sgq+taV2tfhijvT10GKQ6V7Cv7neMGhO5Nbe7pqs/2mmcrwbfX\nZ/9aD6/LXAzd/jaGW1cCl1UmP11/t6l+qBb6Da1rvuqnlyEaj/V6p9W/Y7lcELo3bX1C7eCl\nDG6c9lsPT8v8qwuW39sw93r3UWMUjd7F/Zvjz24duodob1HS1z8P2Ae9eafPR6Fby+xu1h7L\nu8R1Zm5Uzc9C/6waFfXzEJ1btGTYB715n6GL8nuGri73Tj/XlsX6+mTxr07RRdPFJ6F/LmX5\ndn+of/00RHuLkr32+cA+6M1LxfpcQ5fPD8/LVFSd6bKutldX5TfNzsTth5/7ReFvM3x7iMYW\nramhS4QeQFufji5H+fzwtMzqXmDcU/rvNUX/XBsoP1Xi3V+7HD9Fy9prhn4d4nRfhC7HBYTu\nTVvox/2Oa/LtEvppmbO962N1YbcrG2n10ri7t5D/yuc+9LpavG7itYe4/O4x1GO9veFe8QZC\n9+ZJ6Lutrbdwth+elrld0T3uFF4S7+Wey2/9TFVR13cKN9cl6p8uN2z+nobYtmvpP+4UIvQA\nnoU+l7JF+70cLw9Py1yL3/W+ucLmmqKrt2Fs6uWO22qpW7Fx/qnYHo61+o0hLis36vfqDSH3\nQRYKQoMUCA1SIDRIgdAgBUKDFAgNUiA0SIHQIAVCgxQIDVIgNEiB0CAFQoMUCA1SIDRIgdAg\nBUKDFAgNUiA0SIHQIAVCgxQIDVIgNEiB0CAFQoMUCA1SIDRIgdAgBUKDFAgNUiA0SIHQIAVC\ngxQIDVIgNEiB0CAFQoMUCA1SIDRIYSB0ABjJCNviC5whBGiC0CAFQoMUCA1SIDRIgdAgBUKD\nFAgNUiA0SIHQ1oy6mQV9QWhbKptROh0IbUtofIUEILQp4ekRYoPQpiB0ahDaFIRODULbkqSG\npnHyAKFtSdDloHHSBKGtiZ5OaZw0Qei5Q1newlLo0zaE9W89yMdRljc54/M2QrcwFPpUVP+I\ncXMdZHFCf1J2ShmM0C0Mhd6F/dnqfbGuBlmY0J+VnVQGU0M3MRS6uK54LFbHBQrd+PrmybEv\nmy5HE0Ohb/v8tF53TcDED1fwzWdlp1YNgjtsNIZCr8Lp9t16aRk6rdDwwFDofdjW3x3DGqFf\nnx7wqsnJ77Bs2+3us/D7ZUL0ZuuzsgPLYKrm95jeWDlsbt8dt0sT+ouDg1IufY33cKfQinhV\nAiX3BxB6fiD0BxB6fiD0BxB6hlBDvwehvTCgxqbL8R6E9sHgxt3ydlE/ENoHVBGRQGgXcJ0X\nC4R2AULHAqFdgNCxQGgfUENHAqF9QCcuEgjthfgfb7DI4wOhRVlqykdoUZZalCO0JottmyC0\nJpmEzl+3I7QmWYT2ULcjtCg5amgPdTtCi5IhW7qo2xFaFvN6FqFzhoDoIHTOEBAfauiMIRyT\nv/c1ErocGUO4xYMVo8l/LCL0AEymy8N5e8YgdG9sUqeLK6sZg9C9sUmdPoTOXzqMBaH7YmSa\nB6HnXMYjdF+sTHNQQzvYhNEgdF/MhM6eHj2cJEaD0L0xy1u5C1iEjo75ruzjUP7UaQRCR8f8\nbellL1Vzp04rqKFjk+VtYvOcwBTM+VyE0DM/xcbkcQaa77kIoRG6Zs55+QFCI3SNRuGF0KXK\nVE7E32E9qu5B6FLlZNufTlO8CT1yUhD6GnBROpddprgTuvF18GqpV3EYQo7ex+Q7U3wVXmOP\nL4Q2JtG5oP8J+q0pvgovhJ4FyaTpn14/mOKp8ELoWZDqtD5g+r3Vyu+ghp4ByWQaMrCvWvkt\ndDmGjB/r3DpwIB9Cu6qVP0Afuu/osWZ08EDpTveD0q6nWjk2ixQ6VozhAyU73c8m7aZmgUJH\nS5MjBkronXLaHQBCGw+Uz7tlGI/QDgayYCk1yQKFzllD52NO2zoFcaG731eWrcuRj1mdTaYg\nLfRb4XL1oS/LJytlPw6M0FFXyRTC22n2bFzlc5I3c3w+WyB01FXyhBg3iQmbAaHxX4Kxy08D\nezu4U4HQT+skLItvJqcw+utr7X5heq08hO5aR1DoLnlndFHbG2WhR9+aTvQaMwv9biWEjr8V\niUIMz0Bpr51y1tBvVxEzWlroEW212BvQ2oiMXY7OVZ4eJRAXemTkmPFbpmXrQ3eu8PQoAUI/\nRY5+neS4UHW8aaNB6JfYcTOo5zRIl2PsKg5DWOFZaPrQY1dxGOJT9Jiz7FtoPQyFDm1ShHgT\nduDyZdTzsGKh6hhDofefhe5t+yDGtqLHbUHXlisWqo6xLDkOxTp1iDcjDRhvQonQpe617YzO\nZpjW0IewSx3izUD9B5wi9MuKZGdzbC8K9+GQOkT3QBZCd6xJ/WyOeJdjhJ6jJXyNRYfDHnGh\nx7zhbmyZgNAekBd6hJ5jL+JeDh6EtkddaMubYa8HT5QamibJEPSFtuTZvQhdDholw0DotExO\nrzRKhoHQvhlXhnffiI2xPe5BaN+MEbqjShnwN4VmLj5C+2aU0K9r9C1c5l+xI7RzOlUc+qlf\nvQ+L+VfsCO2c4fXDBKEFGucI7Z6XdPwljSJ0+lUchpgvX60bX0MjdCJmvEPT813o8V0Oaug0\nZN2j3htXPdLo6D40XY40ZNyjM5jSpGl03OHsJwkgdGdoL9PTibtjztMGIXR3ZB+z8w4/CbHC\nUxJA6O7IPmZnHrjaZwjdHTn75DhLwp9ws88uIHRn6Nxz46kq/QpCewjxNrQLlXwcVn3xtLUI\n/Ro8u86+ct53fCSBKwjtkZkJ7SIJ1CB0AiL9u6vZ74ccIHR0YpyAPVWl8wKhoxNDxncHhZ9T\nu1cQOjaRygU+mXccCB2bCUJ/y78UIt9B6NiMFvpr/uVSsQcIHZ2xefTregjdA4SOzshK97uu\nCN0DhE7AqF5ED12pob+D0F7oIzRdjq8gtBv65F/60N9AaDeQf2OA0GNJkCzJv9NB6HG8Saco\nmRuEHkdnwWtbNHDwdIHQo+huSVi21ai4u0HoUXQKbXrjg550Nwg9iuxC579r6LTiQehxdCXI\nJQnttuJB6HF0TqhhGTBe6DiZ1W3Fg9Bj6fyEz9IsbY00KtIm5j5BVLE7XwZCRyXnn639tOx9\nuUiZNb/Qb/+R2oihpm+NgxDz59PB03yuOfWxRHQg9Jv4CC1IO3uFl6/RUnRunzu2AKHtMKtH\nWra1pj6e0Jm7HAidg5bBdgp0pebuRD0pStamHULb82Sw3Un6o9Be+8dDoYY2p7tqNdx7rZrj\ncWAp6EyXw55OrSxT9D2iSlJ+gj60LTmFflJYJCn3AqFT0V1rWPU5FqRwG4ROxkvtWs7/zO//\nQEHoZLwY3MsGz8rM4ZhE6IQMl3O0MibHQe7bg31AaFeMVCZcfE6utOV17WgQ2hMjlanzOkKX\nCG3MF+vGCl1/Tbzbhm9dhgsChDYk6idAP2S5fWOUontHyXINidCGfBeitzKtNzmbCT3M0CzX\nkAhtR4/8261M17/2an2NoE6/6mBIDZGn5DYV+u/f5nI9Hja7v+EhPDdo+9Frhl9fZpfkT7cf\nJ5/cU1QH8kKfVuHBemCIOfT0vzHxku/9UFXPbtLOSVEdyAu9C8XPofru+FuE3bAQnnv6vV0a\n9SI6vXj+5cTTVxr31GvoIhzu3x9CMSiEzxZopdGAc8eo00z3S48rSyKhxbscrZfW+Um0D97F\n9CR063ZG3xwd648JxZUl1e59ebl2d3/SrlKhlqHbJqfbsjdHTFQ3bKoDi5RtW0P/HqvvJGro\nelvuTeB0gSw0MKkOLCbRsm23btQUq9OwEP66HGZC23QsDWKYnGZt+9C7qg9dbP4J9KGfOmf5\nNmQ26AntKcR0bj1gd+cOryC0b+4m9zx3eDvF2KNWQ7sKEYNB72wgkat1OXyFsIZS+4JUH9pX\nCGM8NtL7M6NqCaFtmLPQs6qWFiy0adqZtdCNr+5ZrNDWaWdWVrSY17E4QejQJvNWjQthKLTt\nARRxQhYj9H7OQseepR47wLDEiXr0LEbo8lB8/ncn45mb0OPe6JxO8OZNzGijRRkrOZNq6MPn\n98yNZ3ZCDx8rZQkS6gCRPk1pQV2OfeMtzjGZWQ095uhImfZC/SXEOwPNRGe6HHHGenr8FjnE\nPkN0bE/z/yWxWKFjpp0hdj59Dl2yFB0eiXpRLFjoiAyoH27Xa60fo2/Po36e266cymShfzeX\nHbc5RtqerhD+6V+/PJr4rR8TbNGsehPxmCr0+tqCDkVUo+c3DX3rl8ZdqTJt62BWvYl4TBR6\nH9any07bh220TSrnKHRfGqVG8tbBjHoT8ZgodBFOKVKB8EQstBIwY6LQAz86aEwIMRZaCZgx\nUehVnaEPYRVtk0ppoVUqAa+vIk4N/VuEfbRNKsWFVsDveWZql2PT6+NxJ4WYM17z2FT8XglE\n6UOHzU+kzekMMVv85rGJdN8WcnH0cqcwIX7z2ES6hHZy9CL0VN7npbS3t3PSKfTLb7KA0NP4\nlJd0he6w18uLRehpfMpL7Tl2UWHG4vU4RujMIeLweRobtj8ZMH+7n18BQmcOEYcvQj8sbmVy\nJ9dPcaGGzhsiDt/y0vO7OMPLVxmcHKUIPY1+bt7Pz41s/WatGRcjLjYdoafRJy89//uRD0I7\nSXMzBqGn8j0vVZLWX8vys9DvnoCeIHRybl2Ox6dkvNXWS6tgxiB0cu6tjUbTruwuLBB6Mgid\nnM77xN11CkJPvrBE6PQMKIxH1tAu+gsRiHBNjNDpGTBNIz/1ccRKLolwTYzQFgzIoKP/vP3U\nfeYgy8eouBB69kQpvF1keYSGMpbQ04eYDkJDGVcDF0ZTQ7sgXw1602DCFngRmi6HF3LWoI1P\n6B27BU6Epg/thrw1aAhh4ha4qKFjgNBRyJ/hJm6Biy5HDBA6CrMX2kUfOgYIHQUBoUVA6Djk\nr0Hzb4ELEDoO+WvQ/FtQeqhbEDoWDuYy9xZ4OKYQGqLhoepBaIiFi+tShIZYIHTOEBAdhM4Z\nAuJDDZ0xRDqyNxuyQZcjY4hUeJjUfOQ/mBE6Mh5Ou0sGoePi4sJoySB0XBA6M4ZChzYpQuQH\noTNjKPR+CUJTQ2fGsuQ4FH3/3uyMfVh2lyM/pjX0IexSh3BA/tbVkrG9KNyHQ+oQMnBcjMJP\nl6N3gb0IqFxG4kdo4xDOsbq2lEseCO0So+6f4Hkgh9Df96DULh6DldAWQWxBaJfYCK14Fwih\nfWKSOxF67CpPAyD0V0yqW4Qeu8rTAAjdA4v+AzX0yFWeBkBoJ9DlGLmKwxBQQR8aoWeCnKr9\nQGgHJHBPsJjoB0JnJ4l7gpd7/UDo7KRwT7Eh1w+Ezk3LvVjFB0KnXcVhCDc03ItXfCB02lUc\nhsjOLRk3hW79Ytro8YaaFwidh0YyvrsXM63S5Ui6isMQmWlk0Lt7cesE+tAJV3EYIi9td0Mz\nVS/g1acEobPQ7e5iC9+IIHQW3gi91MI3IgidhzfJeKGFb0QQOg8k40QgdC5IxklAaJACoUEK\nhAYpEBqkQGiQAqFBCoQGKRAapEBokAKh4YHA3UuEhhsS7y9BaLgh8XZshIYajX8wg9BQg9Dp\nmPc+nSkInY5579O5Qg2djJnv1JlClyMZM9+ps+VtH3o+DWqEhq/MKXUjNHxlTsU1QsM3ZtX+\nQGj4xvPnpLoGoeEzlc7XKjr3pvQBodWI3ZAIjf9mAEJrEb0hcRtvLo07hNYierVbDzgTnRFa\njPgNiVm1OEqEFiOBfjPqcFxAaClSCD2j24QlQquRIp/Op4AuEVqNmeXT+CC0GrPKp/FBaPDH\nhIMSocEbk8omhAZvTLqwRWhwxrTWI0KDMxAapEBo0IIaGqSYTZfjtA1h/VsP8nEUhF428+hD\nn4pwYXMdBKEhBYZC78L+bPW+WFeDIDSkwFDo4rrisVgdERoSYSj0zeHTeo3QkAhDoVfhdPtu\njdCQBkOh92Fbf3cMa4SGJFi27XZ3i3+/9GUQGkZiemPlsLl9d9y+jBKajA4BC4c7hSAFQoMU\nOYT+XlEgNIwEoUEKhAYpEBqkQGiQAqFBCtp2IAVCgxQIDVIgNEiB0CAFQoMUCA1SIDRIgdAg\nBUKDFAgNUiA0SIHQIAVCgxQIDVIgNEiB0CAFQoMUCA1SIDRIgdAgBUKDFAgNUiA0SIHQIAVC\ngxQIDVIgNEiB0CAFQoMUCA1SIDRIgdAgBUKDFAgNUiA0SIHQIMUihQ7f/w4XzJQFCl3ZjNKi\nLFHo9CEgG8sTOjw9ghQIDVIgNEixPKGpoaVZotB0OYRZoND0oZVZpNCgC0KDFAgNUiA0SIHQ\nIAVCgxQIDVIgNEiB0CCFU6EBRjLCtvgCR8J+y4goEBGhiSgVEaGJKBURoYkoFRGhiSgVEaGJ\nKBURoYkoFRGhiSgVEaGJKBURoYkoFRGhiSgVEaGJKBXRr9AAI0BokAKhQQqEBikQGqRAaJAC\noUEKhAYpEBqkQGiQAqFBCoQGKRAapEBokAKhQQqEBikcC71fhWJ3Mo5puD92hfbrq+KZz6Ff\noXfVx08WpnvjYPg3D9fV61uZxbtg+fouZJhDt0IfwvZ0yShby5iF3YT/heJwCfhnFbC0fX1V\nvAxz6FbozXXLLGdgH9Z24Xbh9/z1J/yzCmj8+i5kmEO/QtdY7oywMwy3CcfyksM2VgGNX18z\nLkLfOYW1XbCD5b4P9tnL9PU9MJ1D70LvqxOzHdJC24ersJ1D30IfC8Mz8gWEjo7xHLoW+lRY\nnqwuIHRsrOfQndDNv360NunSNiPaTXixEKFt5vCBY6GPq/XRNqLlhF+7HEfLLkdpL7TVHD5w\nJ/SdX9OL4xq7Cf9XXSr9hp1VwApjoTPMoVuhjzl8NpzwHHcKrYXOMYduhd6O/2OiEzAMt6pe\nnfGM2+7OHHPoVugJfx13UlSzUKfq3XZm4a7Y7s4cc+hWaIAxIDRIgdAgBUKDFAgNUiA0SIHQ\nIAVCgxQIDVIgNEiB0CAFQoMUCA1SIDRIgdAgBUKDFAgNUiA0SIHQIAVCgxQIDVIgNEiB0CAF\nQoMUCA1SIDRIgdAgBUKDFAgNUiA0SIHQIAVCgxQIDVIgdC629Z+jWIdt5i2RAqGzUYR9efnD\nwUXuDZECobPxF8KxPFn/HSx1EDofl6JjQ8ERF4TOSBH+UXBEBqEzci46KDgig9A52VJwxAah\nc1JQccQGoTOyDVwTxgah8/F3zs8U0ZFB6HwU4Yf7KrFB6GycC46SO9+xQehc/IVwOj8cKTqi\ngtC5uL6VgzdzRAahM3F7sx1FR1wQGqRAaJACoUEKhAYpEBqkQGiQAqFBCoQGKRAapEBokAKh\nQQqEBikQGqRAaJACoUEKhAYpEBqkQGiQAqFBCoQGKRAapEBokAKhQQqEBikQGqRAaJACoUEK\nhAYpEBqkQGiQAqFBiv90NXcH1OHxTwAAAABJRU5ErkJggg==",
      "text/plain": [
       "Plot with title \"homoskedastic\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtAAAAJYCAMAAAB8aiEbAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nEDSkEDSkEDSkEDSkEDSk/AfSdnt70bqkVQAAAABJRU5ErkJggg==",
      "text/plain": [
       "Plot with title \"heteroskedastic\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 100\n",
    "X = rnorm(n)\n",
    "\n",
    "e1 = rnorm(n)\n",
    "plot( y = e1, x = X, main = \"homoskedastic\")\n",
    "\n",
    "e2 = X * rnorm(n)  # the source of heteroskedasticity\n",
    "plot( y = e2, x = X,  main = \"heteroskedastic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These assumptions are about the first and second moment of $e_{i}$\n",
    "conditional on $x_{i}$. Unlike the normality assumption, they do not\n",
    "restrict the entire distribution of $e_{i}$.\n",
    "\n",
    "-   Unbiasedness:\n",
    "    $$E\\left[\\widehat{\\beta}|X\\right]=E\\left[\\left(X'X\\right)^{-1}XY|X\\right]=E\\left[\\left(X'X\\right)^{-1}X\\left(X'\\beta+e\\right)|X\\right]=\\beta.$$\n",
    "    Unbiasedness does not rely on homoskedasticity.\n",
    "\n",
    "-   Variance: $$\\begin{aligned}\n",
    "    \\mathrm{var}\\left(\\widehat{\\beta}|X\\right) & =  E\\left[\\left(\\widehat{\\beta}-E\\widehat{\\beta}\\right)\\left(\\widehat{\\beta}-E\\widehat{\\beta}\\right)'|X\\right]\\\\\n",
    "     & =  E\\left[\\left(\\widehat{\\beta}-\\beta\\right)\\left(\\widehat{\\beta}-\\beta\\right)'|X\\right]\\\\\n",
    "     & =  E\\left[\\left(X'X\\right)^{-1}X'ee'X\\left(X'X\\right)^{-1}|X\\right]\\\\\n",
    "     & =  \\left(X'X\\right)^{-1}X'E\\left[ee'|X\\right]X\\left(X'X\\right)^{-1}\\\\\n",
    "     & =  \\left(X'X\\right)^{-1}X'\\left(\\sigma^{2}I_{n}\\right)X\\left(X'X\\right)^{-1}\\\\\n",
    "     & =  \\sigma^{2}\\left(X'X\\right)^{-1}.\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gauss-Markov Theorem\n",
    "--------------------------\n",
    "\n",
    "Gauss-Markov theorem justifies the OLS estimator as the efficient\n",
    "estimator among all linear unbiased ones. *Efficient* here means that it\n",
    "enjoys the smallest variance in a family of estimators.\n",
    "\n",
    "There are numerous linearly unbiased estimators. For example,\n",
    "$\\left(Z'X\\right)^{-1}Z'y$ for $z_{i}=x_{i}^{2}$ is unbiased because\n",
    "$E\\left[\\left(Z'X\\right)^{-1}Z'y\\right]=E\\left[\\left(Z'X\\right)^{-1}Z'\\left(X\\beta+e\\right)\\right]=\\beta$.\n",
    "\n",
    "Let $\\tilde{\\beta}=A'y$ be a generic linear estimator, where $A$ is any\n",
    "$n\\times K$ functions of $X$. As\n",
    "$$E\\left[A'y|X\\right]=E\\left[A'\\left(X\\beta+e\\right)|X\\right]=A'X\\beta.$$\n",
    "So the linearity and unbiasedness of $\\tilde{\\beta}$ implies\n",
    "$A'X=I_{n}$. Moreover, the variance\n",
    "$$\\mbox{var}\\left(A'y|X\\right)=E\\left[\\left(A'y-\\beta\\right)\\left(A'y-\\beta\\right)'|X\\right]=E\\left[A'ee'A|X\\right]=\\sigma^{2}A'A.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see which variance is bigger, we compare $A'A$ and $(X'X)^{-1}$.\n",
    "$$\\begin{aligned}\n",
    "A'A-\\left(X'X\\right)^{-1} \n",
    " & =  \\left(C+X\\left(X'X\\right)^{-1}\\right)'\\left(C+X\\left(X'X\\right)^{-1}\\right)-\\left(X'X\\right)^{-1}\\\\\n",
    " & = C'C+\\left(X'X\\right)^{-1}X'C+C'X\\left(X'X\\right)^{-1} \\\\\n",
    " & = C'C,\n",
    " \\end{aligned}$$ \n",
    "where $C=A-X\\left(X'X\\right)^{-1}$, and the last equality follows as\n",
    "$$\\left(X'X\\right)^{-1}X'C=\\left(X'X\\right)^{-1}X'\\left(A-X\\left(X'X\\right)^{-1}\\right)=\\left(X'X\\right)^{-1}-\\left(X'X\\right)^{-1}=0.$$\n",
    "Therefore $A'A-\\left(X'X\\right)^{-1}$ is a positive semi-definite\n",
    "matrix. The variance of any $\\tilde{\\beta}$ is no smaller than the OLS\n",
    "estimator $\\widehat{\\beta}$.\n",
    "\n",
    "Homoskedasticity is a restrictive assumption. Under homoskedasticity,\n",
    "$\\mathrm{var}\\left(\\widehat{\\beta}\\right)=\\sigma^{2}\\left(X'X\\right)^{-1}$.\n",
    "Popular estimator of $\\sigma^{2}$ is the sample mean of the residuals\n",
    "$\\widehat{\\sigma}^{2}=\\frac{1}{n}\\widehat{e}'\\widehat{e}$ or the\n",
    "unbiased one $s^{2}=\\frac{1}{n-K}\\widehat{e}'\\widehat{e}$. Under\n",
    "heteroskedasticity, Gauss-Markov theorem does not apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "ir"
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.0"
  },
  "nteract": {
   "version": "0.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
